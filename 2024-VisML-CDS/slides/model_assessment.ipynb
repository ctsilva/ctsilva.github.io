{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Visualization for Machine Learning\"\n",
        "subtitle: \"Spring 2024\"\n",
        "format:\n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    chalkboard: \n",
        "      buttons: false\n",
        "    preview-links: auto\n",
        "    logo: figs/vida.jpg\n",
        "    footer: <https://cds.nyu.edu>\n",
        "    fontsize: 24pt\n",
        "resources:\n",
        "  - model_assessment.pdf\n",
        "---"
      ],
      "id": "1a3f6103"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Assessment\n",
        "\n",
        "## Agenda\n",
        "\n",
        "\\\n",
        "\n",
        "1. Confusion Matrices and ROC Curves\n",
        "\n",
        "2. Visual Analytics Systems for Model Performance\n",
        "\n",
        "3. Calibration\n",
        "\n",
        "# Confusion Matrices, ROC Curves\n",
        "\n",
        "## Accuracy Fails\n",
        "\n",
        "## Scenario: Disease Prediction\n",
        "\n",
        "* Consider a disease prediction model. Suppose the hypothetical disease has a 5% prevalence in the population\n",
        "\n",
        "* The given model converges on the solution of predicting that nobody has the disease (i.e., the model predicts “0” for every observation)\n",
        "\n",
        "* Our model is 95% accurate\n",
        "\n",
        "* Yet, public health officials are stumped\n",
        "\n",
        "## Scenario: Handwritten Digits\n",
        "\n",
        "* Consider a model to identify handwritten digits. All digits are equally probable and equally represented in the training and test datasets.\n",
        "\n",
        "* The model correctly identifies all of the digits, except for digit $5$, classifying half of the $5$s samples as $6$ and the other half is correctly identified\n",
        "\n",
        "* The accuracy of this model is $95\\%$. Is this information enough to determine whether the model is good or not?\n",
        "\n",
        "![](figs/model_assessment_figs/MNIST.webp){fig-align=\"center\"}\n",
        "\n",
        "## Extended Confusion Matrix\n",
        "\n",
        "![](figs/model_assessment_figs/extended_confusion_matrix.png){fig-align=\"center\"}\n",
        "\n",
        "## Confusion Matrices in sklearn\n"
      ],
      "id": "9ee36815"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn import datasets, svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X,y = datasets.make_classification(5000, 10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "clf = LogisticRegression(random_state=0)"
      ],
      "id": "7bdf9104",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| fig-pos: c\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "id": "169d0dca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrices\n",
        "\n",
        "::::::: {.columns}\n",
        "::: {.column}\n",
        "![](figs/model_assessment_figs/sphx_glr_plot_label_propagation_digits_001.png)\n",
        ":::\n",
        "::: {.column}\n",
        "::: {.fragment}\n",
        "**Pros**\n",
        "\n",
        "- Many derived metrics\\\n",
        "- Easy to implement\\\n",
        "- Summary of model mistakes is clear\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "**Cons**\n",
        "\n",
        "- Hard to scale\\\n",
        "- Hard to assess probabilistic output\\\n",
        "- Hard to view individual errors\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "## Neo: Hierarchical Confusion Matrix\n",
        "\n",
        "\n",
        "{{< video https://youtu.be/8ZxvsLPIF_Q width=\"60%\" height=\"60%\" >}}\n",
        "\n",
        "\n",
        "![](figs/model_assessment_figs/neo_reference.png){fig-align=\"center\"}\n",
        "\n",
        "## Receiver Operating Characteristic (ROC)\n",
        "\n",
        "## Receiver Operating Characteristic (ROC)\n",
        "\n",
        "ROC analysis is another way to assess a classifier’s output.\n",
        "\n",
        "ROC analysis developed out of radar operation in the second World War, where operators were interested in detecting signal (enemy aircraft) versus noise. \n",
        "\n",
        "We create an ROC curve by plotting the true positive rate (TPR) against the false positive rate (FPR) at various thresholds.\n",
        "\n",
        "## ROC Curve\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "![](figs/model_assessment_figs/intro_roc.png)\n",
        "\n",
        "::: {.fragment}\n",
        "![](figs/model_assessment_figs/confusion_matrix_roc.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "::: {.fragment}\n",
        "![](figs/model_assessment_figs/discrete_roc_graph.png)\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "## ROC Curve\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"45%\"}\n",
        "![](figs/model_assessment_figs/roc_curve_left.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"55%\"}\n",
        "![](figs/model_assessment_figs/roc_curve_right.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {.notes}\n",
        "Speaker notes go here.\n",
        ":::\n",
        "\n",
        "## ROC Curve\n",
        "\n",
        "![](figs/model_assessment_figs/roc_curve_naive_bayes.png){fig-align=\"center\"}\n",
        "\n",
        "::: {.notes}\n",
        "Speaker notes go here.\n",
        ":::\n",
        "\n",
        "## Area under an ROC curve (AUC)\n",
        "\n",
        "\\\n",
        "\n",
        "![](figs/model_assessment_figs/roc_auc.png){fig-align=\"center\"}\n",
        "\n",
        "## ROC curve in sklearn\n"
      ],
      "id": "75a4d3e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "RocCurveDisplay.from_estimator(clf, X_test, y_test, plot_chance_level=True)\n",
        "plt.show()"
      ],
      "id": "832cd13d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiclass ROC curve\n",
        "\n",
        "![](figs/model_assessment_figs/sphx_glr_plot_roc_003.png){fig-align=\"center\" width=80%}\n",
        "\n",
        ":::{.r-stack}\n",
        "<sup>[code](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)</sup>\n",
        ":::\n",
        "\n",
        "# Visual Analytics Systems for Model Performance\n",
        "\n",
        "## Squares (2016)\n",
        "\n",
        "![](figs/model_assessment_figs/squares_teaser.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "Ren, D., Amershi, S., Lee, B., Suh, J., & Williams, J. D. (2016). *Squares: Supporting interactive performance analysis for multiclass classifiers*. IEEE transactions on visualization and computer graphics.\n",
        ":::"
      ],
      "id": "b2f890a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "{{< video https://youtu.be/yUSwjofGAaQ width=\"100%\" height=\"100%\" >}}\n",
        "\n",
        "\n",
        "\n",
        "## Alsallakh et. al. (2014)\n",
        "\n",
        "![](figs/model_assessment_figs/Alsallakh_teaser.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "Alsallakh, B., Hanbury, A., Hauser, H., Miksch, S., & Rauber, A. (2014). *Visual methods for analyzing probabilistic classification data*. IEEE transactions on visualization and computer graphics.\n",
        ":::\n",
        "\n",
        "## Alsallakh et. al. (2014)\n",
        "\n",
        "![](figs/model_assessment_figs/Alsallakh_digits.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "Alsallakh, B., Hanbury, A., Hauser, H., Miksch, S., & Rauber, A. (2014). *Visual methods for analyzing probabilistic classification data*. IEEE transactions on visualization and computer graphics.\n",
        ":::\n",
        "\n",
        "## Beauxis-Aussalet and Hardman (2014)\n",
        "\n",
        "\\\n",
        "\\\n",
        "\n",
        "![](figs/model_assessment_figs/Beauxis_design.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "Beauxis-Aussalet, E., & Hardman, L. (2014). *Visualization of confusion matrix for non-expert users*. In IEEE Conference on Visual Analytics Science and Technology (VAST)-Poster Proceedings.\n",
        ":::\n",
        "\n",
        "## EnsembleMatrix (2009)\n",
        "\n",
        "![](figs/model_assessment_figs/ensemble_matrix.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "Talbot, J., Lee, B., Kapoor, A., & Tan, D. S. (2009, April). *EnsembleMatrix: interactive visualization to support machine learning with multiple classifiers*. In Proceedings of the SIGCHI conference on human factors in computing systems.\n",
        ":::\n",
        "\n",
        "# Calibration\n",
        "\n",
        "## What is calibration?\n",
        "\n",
        "* When performing classification, we often are interested not only in predicting the class label, but also in the probability of the output\n",
        "\n",
        "* This probability gives us a kind of confidence score on the prediction\n",
        "\n",
        "* However, a model can separate the classes well (having a good accuracy/AUC), but be poorly **calibrated**. In this case, the estimated class probabilities are far from the true class probabilities\n",
        "\n",
        "* We can calibrate the model, changing the scale of the predicted probabilities \n",
        "\n",
        "\n",
        "## Calibration - Forecast Example\n",
        "\n",
        "Weather forecasters started thinking about calibration a long time ago (Brier, 1950): a forecast of \"70% chance of rain\" should be followed by rain 70% of the time. Let’s consider a small toy example: \n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"30%\"}\n",
        "![](figs/model_assessment_figs/calibration_forecast_table.png){fig-align=\"center\"}\n",
        ":::\n",
        "::: {.column width=\"70%\"}\n",
        "This forecast is doing at predicting the rain:\n",
        "\n",
        "- \"10% chance of rain\" was a slight over-estimate:\n",
        "$(\\bar{y} = 0/2 = 0\\%)$\n",
        "- \"40% chance of rain\" was a slight under-estimate: $(\\bar{y} = 1/2 = 50\\%)$\n",
        "- \"70% chance of rain\" was a slight over-estimate: $(\\bar{y} = 2/3 = 67\\%)$\n",
        "- \"90% chance of rain\" was a slight under-estimate: $(\\bar{y} = 1/1 = 100\\%)$\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: footer\n",
        "Slides based on [classifier-calibration.github.io](https://classifier-calibration.github.io/)\n",
        ":::\n",
        "\n",
        "## Visualizing forecasts: the reliability diagram\n",
        "\n",
        "\\\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"30%\"}\n",
        "![](figs/model_assessment_figs/calibration_forecast_table.png){fig-align=\"center\"}\n",
        ":::\n",
        "::: {.column width=\"70%\"}\n",
        "![](figs/model_assessment_figs/calibration_forecast_reldiagram.png){fig-align=\"center\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: footer\n",
        "Slides based on [classifier-calibration.github.io](https://classifier-calibration.github.io/)\n",
        ":::\n",
        "\n",
        "## Reliability diagram - Changing values\n",
        "\n",
        "\\\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"30%\"}\n",
        "![](figs/model_assessment_figs/calibration_forecast_table_2.png){fig-align=\"center\"}\n",
        ":::\n",
        "::: {.column width=\"70%\"}\n",
        "![](figs/model_assessment_figs/calibration_forecast_reldiagram_2.png){fig-align=\"center\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: footer\n",
        "Slides based on [classifier-calibration.github.io](https://classifier-calibration.github.io/)\n",
        ":::\n",
        "\n",
        "## Reliability diagram - Changing grouping\n",
        "\n",
        "\\\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"30%\"}\n",
        "![](figs/model_assessment_figs/calibration_forecast_table_2.png){fig-align=\"center\"}\n",
        ":::\n",
        "::: {.column width=\"70%\"}\n",
        "![](figs/model_assessment_figs/calibration_forecast_reldiagram_3.png){fig-align=\"center\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: footer\n",
        "Slides based on [classifier-calibration.github.io](https://classifier-calibration.github.io/)\n",
        ":::\n",
        "\n",
        "## Reliability Diagram in sklearn\n"
      ],
      "id": "dda0c842"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "lg = LogisticRegression(random_state=0)\n",
        "nb = GaussianNB()\n",
        "\n",
        "lg.fit(X_train, y_train)\n",
        "nb.fit(X_train, y_train)\n",
        "print()"
      ],
      "id": "53cd822d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| panel: center\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "CalibrationDisplay.from_estimator(lg, X_test, y_test, n_bins=10, ax=ax,\n",
        "                                  label='Logistic Regression')\n",
        "CalibrationDisplay.from_estimator(nb, X_test, y_test, n_bins=10, ax=ax,\n",
        "                                  label='Naive Bayes')"
      ],
      "id": "8cc3181d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common sources of miscalibration\n",
        "\n",
        "* **Underconfidence:** a classifier thinks it’s worse at separating classes than it actually is.\n",
        "\n",
        "    - Underconfidence typically gives sigmoidal distortions\n",
        "    - To calibrate these means to pull predicted probabilities away from the centre\n",
        "\n",
        "*   **Overconfidence:** a classifier thinks it’s better at separating classes than it actually is\n",
        "\n",
        "    - Here, distortions are inverse-sigmoidal\n",
        "    - Calibrating these means to push predicted probabilities toward the centre\n",
        "\n",
        "A classifier can be overconfident for one class and underconfident for the other\n",
        "\n",
        "::: footer\n",
        "Slides based on [classifier-calibration.github.io](https://classifier-calibration.github.io/)\n",
        ":::\n",
        "\n",
        "## Reliability Diagram in sklearn\n",
        "\n",
        "![](figs/model_assessment_figs/sphx_glr_plot_compare_calibration_001.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "[code](https://scikit-learn.org/stable/modules/calibration.html)\n",
        ":::\n",
        "\n",
        "## Calibration metrics\n",
        "\n",
        "Let $N$ be the total of samples, $B$ the number of binds, $n^b$ the samples in bin $b$, and $conf(b)$ the average predicted probability in bin $b$.\n",
        "\n",
        "- Expected Calibration Error: \n",
        "\n",
        "$$ECE = \\sum_{b=1}^B \\frac{n^b}{N}|acc(b) - conf(b)|$$\n",
        "\n",
        "- Maximum Calibration Error:\n",
        "\n",
        "$$MCE = \\underset{m \\in \\{1,2,\\dots,|B|\\}}{\\text{max}} |acc(b) - conf(b)|$$\n",
        "\n",
        "\n",
        "## Calibration of modern models\n",
        "\n",
        "![](figs/model_assessment_figs/calibration_nn_2016.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "Image taken from Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017, July). *On calibration of modern neural networks. In International Conference on Machine Learning*. PMLR.\n",
        ":::\n",
        "\n",
        "## Calibration of modern models\n",
        "\n",
        "![](figs/model_assessment_figs/calibration_nn_2021_left.png){.absolute top=\"180\" left=\"0\" width=\"330\" height=\"330\"}\n",
        "\n",
        "![](figs/model_assessment_figs/calibration_nn_2021_right.png){.absolute top=\"170\" left=\"330\" width=\"800\" height=\"350\"}\n",
        "\n",
        ":::footer\n",
        "Image taken from Minderer, Matthias, et al. (2021). *Revisiting the calibration of modern neural networks*. Advances in Neural Information Processing Systems.\n",
        ":::\n",
        "\n",
        "## Hyperparameters of reliability diagrams\n",
        "\n",
        "\\\n",
        "\n",
        "![](figs/model_assessment_figs/calibrate_reliability_hyperparameters.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "Image taken from Xenopoulos, P., Rulff, J., Nonato, L. G., Barr, B., & Silva, C. (2022). *Calibrate: Interactive analysis of probabilistic model output*. IEEE Transactions on Visualization and Computer Graphics.\n",
        ":::\n",
        "\n",
        ":::notes\n",
        "Reliability diagrams consider a few parameters, namely, the number of bins you show and how you create these bins, which is often referred to as the binning strategy. We can see here that these choices have significant effects on what we see. One one hand, the quantile strategy would suggest the model here is calibrated. But the uniform strategy shows regions of miscalibration. Which parameters do we choose?\n",
        ":::\n",
        "\n",
        "## Calibration Techniques\n",
        "\n",
        "**Parametric** calibration involves modelling the score distributions within each class\n",
        "\n",
        "- **Platt scaling:** Logistic calibration can be derived by assuming that the scores within both classes are normally distributed with the same variance (Platt, 2000)\n",
        "\n",
        "- **Beta calibration:** employs Beta distributions instead, to deal with\n",
        "scores already on a [0, 1] scale (Kull et al., 2017)\n",
        "\n",
        "- **Dirichlet calibration** for more than two classes (Kull et al., 2019)\n",
        "\n",
        "**Non-parametric** calibration often ignores scores and employs ranks\n",
        "\n",
        "- **Isotonic regression** fits a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function\n",
        "\n",
        "::: footer\n",
        "Slides based on [classifier-calibration.github.io](https://classifier-calibration.github.io/)\n",
        ":::\n",
        "\n",
        "\n",
        "## Platt scaling\n",
        "\n",
        "* Assumes the calibration curve can be corrected by applying a sigmoid to the raw predictions\n",
        "\n",
        "$$p(y_i = 1 | f_i) = \\frac{1}{1 + exp(Af_i + B)}$$\n",
        "\n",
        "\n",
        "* Works best if the calibration error is symmetrical (classifier output for each binary class is normally distributed with the same variance) \n",
        "\n",
        "* This can be a problem for highly imbalanced classification problems, where outputs do not have equal variance\n",
        "\n",
        "* In general this method is most effective when the un-calibrated model is under-confident and has similar calibration errors for both high and low outputs\n",
        "\n",
        "## Isotonic regression\n",
        "\n",
        "* Fits a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function\n",
        "\n",
        "* Isotonic regression is more general when compared to Platt scaling, as the only restriction is that the mapping function is monotonically increasing\n",
        "\n",
        "* Is more powerful as it can correct any monotonic distortion of the un-calibrated model\n",
        "\n",
        "* However, it is more prone to overfitting, especially on small datasets\n",
        "\n",
        "## Calibration in sklearn\n",
        "\n",
        "![](figs/model_assessment_figs/sphx_glr_plot_calibration_curve_001.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "[code](https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html)\n",
        ":::\n",
        "\n",
        "## Calibration in sklearn\n",
        "\n",
        "![](figs/model_assessment_figs/sphx_glr_plot_calibration_curve_002.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "[code](https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html)\n",
        ":::\n",
        "\n",
        "## Visualization Calibration for Multi-Class Problems\n",
        "\n",
        "![](figs/model_assessment_figs/calibration_multiclass.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "Image taken from Vaicenavicius, Juozas, et al. *Evaluating model calibration in classification*. The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.\n",
        ":::\n",
        "\n",
        "## Calibrate (2023)\n",
        "\n",
        "\n",
        "![](figs/model_assessment_figs/calibrate_teaser.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "Xenopoulos, P., Rulff, J., Nonato, L. G., Barr, B., & Silva, C. (2022). *Calibrate: Interactive analysis of probabilistic model output*. IEEE Transactions on Visualization and Computer Graphics.\n",
        ":::\n",
        "\n",
        "## Smooth ECE (2023)\n",
        "\n",
        "\\\n",
        "\n",
        "![](figs/model_assessment_figs/smoothece_teaser.png){fig-align=\"center\"}\n",
        "\n",
        ":::footer\n",
        "Błasiok, J., & Nakkiran, P. (2023). *Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing*. arXiv preprint arXiv:2309.12236.\n",
        ":::\n",
        "\n",
        "## Suggested Calibration Literature\n",
        "\n",
        "* Niculescu-Mizil, A., & Caruana, R. (2005, August). [Predicting good probabilities with supervised learning](https://dl.acm.org/doi/pdf/10.1145/1102351.1102430?casa_token=o_8UMED_0fIAAAAA:MIq2GzQTPT0f-aWNDSijbcnzJN1riBdGqjq9FGn-wOZ188AOtXbTqRPkc9PuQGKSFIo5b4fM8-ItjQ). In Proceedings of the 22nd international conference on Machine learning (pp. 625-632).\n",
        "\n",
        "* Nixon, J., Dusenberry, M. W., Zhang, L., Jerfel, G., & Tran, D. (2019, June). [Measuring Calibration in Deep Learning](http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Nixon_Measuring_Calibration_in_Deep_Learning_CVPRW_2019_paper.pdf). In CVPR Workshops (Vol. 2, No. 7).\n",
        "\n",
        "* Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017, July). [On calibration of modern neural networks](http://proceedings.mlr.press/v70/guo17a/guo17a.pdf). In International Conference on Machine Learning (pp. 1321-1330). PMLR.\n",
        "\n",
        "* Vaicenavicius, J., Widmann, D., Andersson, C., Lindsten, F., Roll, J., & Schön, T. (2019, April). [Evaluating model calibration in classification](http://proceedings.mlr.press/v89/vaicenavicius19a/vaicenavicius19a.pdf). In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 3459-3467). PMLR.\n",
        "\n",
        "* Kull, M., & Flach, P. (2015, September). [Novel decompositions of proper scoring rules for classification: Score adjustment as precursor to calibration](https://link.springer.com/content/pdf/10.1007/978-3-319-23528-8_5.pdf). In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 68-85). Springer, Cham.\n",
        "\n",
        "* [ECML/PKDD 2020 Tutorial: Evaluation metrics and proper scoring rules](https://classifier-calibration.github.io/assets/slides/clacal_tutorial_ecmlpkdd_2020_evaluation.pdf)\n",
        "\n",
        "* [Google Colab notebook for calibration curves](https://colab.research.google.com/drive/1mqDVJICMBg2eoIr2VPaDjQFUzlDT3grc?usp=sharing)"
      ],
      "id": "86356b01"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}