---
title: "Visualization for Machine Learning"
subtitle: "Spring 2024"
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: figs/vida.jpg
    footer: <https://cds.nyu.edu>
    fontsize: 24pt
resources:
  - model_assessment.pdf
---

# Black Box Model Assessment

## Agenda

\

### Goal: Study Model Agnostic Interpretability Methods. These should help to explain any type of ML Models.

1. Partial Dependence Plot (PDP)

2. Local Interpretable Model-agnostic Explanations (LIME)

3. SHAP (SHapley Additive exPlanations)


Examples and materials from Molnar’s book: 
           https://christophm.github.io/interpretable-ml-book/


## Bike Rentals (Regression) 

::: {.r-fit-text}
This dataset contains daily counts of rented bicycles from the bicycle rental company Capital-Bikeshare in Washington D.C., along with weather and seasonal information. The goal is to predict how many bikes will be rented depending on the weather and the day. The data can be downloaded from the UCI Machine Learning Repository.

Here is the list of features used in Molnar's book:

- Count of bicycles including both casual and registered users. The count is used as the target in the regression task.
- The season, either spring, summer, fall or winter.
- Indicator whether the day was a holiday or not.
- The year, either 2011 or 2012.
- Number of days since the 01.01.2011 (the first day in the dataset). This feature was introduced to take account of the trend over time.
- Indicator whether the day was a working day or weekend.
- The weather situation on that day. One of:
clear, few clouds, partly cloudy, cloudy
mist + clouds, mist + broken clouds, mist + few clouds, mist
light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds
heavy rain + ice pallets + thunderstorm + mist, snow + mist
- Temperature in degrees Celsius.
- Relative humidity in percent (0 to 100).
- Wind speed in km per hour.
:::

## Partial Dependence Plot (PDP)

Shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 2001).

![](figs/bike-use-temperature.jpg)

## Partial Dependence Plot (PDP)

High level idea: marginalizing the machine learning model output over the distributions of the all other features to show the relationship between the feature we are interested in and the predicted outcome.

::: {.r-stack}
![](figs/pdp-feature1.jpg){.fragment width="950" height="450"}

![](figs/pdp-feature2.jpg){.fragment width="950" height="450"}

![](figs/pdp-feature3.jpg){.fragment width="950" height="450"}

![](figs/pdp-feature4.jpg){.fragment width="950" height="450"}

![](figs/pdp-feature5.jpg){.fragment width="950" height="450"}

![](figs/pdp-feature6.jpg){.fragment width="950" height="450"}
:::

## Partial Dependence Plot (PDP)

::: {.fragment}
**Pros**

- Intuitive
- Interpretation is clear
- Easy to implement

:::

::: {.fragment}
**Cons**

- Assume independence among features
- Can only show few features
- Hidden heterogeneous effects from averaging
:::

## Local Interpretable Model-agnostic Explanations (LIME)

Training local surrograte models to explain *individual* predictions

:::: {.columns}

::: {.column width="40%"}
![](figs/lime-global-decision-boundaries.jpg)
:::

::: {.column width="60%"}
![](figs/lime-paper.jpg)
:::

::::

https://arxiv.org/pdf/1602.04938.pdf

## Local Interpretable Model-agnostic Explanations (LIME)

::: {.r-fit-text}

The idea is quite intuitive. 

- First, forget about the training data and imagine you only have the black box model where you can input data points and get the predictions of the model. You can probe the box as often as you want. Your goal is to understand why the machine learning model made a certain prediction. LIME tests what happens to the predictions when you give variations of your data into the machine learning model. 

- LIME generates a new dataset consisting of perturbed samples and the corresponding predictions of the black box model. 

- On this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest. The interpretable model can be anything from the interpretable models chapter, for example Lasso or a decision tree. The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation. This kind of accuracy is also called *local fidelity*.
:::

https://christophm.github.io/interpretable-ml-book/

## Local Interpretable Model-agnostic Explanations (LIME)

![](figs/lime-equations.jpg)

https://arxiv.org/pdf/1602.04938.pdf

## Local Interpretable Model-agnostic Explanations (LIME)

### Algorithm

1. Pick an input that you want an explanation for.
2. Sample the neighbors of the selected input (i.e. perturbation).
3. Train a linear classifier on the neighbors.
4. The weights on the linear classifier is the explanation.

## Local Interpretable Model-agnostic Explanations (LIME)

:::: {.columns}

::: {.column width="60%"}
![](figs/lime-random-forest-model.jpg)
:::

::: {.column width="40%"}
Random forest predictions given features x1 and x2. 

Predicted classes: 1 (dark) or 0 (light).
:::

::::


## Local Interpretable Model-agnostic Explanations (LIME)

:::: {.columns}

::: {.column width="60%"}
![](figs/lime-random-forest-sampling.jpg)
:::

::: {.column width="40%"}
Instance of interest (big yellow dot) and data sampled from a normal distribution (small dots).
:::

::::


## Local Interpretable Model-agnostic Explanations (LIME)

:::: {.columns}

::: {.column width="60%"}
![](figs/lime-random-forest-weighting.jpg)
:::

::: {.column width="40%"}
Assign higher weight to points near the instance of interest.
I.e., $weight(p) = \sqrt{\frac{eˆ{-dˆ2}}{wˆ2}}$ 
where $d$ is the distance between $p$  and the
instantce of interest, and $w$ is the kernel width (self-defined).
EQUATION WRONG! (need to figure out how to do this in Markdown)
:::

::::


## Local Interpretable Model-agnostic Explanations (LIME)

:::: {.columns}

::: {.column width="60%"}
![](figs/lime-random-forest-line.jpg)
:::

::: {.column width="40%"}
Use both the samples and sample weights to train a linear classifier. 

Signs of the grid show the classifications of the locally learned model from the weighted samples. The red line marks the decision boundary (P(class=1) = 0.5).

The official implementation uses a Ridge Classifier as the linear model for explanation.
:::

::::


## Local Interpretable Model-agnostic Explanations (LIME)

::: {.r-fit-text}

:::: {.columns}

::: {.column width="40%"}
Let us look at a concrete example. We go back to the bike rental data and turn the prediction problem into a classification: After taking into account the trend that the bicycle rental has become more popular over time, we want to know on a certain day whether the number of bicycles rented will be above or below the trend line. You can also interpret “above” as being above the average number of bicycles, but adjusted for the trend.

First we train a random forest with 100 trees on the classification task. On what day will the number of rental bikes be above the trend-free average, based on weather and calendar information?

The explanations are created with 2 features. The results of the sparse local linear models trained for two instances with different predicted classes:
:::

::: {.column width="60%"}
![](figs/lime-example-result.jpg)
:::

::::

:::

## Local Interpretable Model-agnostic Explanations (LIME)


::: {.fragment}
**Pros**

- Explanations are short (= selective) and possibly contrastive.
  * we can control the sparsity of weight coefficients in the regressions method.
- Very easy to use.

:::

::: {.fragment}
**Cons**

- Unstable results due to sampling.
- Hard to weight similar neighbors in a high dimensional dataset.
- Many parameters for data scientists to hide biases.
:::

## SHAP (SHapley Additive exPlanations)

