{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Visualization for Machine Learning\"\n",
        "subtitle: \"Spring 2024\"\n",
        "format:\n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    chalkboard: \n",
        "      buttons: false\n",
        "    preview-links: auto\n",
        "    logo: figs/vida.jpg\n",
        "    footer: <https://cds.nyu.edu>\n",
        "    fontsize: 24pt\n",
        "resources:\n",
        "  - model_assessment.pdf\n",
        "---"
      ],
      "id": "7e33d4a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agenda\n",
        "\n",
        "\\\n",
        "\n",
        "1. Clustering\n",
        "2. Dimensionality Reduction\n",
        "\n",
        "<!-- Examples and materials from... -->\n",
        "\n",
        "## Clustering\n",
        "\n",
        "Etienne Bernard: \"... the goal of clustering is to separate a set of examples into groups called clusters\"\n",
        "\n",
        "![](figs/iris.jpg)\n",
        "\n",
        "## IRIS\n",
        "\n",
        "``` python\n",
        "# Code source: GaÃ«l Varoquaux\n",
        "# Modified for documentation by Jaques Grobler\n",
        "# License: BSD 3 clause\n",
        "#\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "_, ax = plt.subplots()\n",
        "scatter = ax.scatter(iris.data[:, 2], iris.data[:, 1])\n",
        "ax.set(xlabel=iris.feature_names[2], ylabel=iris.feature_names[1])\n",
        "_ = ax.legend(\n",
        "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
        ")\n",
        "```\n",
        "\n",
        "## IRIS\n"
      ],
      "id": "31d94ec1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "_, ax = plt.subplots()\n",
        "scatter = ax.scatter(iris.data[:, 2], iris.data[:, 1])\n",
        "ax.set(xlabel=iris.feature_names[2], ylabel=iris.feature_names[1])\n",
        "_ = ax.legend(\n",
        "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
        ")"
      ],
      "id": "0bcfdde3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IRIS -- another look (Bernard)\n",
        "\n",
        "![](figs/iris-bernard.jpg)\n",
        "\n",
        "## IRIS -- clustering\n",
        "\n",
        "![](figs/iris-bernard-clustering.jpg)\n",
        "\n",
        "## IRIS -- k-means\n",
        "\n",
        "![](figs/iris-bernard-clustering-kmeans.jpg)\n",
        "\n",
        "## Wolfram Mathematica FindClusters\n",
        "\n",
        "![](figs/findclusters-methods.jpg)\n",
        "\n",
        "## Wolfram Mathematica FindClusters\n",
        "\n",
        "![](figs/findclusters-methods-examples.jpg)\n",
        "\n",
        "\n",
        "## IRIS - classes\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "8cf94e74"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "_, ax = plt.subplots()\n",
        "scatter = ax.scatter(iris.data[:, 2], iris.data[:, 1], c=iris.target)\n",
        "ax.set(xlabel=iris.feature_names[2], ylabel=iris.feature_names[1])\n",
        "_ = ax.legend(\n",
        "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
        ")"
      ],
      "id": "6bd422a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/iris-bernard-clustering-kmeans-crop.jpg)\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Recommended reading\n",
        "\n",
        "* **Required** https://www.wolfram.com/language/introduction-machine-learning/clustering/ [link](https://www.wolfram.com/language/introduction-machine-learning/clustering/)\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Cluster_analysis\n",
        "\n",
        "* https://en.wikipedia.org/wiki/K-means_clustering\n",
        "\n",
        "* https://en.wikipedia.org/wiki/DBSCAN\n",
        "\n",
        "\n",
        "## Dimensionality Reduction\n",
        "\n",
        "* Input data may have thousands or millions of dimensions!\n",
        "\n",
        "* **Dimensionality Reduction** represents data with fewer dimensions\n",
        "  - easier learning -- fewer parameters\n",
        "  - visualization -- show high-dimensional data in 2D or 3D\n",
        "  - discover \"intrinsic dimensionality\" of the data\n",
        "\n",
        "## Dimensionality Reduction (Yi Zhang)\n",
        "\n",
        "* Assumption: data lies on a lower dimensional space\n",
        "\n",
        "![](figs/dm-data-is-low-dimensional.jpg)\n",
        "\n",
        "\n",
        "## Dimensionality Reduction (Bishop)\n",
        "\n",
        "* Supposed a dataset of \"3s\" perturbed in various ways\n",
        "\n",
        "![](figs/perturbed-3.jpg)\n",
        "\n",
        "* What operations did we perform? What's the intrinsic dimensionality?\n",
        "\n",
        "* Here the underlying **manifold** is **non-linear**\n",
        "\n",
        "## Digits\n",
        "\n",
        "``` python\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits(n_class=6)\n",
        "X, y = digits.data, digits.target\n",
        "```\n",
        "\n",
        "## Digits - 0\n"
      ],
      "id": "d6a146e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits(n_class=6)\n",
        "digits.images[0]"
      ],
      "id": "9c2337ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Digits - 1\n"
      ],
      "id": "8a1840c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits(n_class=6)\n",
        "digits.images[1]"
      ],
      "id": "a5536aca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Digits\n"
      ],
      "id": "4c21f60d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits(n_class=6)\n",
        "X, y = digits.data, digits.target\n",
        "n_samples, n_features = X.shape\n",
        "n_neighbors = 30\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
        "for idx, ax in enumerate(axs.ravel()):\n",
        "    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\n",
        "    ax.axis(\"off\")\n",
        "_ = fig.suptitle(\"A selection from the 64-dimensional digits dataset\", fontsize=16)"
      ],
      "id": "2c327986",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Digits"
      ],
      "id": "f62e9aba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from matplotlib import offsetbox\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def plot_embedding(X, title):\n",
        "    _, ax = plt.subplots()\n",
        "    X = MinMaxScaler().fit_transform(X)\n",
        "\n",
        "    for digit in digits.target_names:\n",
        "        ax.scatter(\n",
        "            *X[y == digit].T,\n",
        "            marker=f\"${digit}$\",\n",
        "            s=60,\n",
        "            color=plt.cm.Dark2(digit),\n",
        "            alpha=0.425,\n",
        "            zorder=2,\n",
        "        )\n",
        "    shown_images = np.array([[1.0, 1.0]])  # just something big\n",
        "    for i in range(X.shape[0]):\n",
        "        # plot every digit on the embedding\n",
        "        # show an annotation box for a group of digits\n",
        "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
        "        if np.min(dist) < 4e-3:\n",
        "            # don't show points that are too close\n",
        "            continue\n",
        "        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\n",
        "        imagebox = offsetbox.AnnotationBbox(\n",
        "            offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), X[i]\n",
        "        )\n",
        "        imagebox.set(zorder=1)\n",
        "        ax.add_artist(imagebox)\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import RandomTreesEmbedding\n",
        "from sklearn.manifold import (\n",
        "    MDS,\n",
        "    TSNE,\n",
        "    Isomap,\n",
        "    LocallyLinearEmbedding,\n",
        "    SpectralEmbedding,\n",
        ")\n",
        "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.random_projection import SparseRandomProjection\n",
        "\n",
        "embeddings = {\n",
        "    \"Random projection embedding\": SparseRandomProjection(\n",
        "        n_components=2, random_state=42\n",
        "    ),\n",
        "    \"Truncated SVD embedding\": TruncatedSVD(n_components=2),\n",
        "    \"Linear Discriminant Analysis embedding\": LinearDiscriminantAnalysis(\n",
        "        n_components=2\n",
        "    ),\n",
        "    \"Isomap embedding\": Isomap(n_neighbors=n_neighbors, n_components=2),\n",
        "    \"Standard LLE embedding\": LocallyLinearEmbedding(\n",
        "        n_neighbors=n_neighbors, n_components=2, method=\"standard\"\n",
        "    ),\n",
        "    \"Modified LLE embedding\": LocallyLinearEmbedding(\n",
        "        n_neighbors=n_neighbors, n_components=2, method=\"modified\"\n",
        "    ),\n",
        "    \"Hessian LLE embedding\": LocallyLinearEmbedding(\n",
        "        n_neighbors=n_neighbors, n_components=2, method=\"hessian\"\n",
        "    ),\n",
        "    \"LTSA LLE embedding\": LocallyLinearEmbedding(\n",
        "        n_neighbors=n_neighbors, n_components=2, method=\"ltsa\"\n",
        "    ),\n",
        "    \"MDS embedding\": MDS(n_components=2, n_init=1, max_iter=120, n_jobs=2),\n",
        "    \"Random Trees embedding\": make_pipeline(\n",
        "        RandomTreesEmbedding(n_estimators=200, max_depth=5, random_state=0),\n",
        "        TruncatedSVD(n_components=2),\n",
        "    ),\n",
        "    \"Spectral embedding\": SpectralEmbedding(\n",
        "        n_components=2, random_state=0, eigen_solver=\"arpack\"\n",
        "    ),\n",
        "    \"t-SNE embedding\": TSNE(\n",
        "        n_components=2,\n",
        "        n_iter=500,\n",
        "        n_iter_without_progress=150,\n",
        "        n_jobs=2,\n",
        "        random_state=0,\n",
        "    ),\n",
        "    \"NCA embedding\": NeighborhoodComponentsAnalysis(\n",
        "        n_components=2, init=\"pca\", random_state=0\n",
        "    ),\n",
        "}\n",
        "\n",
        "from time import time\n",
        "\n",
        "projections, timing = {}, {}\n",
        "for name, transformer in embeddings.items():\n",
        "    if name.startswith(\"Linear Discriminant Analysis\"):\n",
        "        data = X.copy()\n",
        "        data.flat[:: X.shape[1] + 1] += 0.01  # Make X invertible\n",
        "    else:\n",
        "        data = X\n",
        "\n",
        "    print(f\"Computing {name}...\")\n",
        "    start_time = time()\n",
        "    projections[name] = transformer.fit_transform(data, y)\n",
        "    timing[name] = time() - start_time\n",
        "\n",
        "for name in timing:\n",
        "  if name==\"t-SNE embedding\":\n",
        "    title = f\"{name} (time {timing[name]:.3f}s)\"\n",
        "    plot_embedding(projections[name], title)\n",
        "\n",
        "plt.show()"
      ],
      "id": "811a5dd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principal Component Analysis (Luis Gustavo Nonato)\n",
        "\n",
        "\n",
        "::: incremental\n",
        "-   PCA is directly related to the eigenvectors and eigenvalues of covariance matrices.\n",
        "-   Lets so make a quick review of eigenvectors, eigenvalues, and covariance matrices.\n",
        ":::\n",
        "\n",
        "## Eigenvectors and Eigenvalues\n",
        "\n",
        "Given a $d \\times d$ matrix $A$, a pair $(\\lambda, u)$ that satisfies\n",
        "\n",
        "$A u = \\lambda u$\n",
        "\n",
        "is called eigenvalue $\\lambda$ and corresponding eigenvector $u$ of $A$.\n",
        "\n",
        "## Symmetric Matrices\n",
        "\n",
        "- $\\lambda \\in \\mathbb{R}$ and $u \\in \\mathbb{R}^d$ (no complex numbers involved)\n",
        "- The eigenvectors are orthogonal\n",
        "\n",
        "\n",
        "![](figs/pca-spectral-decomposition.jpg)\n",
        "\n",
        "## Covariance Matrix\n",
        "\n",
        "![](figs/covariance-matrix.jpg)\n",
        "\n",
        "## Covariance Matrix\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/large-covariance.jpg)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/zero-convariance.jpg)\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Covariance Matrix\n",
        "\n",
        "![](figs/covariance-summary.jpg)\n",
        "\n",
        "## Principal Component Analysis:  intuition\n",
        "\n",
        "![](figs/pca-intuition.jpg)\n",
        "\n",
        "## Principal Component Analysis:  intuition\n",
        "\n",
        "![](figs/pca-intuition2.jpg)\n",
        "\n",
        "\n",
        "## Principal Component Analysis\n",
        "\n",
        "![](figs/pca-description.jpg)\n",
        "\n",
        "## Principal Component Analysis\n",
        "\n",
        "![](figs/pca-filtering.jpg)\n",
        "\n",
        "## PCA of digits  \n"
      ],
      "id": "8e968b11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "data, labels = load_digits(return_X_y=True)\n",
        "reduced_data=pca.fit_transform(data)\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1])\n",
        "plt.show()"
      ],
      "id": "208b8664",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PCA of digits\n"
      ],
      "id": "9de01dc5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "data, labels = load_digits(return_X_y=True)\n",
        "reduced_data=pca.fit_transform(data)\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels)\n",
        "plt.show()"
      ],
      "id": "9fb48724",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling Up\n",
        "\n",
        "* Covariance matrix can be really big!\n",
        "  - $\\Sigma$ is $n$ by $n$\n",
        "  - 10000 features are not uncommon\n",
        "  - computing eigenvectors is slow...\n",
        "\n",
        "* Solution: Singular Value Decomposition (SVD)\n",
        "  - Finds the $k$ largest eigenvectors\n",
        "  - Widely implemented robustly in major packages\n",
        "\n",
        "\n",
        "## Singular Value Decomposition (SVD)\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Singular_value_decomposition\n",
        "\n",
        "![](figs/svd.jpg)\n",
        "\n",
        "## Dimensionality Reduction Techniques\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Dimensionality_reduction\n",
        "  - Principal component analysis (PCA)\n",
        "  - Non-negative matrix factorization (NMF)\n",
        "  - Linear discriminant analysis (LDA)\n",
        "  - t-SNE\n",
        "  - UMAP\n",
        "  - **many others**\n",
        "\n",
        "## Local Linear Embedding\n",
        "\n",
        "![](figs/lle.jpg)\n",
        "\n",
        "## Preserving Local Manifold Neighborhoods\n",
        "\n",
        "![](figs/preserving-local-neighborhoods.jpg)\n",
        "\n",
        "## LLE\n",
        "\n",
        "![](figs/lle-algorithm.jpg)\n",
        "\n",
        "https://www.science.org/doi/10.1126/science.290.5500.2323\n",
        "\n",
        "\n",
        "## PCA vs LLE\n",
        "\n",
        "![](figs/pca-vs-lle.jpg){height=\"600\"}\n",
        "\n",
        "\n",
        "## Graph Layout using force based approach\n",
        "\n",
        "[video link](https://www.youtube.com/watch?v=_Oidv5M-fuw)\n",
        "\n",
        "## SNE and t-SNE\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/sne.jpg)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "![](figs/tsne.jpg)\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "\n",
        "HERE is an excellent talk by t-SNE creator: \n",
        "[video link](https://www.youtube.com/watch?v=RJVL80Gg3lA&list=UUtXKDgv1AVoG88PLl8nGXmw)\n",
        "\n",
        "\n",
        "<!-- ##\n",
        "\n",
        "{{< video https://www.youtube.com/watch?v=RJVL80Gg3lA&list=UUtXKDgv1AVoG88PLl8nGXmw >}} -->\n",
        "\n",
        "\n",
        "## \n",
        "\n",
        "![](figs/using-tsne.jpg)\n",
        "\n",
        "https://distill.pub/2016/misread-tsne/\n",
        "\n",
        "## \n",
        "\n",
        "![](figs/understanding-umap.jpg)\n",
        "\n",
        "https://pair-code.github.io/understanding-umap/\n",
        "\n",
        "\n",
        "## What about user interaction?\n",
        "\n",
        "![](figs/projections.jpg)\n"
      ],
      "id": "bd61a6d6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}