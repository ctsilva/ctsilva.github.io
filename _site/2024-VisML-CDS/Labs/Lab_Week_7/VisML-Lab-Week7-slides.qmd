

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Welcome back to the Visualization for Machine Learning Lab! - Claudio Silva</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Claudio Silva">
<meta property="og:title" content="Welcome back to the Visualization for Machine Learning Lab!">


  <link rel="canonical" href="http://localhost:4000/2024-VisML-CDS/Labs/Lab_Week_7/VisML-Lab-Week7-slides.qmd">
  <meta property="og:url" content="http://localhost:4000/2024-VisML-CDS/Labs/Lab_Week_7/VisML-Lab-Week7-slides.qmd">







  

  












  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Claudio Silva",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Claudio Silva Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="http://localhost:4000/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="http://localhost:4000/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="http://localhost:4000/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="http://localhost:4000/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="http://localhost:4000/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="http://localhost:4000/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="http://localhost:4000/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="http://localhost:4000/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://localhost:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Claudio Silva</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/team/">Research Team</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/teaching/">Teaching</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/portfolio/">Portfolio</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/cv/">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/markdown/">Guide</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/claudiosilva.jpg" class="author__avatar" alt="Claudio Silva">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Claudio Silva</h3>
    <p class="author__bio">Institute Professor, New York University</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> New York, NY</li>
      
      
      
      
        <li><a href="mailto:csilva@nyu.edu"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
        <li><a href="https://twitter.com/csilvanyc"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
      
      
        <li><a href="https://www.linkedin.com/in/claudio-silva-839634"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
      
      
        <li><a href="https://github.com/ctsilva"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=YIwiAAsAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Welcome back to the Visualization for Machine Learning Lab!">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Welcome back to the Visualization for Machine Learning Lab!
</h1>
          
        
        
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        ## Miscellaneous 

* Homework 3 has been posted and is due next Friday (3/15) at 11:59pm
* Homework 2 is being graded, grades will likely be posted next week

## Dimensionality Reduction

* Various methods of projecting data into a low-dimensional space while preserving the clusters from the high-dimensional space
    * Principal Component Analysis (PCA)
    * Multidimensional Scaling (MDS)
    * Sparse Random Projection
    * Locally Linear Embedding
    * t-Distributed Stochastic Neighbor Embedding (t-SNE)
    * Uniform Manifold Approximation and Projection (UMAP) 

## MNIST Dataset

* Dataset of 28x28 pixel images of handwritten digits

![](MNIST_nums.png)

::: footer
Ref: @Olah_2014
:::

## MNIST Dataset

<ul >
<li style="font-size:25px";>Every MNIST image can be thought of as a 28x28 array of numbers describing how dark each pixel is:
</li>
</ul>
![](MNIST_arr.png)
<ul >
<li style="font-size:25px";>We can flatten each array into a 28 * 28 = 784 dimensional vector, where each component of the vector is a value between zero and one describing the intensity of the pixel
</li>
<li style="font-size:25px";>We can think of MNIST as a collection of 784-dimensional vectors
</li>
</ul>

::: footer
Ref: @Olah_2014
:::

## MNIST Dataset

* But not all vectors in this 784-dimensional space are MNIST digits! Typical points in this space are very different 
* To get a sense of what a typical point looks like, we can randomly pick a few random 28x28 images – each pixel is randomly black, white or some shade of gray. These random points look like noise:

![](MNIST_noise.png)

::: footer
Ref: @Olah_2014
:::

## MNIST Dataset

<ul >
<li style="font-size:30px";>28x28 images that look like MNIST digits are very rare - they make up a very small subspace of 784-dimensional space 
<ul>
<li style="font-size:25px";>With some slightly harder arguments, we can see that they occupy a lower (than 748) dimensional subspace</li>
</ul>
</li>
<li style="font-size:30px";>Many theories about lower-dimensional structure of MNIST (and similar data)
  <ul>
    <li style="font-size:25px";>Manifold hypothesis (popular among ML researchers): MNIST is a low dimensional manifold curving through its high-dimensional embedding space</li>
    <li style="font-size:25px";>Another hypothesis (rooted in topological data analysis) is that data like MNIST consists of blobs with tentacle-like protrusions sticking out into the surrounding space</li>
    <li style="font-size:25px";>But no one actually knows for sure!</li>
   </ul>
</li>
</ul>

::: footer
Ref: @Olah_2014
:::

## MNIST Cube

<ul>
  <li style="font-size:30px";>Imagine the MNIST data points as points suspended in a 784-dimensional cube
  <ul>
    <li style="font-size:25px";>Each dimension of the cube corresponds to a particular pixel</li>
    <li style="font-size:25px";>The data points range from zero to one according to pixel intensity</li>
    <li style="font-size:25px";>On one side of the dimension, there are images where that pixel is white. On the other side of the dimension, there are images where it is black. In between, there are images where it is gray.</li>
   </ul>
</li>
<li style="font-size:30px";>What does this cube look like if we look at a particular two-dimensional face? Let's look at Olah's visualizations.
</li>
</ul>

![](MNIST_Cube.png)

::: footer
Ref: @Olah_2014
:::

## MNIST Cube

* What qualities would the ‘perfect’ visualization of MNIST have? What should our goal be?
* What is the best way to cluster MNIST data?
* We can try what we learned last week...

::: footer
Ref: @Olah_2014
:::

## Principal Component Analysis Recap

:::: {.columns}

::: {.column width="40%"}

<ul >
<li style="font-size:30px";>Dimensionality reduction technique that allows us to find clusters of similar data points based on many features (which we boil down to two “principal components” PC1 and PC2)</li>
<li style="font-size:30px";>PC1 and PC2 represent the directions in the data space with the highest and second-highest variances, respectively
  <ul>
    <li style="font-size:25px";>Way to bring out strong patterns from large and complex datasets</li>
   </ul>
</li>
</ul>
  
:::

::: {.column width="60%"}

![](PCA_iris.png)
:::

::::

::: footer
Ref: @Team_2018a
:::

## PCA on MNIST

Import libraries:

``` {python}
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn.datasets import load_digits
from sklearn.decomposition import TruncatedSVD
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import RandomTreesEmbedding
from sklearn.manifold import (Isomap, LocallyLinearEmbedding, MDS, SpectralEmbedding, TSNE)
from sklearn.neighbors import NeighborhoodComponentsAnalysis
from sklearn.pipeline import make_pipeline
from sklearn.random_projection import SparseRandomProjection
from sklearn.decomposition import PCA
import plotly.graph_objects as go
from umap import UMAP
```

## PCA on MNIST 

Define helper functions for visualizing clusters in 2D and 3D:

``` {python}
# Authors: Gael Varoquaux
# License: BSD 3 clause (C) INRIA 2014
def plot_clustering(X_red, labels, title=None):
    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)
    X_red = (X_red - x_min) / (x_max - x_min)

    plt.figure(figsize=(6, 6))
    for digit in digits.target_names:
        plt.scatter(
            *X_red[y == digit].T,
            marker=f"${digit}$",
            s=50,
            # c=plt.cm.nipy_spectral(labels[y == digit] / 10),
            alpha=0.5,
        )

    plt.xticks([])
    plt.yticks([])
    if title is not None:
        plt.title(title, size=17)
    plt.axis("off")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
```

## PCA on MNIST 

Define helper functions for visualizing clusters in 2D and 3D:

``` {python}
def plot_3d_clustering(X_red, labels, title=None):
    component0 = X_red[:, 0]
    component1 = X_red[:, 1]
    component2 = X_red[:, 2]
    indices =  list(range(X_red.shape[0]))
    labels = list (map(lambda x: "label:" + str(y[x]) + ", id:" + str(x),indices))

    fig = go.Figure(data=[go.Scatter3d(
        x=component0,
        y=component1,
        z=component2,
        mode='markers',
        marker=dict(
            size=10,
            color=y,                # set color to an array/list of desired values
            colorscale='Rainbow',   # choose a colorscale
            opacity=1,
            line_width=1,
        ),
        text = labels

    )])
# tight layout
    fig.update_layout(margin=dict(l=50,r=50,b=50,t=50),width=520,height=520)
    fig.layout.template = 'plotly_dark'

    fig.show()
```

## PCA on MNIST

Load the MNIST dataset:
``` {python}
digits = load_digits(n_class=6)
X, y = digits.data, digits.target
n_samples, n_features = X.shape
n_neighbors = 30

print("Samples: " + str(n_samples))
print("Features: " + str(n_features))
```

## PCA on MNIST 

Peform PCA:

``` {python}
pca = PCA(n_components=2)
pca.fit(X)
X_trans = pca.transform(X)
print("Singular values: " + str(pca.singular_values_))
```

## PCA on MNIST 

<ul style="font-size:25px";>
We see that we are able to reasonbly cluster the data by digit using PCA:
</ul>

``` {python}
plt.scatter(X_trans[:,0],X_trans[:,1])
```

## PCA on MNIST 

<ul style="font-size:25px";>
We see that we are able to reasonbly cluster the data by digit using PCA:
</ul>

``` {python}
plot_clustering(X_trans, "", "PCA")
```

## PCA on MNIST 

<ul style="font-size:25px";>
We can also visualize this in 3D:
</ul>

``` {python}
pca_3d = PCA(n_components=3)
X_trans_3d = pca_3d.fit_transform(X)
plot_3d_clustering(X_trans_3d, "", "PCA")
```

## Optimization-Based Dimensionality Reduction

<ul >
<li style="font-size:30px";>What if the distances between points in our visualization were the same as the distances between points in the original space?
<ul>
    <li style="font-size:25px";>This captures the global geometry of the data</li>
   </ul>
</li>
<li style="font-size:30px";>In this case, we say that for any two MNIST data points, x<sub>i</sub> and x<sub>j</sub>, there are two notions of distance between them: 
  <ul>
    <li style="font-size:25px";>d<sup>*</sup><sub>i,j</sub> denotes the distance between x<sub>i</sub> and x<sub>j</sub> in the original space</li>
    <li style="font-size:25px";>d<sub>i,j</sub> denotes the distance between x<sub>i</sub> and x<sub>j</sub> in our visualization.</li>
   </ul>
</li>
<li style="font-size:30px">Now we can define a cost:</li>
</ul>

![](MDS_cost.png)

::: footer
Ref: @Olah_2014
:::

## Multidimensional Scaling (MDS)

![](MDS_cost.png)

* If these distances are similar, the cost is low (and vice versa). So a cost of zero is optimal (though we can never actually reach this)
* We can solve this as an optimization problem - we start with a random point and apply gradient descent 
* This is called multidimensional scaling (MDS) 

::: footer
Ref: @Olah_2014
:::

## MDS on MNIST

``` {python}
mds = MDS(n_components=2, n_init=1, max_iter=120, n_jobs=2)
X_mds = mds.fit_transform(X)
plot_clustering(X_mds, "", "MDS")
```

## MDS on MNIST

``` {python}
mds_3d = MDS(n_components=3, n_init=1, max_iter=120, n_jobs=2)
mds_3d.fit(X)
X_mds_3d = mds_3d.fit_transform(X)
plot_3d_clustering(X_mds_3d, "", "MDS")
```

## Sammon's Mapping

<ul >
<li style="font-size:30px";>There are many variations of MDS
  <ul>
    <li style="font-size:25px";>Common theme is to use a cost function that emphasizes local structure as more important to maintain than global structure</li>
    <li style="font-size:25px";>One version of this is Sammon's Mapping, which uses the cost function:</li>
   </ul>
</li>
</ul>
![](Sammons_Mapping_Cost.png)
<ul >
<li style="font-size:30px";>In Sammon’s mapping, we try harder to preserve the distances between nearby points than between those which are far apart. If two points are twice as close in the original space as two others, it is twice as important to maintain the distance between them.</li>
</ul>

::: footer
Ref: @Olah_2014
:::

## Sparse Random Projection

<ul >
<li style="font-size:30px";>Reduces the dimensionality by projecting the original input space using a sparse random matrix
  <ul>
    <li style="font-size:25px";>Alternative to dense Gaussian random projection matrix - guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data</li>
   </ul>
</li>
<li style="font-size:30px";>If we define s = 1 / density, the elements of the random matrix are drawn from</li>
</ul>
![](sparse_random_projection.png){width=50%}
<ul style="font-size:25px"> where n<sub>components</sub> is the size of the projected subspace. 
<li style="font-size:30px";>By default the density of nonzero elements is set to the minimum density as recommended by Ping Li et al: 1 / <SQRT>(n<sub>features</sub>)</SQRT></li>
</ul>

::: footer
Ref: @scikit
:::

## Sparse Random Projection on MNIST 

``` {python}
srp=SparseRandomProjection(n_components=2, random_state=42)
X_srp=srp.fit_transform(X)
plot_clustering(X_srp, "", "Sparse Random Projection")
```

## Sparse Random Projection on MNIST 

``` {python}
srp_3d=SparseRandomProjection(n_components=3, random_state=42)
X_srp_3d=srp_3d.fit(X)
X_srp_3d=srp_3d.fit_transform(X)
plot_3d_clustering(X_srp_3d, "", "Sparse Random Projection")
```

## Locally Linear Embedding

* First we find the k-nearest neighbors of each data point
  * One advantage of the LLE algorithm is that there is only one parameter to tune (K). If K is chosen to be too small or too large, it will not be able to accomodate the geometry of the original data. Here, for each data point that we have we compute the K nearest neighbours.
* Next, we approximate each data vector as a weighted linear combination of its k-nearest neighbors to construct new points. We try to minimize the cost function, where j’th nearest neighbour for point X<sub>i</sub>.

![](LLE_1.png){width=50%}

::: footer
Ref: @Mihir_2024
:::

## Locally Linear Embedding

* Finally, it computes the weights that best reconstruct the vectors from its neighbors, then produce the low-dimensional vectors best reconstructed by these weights. 
  * In other words, we define the new vector space Y such that we minimize the cost for Y as the new points:

::: footer
Ref: @Mihir_2024
:::

## Locally Linear Embedding

![](LLE_2.png)

::: footer
Ref: @Mihir_2024
:::

## LLE on MNIST

``` {python}
lle=LocallyLinearEmbedding(n_neighbors=5, n_components=2, method="standard")
X_lle=lle.fit(X)
X_lle=lle.fit_transform(X)
plot_clustering(X_lle, "", "LLE")
```

## LLE on MNIST

``` {python}
lle_3d=LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=3, method="standard")
X_lle_3d=lle_3d.fit(X)
X_lle_3d=lle_3d.fit_transform(X)
plot_3d_clustering(X_lle_3d, "", "LLE")
```

## t-Distributed Stochastic Neighbor Embedding (t-SNE)

* Very popular for deep learning
* Tries to optimize for preserving the topology of the data
* For every point, t-SNE constructs a notion of which other points are its ‘neighbors,’ trying to make all points have the same number of neighbors. Then it tries to embed them so that those points all have the same number of neighbors.

::: footer
Ref: @Olah_2014
:::

## t-SNE



::: footer
Ref: @Starmer_2017a
:::

## t-SNE on MNIST

``` {python}
tsne = sklearn.manifold.TSNE()
X_tsne = tsne.fit_transform(X)
plot_clustering(X_tsne, "", "T-SNE")
```

## t-SNE on MNIST

``` {python}
tsne_3d = sklearn.manifold.TSNE(3)
X_tsne_3d = tsne_3d.fit_transform(X)
plot_3d_clustering(X_tsne_3d, "", "T-SNE")
```

## Using t-SNE Effectively
* t-SNE plots are popular and can be useful, but only if you avoid common misreadings
  * Beware of hyperparameter values
  * Remember cluster sizes in a t-SNE plot mean nothing
  * Know that distances BETWEEN clusters may not mean anything either
  * Random noise doesn't always look random
  * You may need multiple plots for topology
* See @Wattenberg_Viégas_Johnson_2016 for more details

::: footer
Ref: @Wattenberg_Viégas_Johnson_2016
:::

## Uniform Manifold Approximation and Projection (UMAP)

* Similar to t-SNE, but with some advantages:
  * Increased speed 
  * Better preservation of the data's global structure

::: footer
Ref: @Coenen_Pearce
:::

## Uniform Manifold Approximation and Projection (UMAP)
<ul >
<li style="font-size:30px";>UMAP constructs a high dimensional graph representation of the data then optimizes a low-dimensional graph to be as structurally similar as possible
  <ul>
    <li style="font-size:25px";>Starts with "fuzzy simplicial complex," a representation of a weighted graph with edge weights representing the likelihood that two points are connected.</li>
    <li style="font-size:25px";>To determine connectedness, UMAP extends a radius outwards from each point, connecting points when those radii overlap.</li>
    <li style="font-size:25px";>Choice of radius matters! Too small = small, isolated clusters. Too large = everything is connected. UMAP overcomes this challenge by choosing a radius locally, based on the distance to each point's n<sup>th</sup> nearest neighbor.</li>
    <li style="font-size:25px";>The graph is made "fuzzy" by decreasing the likelihood of connection as the radius grows</li>
    <li style="font-size:25px";>By stipulating that each point must be connected to at least its closest neighbor, UMAP ensures that local structure is preserved in balance with global structure</li>
   </ul>
</li>
</ul>

::: footer
Ref: @Coenen_Pearce
:::

## UMAP on MNIST

``` {python}
ump=UMAP()
ump.fit(X)
X_umap=ump.fit_transform(X)
plot_clustering(X_umap, "not used", "UMAP")
```

## References (Good for Further Reading)


        
      </section>

      <footer class="page__meta">
        
        




      </footer>

      

      


    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/ctsilva"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Claudio Silva. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

