---
title: "Deep Learning Visualization"
subtitle: "CS-GY 9223 - Fall 2025"
author: "Claudio Silva"
institute: "NYU Tandon School of Engineering"
date: "October 14, 2025"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: c/t
    show-slide-number: all
    hash-type: number
    logo: figs/vida.jpg
    width: 1920
    height: 1080
    preview-links: auto
    transition: fade
    transition-speed: fast
    footer: <https://engineering.nyu.edu>
    fontsize: 24pt
resources:
  - tda.pdf
---

## Deep Learning

- We will start by reviewing concepts of Deep Learning.

## Deep Learning

:::: {.columns}
::: {.column width="50%"}
Understanding Deep Learning by Simon J.D. Prince.
Published by MIT Press, 2023.

https://udlbook.github.io/udlbook
:::
::: {.column width="50%"}
![](figs/UDLCoverSmall.jpg){fig-align="center"}
:::
::::

## Deep Learning Terminology

- Inference: $y = f[x, \Phi]$

- $y$: prediction

- $x$: input 

- $\Phi$: model parameters

- We "learn" the parameters from pairs of "training data" $\{x_i, y_i\}$

- We quantify the accuracy by using a (scalar) loss function $L[\Phi]$. The smaller the loss, the better our model "fits" the data.

- To check the "generalization" of the model, we run the model on "test data", which is separate from the training data.

## 1-D linear regression model

- $y = f[x, \Phi] = \Phi_0 + \Phi_1 x$

![](figs/UDLChap2PDF/SupervisedLinear.jpg){fig-align="center"}

## 1-D linear regression model: Loss

![](figs/UDLChap2PDF/SupervisedLinearFitError.jpg){fig-align="center"}

## 1-D linear regression model: Loss 

![](figs/UDLChap2PDF/SupervisedLinearFitError.jpg){fig-align="center"}

## 1-D linear regression model: Loss Surface

![](figs/UDLChap2PDF/SupervisedSurface.jpg){fig-align="center"}

## 1-D linear regression model: Optimization

![](figs/UDLChap2PDF/SupervisedOpt.jpg){fig-align="center"}

## Shallow neural networks

- $y = f[x, \Phi] = \Phi_0 \\
   + \Phi_1 a[ \Theta_{10} + \Theta_{11} x] \\
   + \Phi_2 a[ \Theta_{20} + \Theta_{21} x] \\
   + \Phi_3 a[ \Theta_{30} + \Theta_{31} x]$

- We now have 10 parameters
- And also, an "activation" function $a[]$

## Shallow neural networks: Activation function ReLU

![](figs/UDLChap3PDF/ShallowReLU.jpg){fig-align="center"}

## Shallow neural networks: Neural network intuition

![](figs/UDLChap3PDF/ShallowFunctions.jpg){fig-align="center"}


## Shallow neural networks: Neural network intuition

![](figs/UDLChap3PDF/ShallowBuildUp.jpg){fig-align="center"}

## Shallow neural networks: Neural network intuition

![](figs/UDLChap3PDF/ShallowNet.jpg){fig-align="center"}

## Shallow neural networks: Universal Approximation Theorem

![](figs/UDLChap3PDF/ShallowApproximate.jpg){fig-align="center"}

## Shallow neural networks: Terminology

- Neural networks is composed of "layers"
- "input" layer, "hidden" layers, "output layer"
- Hidden units are called "neurons"
- As data passes through, the values are called "pre-activation" and "activations"

## Deep neural networks: Composing multiple networks

![](figs/UDLChap4PDF/DeepConcat.jpg){fig-align="center"}

## Deep neural networks: Folding Input Space

![](figs/UDLChap4PDF/DeepFold.jpg){fig-align="center"}

## Deep neural networks: Two hidden layers

![](figs/UDLChap4PDF/DeepTwoLayer.jpg){fig-align="center"}

## Deep neural networks: Multiple hidden layers

![](figs/UDLChap4PDF/DeepKLayer.jpg){fig-align="center"}


## https://playground.tensorflow.org

![](figs/tensorflow-playground.jpg){fig-align="center"}

## https://poloclub.github.io/cnn-explainer/

![](figs/cnnexplainer.jpg){fig-align="center"}

## Further References

- https://distill.pub/2020/grand-tour/

- http://projector.tensorflow.org/

- https://ml4a.github.io/ml4a/looking_inside_neural_nets/

