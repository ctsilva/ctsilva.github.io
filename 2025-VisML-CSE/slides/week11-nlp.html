<!DOCTYPE html>
<html lang="en"><head>
<script src="week11-nlp_files/libs/clipboard/clipboard.min.js"></script>
<script src="week11-nlp_files/libs/quarto-html/tabby.min.js"></script>
<script src="week11-nlp_files/libs/quarto-html/popper.min.js"></script>
<script src="week11-nlp_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="week11-nlp_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="week11-nlp_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="week11-nlp_files/libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Claudio Silva">
  <meta name="dcterms.date" content="2025-11-03">
  <title>Visualization for NLP and LLMs</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="week11-nlp_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="week11-nlp_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="week11-nlp_files/libs/revealjs/dist/theme/quarto-743137726eb562984e8d4ff610b648a8.css">
  <link rel="stylesheet" href="lab-light-theme.css">
  <link href="week11-nlp_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="week11-nlp_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="week11-nlp_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="week11-nlp_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section class="quarto-title-block center">
  <h1 class="title">Visualization for NLP and LLMs</h1>
  <p class="subtitle">CS-GY 9223 - Fall 2025</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Claudio Silva 
</div>
        <p class="quarto-title-affiliation">
            NYU Tandon School of Engineering
          </p>
    </div>
</div>

  <p class="date">2025-11-03</p>
</section>
<section>
<section class="title-slide slide level1 center">
<h1>NLP and Large Language Models</h1>

</section>
<section class="slide level2">
<h2>Today’s Agenda</h2>
<ol type="1">
<li><strong>Natural Language Processing (NLP) basics</strong>
<ul>
<li>Tasks and challenges</li>
<li>Analysis and representation</li>
<li>General resources</li>
<li>Sparse and dense text representation</li>
<li>Neural Network recap.</li>
</ul></li>
<li><span style="color: grey;">Visualization for NLP
<ul>
<li>General Text Visualization</li>
<li>Model agnostic explanation</li>
<li>Recurrent Neural Network (RNN) Visualization</li>
<li>Transformers (LLM) Visualization</li>
</ul></span></li>
</ol>
<aside class="notes">
<p>Natural Language Processing has undergone a revolution with the advent of large language models. Today’s lecture bridges traditional NLP visualization techniques with modern LLM analysis methods. We’ll start with fundamental text processing concepts, then explore how visualization helps us understand both classical models like RNNs and modern transformer-based architectures. The transition from traditional NLP to LLMs represents not just a scale change but a fundamental shift in how we approach language understanding. Visualization plays a crucial role in making these black-box models more interpretable.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>NLP basics: Tasks and challenges</h2>

<img data-src="figs/week11-nlp/slide1.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<p>The NLP pipeline begins with raw text and transforms it through multiple stages into a form suitable for machine learning. Tokenization splits text into units (words, subwords, or characters). Normalization handles case, punctuation, and special characters. Vectorization converts tokens to numbers - from simple one-hot encoding to sophisticated contextual embeddings. Each stage involves design decisions that significantly impact downstream performance. For example, aggressive normalization might remove important signals (like capitalization indicating proper nouns), while preserving too much variation increases vocabulary size and sparsity.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>NLP basics: Tasks and challenges</h2>

<img data-src="figs/week11-nlp/slide2.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<p>Word embeddings revolutionized NLP by providing dense, continuous representations where similar words have similar vectors. The key insight is the distributional hypothesis: words appearing in similar contexts have similar meanings. Word2Vec uses shallow neural networks to predict context words (CBOW) or target words (Skip-gram). GloVe combines global matrix factorization with local context windows. FastText extends Word2Vec by using character n-grams, helping with out-of-vocabulary words. These embeddings capture surprising regularities - the famous “king - man + woman = queen” demonstrates that vector arithmetic can encode analogical reasoning.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>NLP basics: Tasks and challenges</h2>
<div id="tbl-tasks" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-tasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: NLP can be divided into tasks related to acquisition, analysis, representation, etc.
</figcaption>
<div aria-describedby="tbl-tasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top">
<thead>
<tr class="header">
<th>acquisition</th>
<th>structure</th>
<th>meaning</th>
<th>representation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sound wave</td>
<td>phonetics/phonology</td>
<td>semantics</td>
<td>bag-of-words</td>
</tr>
<tr class="even">
<td>text corpus</td>
<td>morphology</td>
<td>pragmatics</td>
<td>n-gram</td>
</tr>
<tr class="odd">
<td></td>
<td>syntax</td>
<td>discourse</td>
<td>word2vec</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<aside class="notes">
<p>sound wave (signal captured by micropones, mixing consoles, audio interfaces) text corpus ()</p>
<p>phonetics/phonology (knowledge about linguistic sounds) morphology (knowledge of the meaningful components of words)<br>
syntax (knowledge of the structural relationships between word)</p>
<p>semantics (knowledge of meaning) pragmatics (knowledge of the relationship of meaning to the goals and intentions of the speaker) discourse (knowledge about linguistic units larger than a single utterance)</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>NLP basics: Tasks and challenges</h2>
<aside class="notes">
<p>NLP tasks span from low-level syntactic analysis to high-level semantic understanding and generation. Token-level tasks like POS tagging and NER identify properties of individual words. Sequence-level tasks like machine translation and summarization transform entire texts. Classification tasks like sentiment analysis and topic modeling categorize documents. Generation tasks create new text. This taxonomy matters for visualization because different tasks benefit from different visual representations. Token-level tasks use attention heatmaps and dependency graphs. Document-level tasks use embedding projections and clustering visualizations. Understanding task requirements helps select appropriate models and interpretability methods.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="columns">
<div class="column" style="width:50%;">
<h3>Word &amp; Morphosyntactic Level</h3>
<ul>
<li>Named entity recognition</li>
<li>Parts-of-speech tagging</li>
<li>Dependency parsing</li>
<li>Grammatical error correction</li>
<li>Word sense disambiguation</li>
<li>Coreference resolution</li>
</ul>
</div><div class="column" style="width:50%;">
<h3>Document &amp; Semantic Level</h3>
<ul>
<li>Text summarization</li>
<li>Question answering</li>
<li>Machine translation</li>
<li>Sentiment analysis</li>
<li>Topic modeling</li>
<li>Dialogue systems</li>
</ul>
</div></div>
</section>
<section class="slide level2">
<h2>NLP basics: Analysis and representation</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ol type="1">
<li><strong>Lexical and morphological analysis</strong>
<ul>
<li>Finite-state morphological parsers</li>
</ul></li>
<li><span style="color: grey;">Syntactic recognition and representation
<ul>
<li>Shallow parser or chunker</li>
<li>Context-free Grammar</li>
</ul></span></li>
<li><span style="color: grey;">Morphosyntactic analysis
<ul>
<li>Part-of-Speech (POS) tagging</li>
</ul></span></li>
<li><span style="color: grey;">Representing Meaning
<ul>
<li>First-order logic</li>
<li>Semantic Network</li>
<li>Conceptual Dependency Diagram</li>
<li>Frame-based approach</li>
</ul></span></li>
</ol>
</div><div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="figs/week11-nlp/morph_parser.png"></p>
<figcaption><a href="https://web.stanford.edu/~jurafsky/slp3/">Jurafsky, 2025</a></figcaption>
</figure>
</div>
</div></div>
</section>
<section class="slide level2">
<h2>NLP basics: Analysis and representation</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ol type="1">
<li><span style="color: grey;">Lexical and morphological analysis
<ul>
<li>Finite-state morphological parsers</li>
</ul></span></li>
<li><strong>Syntactic recognition and representation</strong>
<ul>
<li>Shallow parser or chunker</li>
<li>Context-free Grammar</li>
</ul></li>
<li><span style="color: grey;">Morphosyntactic analysis
<ul>
<li>Part-of-Speech (POS) tagging</li>
</ul></span></li>
<li><span style="color: grey;">Representing Meaning
<ul>
<li>First-order logic</li>
<li>Semantic Network</li>
<li>Conceptual Dependency Diagram</li>
<li>Frame-based approach</li>
</ul></span></li>
</ol>
</div><div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/context-free_grammar.png"></p>
<figcaption><a href="https://web.stanford.edu/~jurafsky/slp3/">Jurafsky, 2025</a></figcaption>
</figure>
</div>
</div></div>
</section>
<section class="slide level2">
<h2>NLP basics: Analysis and representation</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ol type="1">
<li><span style="color: grey;">Lexical and morphological analysis
<ul>
<li>Finite-state morphological parsers</li>
</ul></span></li>
<li><span style="color: grey;">Syntactic recognition and representation
<ul>
<li>Shallow parser or chunker</li>
<li>Context-free Grammar</li>
</ul></span></li>
<li><strong>Morphosyntactic analysis</strong>
<ul>
<li>Part-of-Speech (POS) tagging</li>
</ul></li>
<li><span style="color: grey;">Representing Meaning
<ul>
<li>First-order logic</li>
<li>Semantic Network</li>
<li>Conceptual Dependency Diagram</li>
<li>Frame-based approach</li>
</ul></span></li>
</ol>
</div><div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-left">
<figure>
<p><img data-src="figs/week11-nlp/pos_tagging.png"></p>
<figcaption><a href="https://web.stanford.edu/~jurafsky/slp3/">Jurafsky, 2025</a></figcaption>
</figure>
</div>
</div></div>
</section>
<section class="slide level2">
<h2>NLP basics: Analysis and representation</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ol type="1">
<li><span style="color: grey;">Lexical and morphological analysis
<ul>
<li>Finite-state morphological parsers</li>
</ul></span></li>
<li><span style="color: grey;">Syntactic recognition and representation
<ul>
<li>Shallow parser or chunker</li>
<li>Context-free Grammar</li>
</ul></span></li>
<li><span style="color: grey;">Morphosyntactic analysis
<ul>
<li>Part-of-Speech (POS) tagging</li>
</ul></span></li>
<li><strong>Representing Meaning</strong>
<ul>
<li>First-order logic</li>
<li>Semantic Network</li>
<li>Conceptual Dependency Diagram</li>
<li>Frame-based approach</li>
</ul></li>
</ol>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/first-order-1.png"></p>
<figcaption><a href="https://web.stanford.edu/~jurafsky/slp3/">Jurafsky, 2025</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/first-order-2.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>
<section class="slide level2">
<h2>NLP basics: General resources</h2>
<ol type="1">
<li><a href="https://openlexicon.fr/datasets-info/EnglishLexiconProject/README-ELP.html">Lexicon</a>: list of stems and affixes (prefix or suffix), together with basic information about them.</li>
<li><a href="https://wordnet.princeton.edu/">Thesaurus</a>: list of words and their synonyms from a specific domain</li>
<li><a href="https://repository.upenn.edu/bitstreams/560a804b-b3f7-40c7-8f57-052bdd5ca6cd/download">Treebank</a>: list of words labeled with syntatic (POS-tagging) trees</li>
<li><a href="https://propbank.github.io/">Prop(osition) bank</a>: sentences annotated with semantic roles related to verbs</li>
<li><a href="https://framenet.icsi.berkeley.edu/">FrameNet</a>: sentences annotated with semantic roles related to frames of words.</li>
<li><a href="https://course.ccs.neu.edu/cs5100f11/resources/noy01.pdf">Ontology</a>: hierachy of concepts related to a domain</li>
</ol>
</section>
<section class="slide level2">
<h2>NLP basics: Sparse and dense text representation</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ol type="1">
<li><strong>Sparse embeddings</strong> <sup>1</sup>
<ol type="1">
<li>One-hot encoding</li>
<li><span style="color: grey;">Bag-of-Words (BoW)</span></li>
<li><span style="color: grey;">Term Frequency-Inverse Document Frequency (TF-IDF)</span></li>
</ol></li>
<li><span style="color: grey;">Dense embeddings</span></li>
</ol>
</div><div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/one_hot.png"></p>
<figcaption><a href="https://www.blog.trainindata.com/one-hot-encoding-categorical-variables/">Galli, 2023</a></figcaption>
</figure>
</div>
</div><!-- https://www.blog.trainindata.com/one-hot-encoding-categorical-variables/ -->
</div>
<aside><ol class="aside-footnotes"><li id="fn1"><p>We can represent words, n-grams, sentences, etc.</p></li></ol></aside></section>
<section class="slide level2">
<h2>NLP basics: Sparse and dense text representation</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ol type="1">
<li><strong>Sparse embeddings</strong> <sup>1</sup>
<ol type="1">
<li><span style="color: grey;">One-hot encoding</span></li>
<li>Bag-of-Words (BoW)</li>
<li><span style="color: grey;">Term Frequency-Inverse Document Frequency (TF-IDF)</span></li>
</ol></li>
<li><span style="color: grey;">Dense embeddings</span></li>
</ol>
</div><div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/bow.png"></p>
<figcaption><a href="https://medium.com/@shantanu_sharma/natural-language-processing-nlp-playlist-chapter-2-bag-of-words-n-gram-tf-idf-458a9669a746">Sharma, 2024</a></figcaption>
</figure>
</div>
</div><!-- https://medium.com/@shantanu_sharma/natural-language-processing-nlp-playlist-chapter-2-bag-of-words-n-gram-tf-idf-458a9669a746 -->
</div>
<aside><ol class="aside-footnotes"><li id="fn2"><p>We can represent words, n-grams, sentences, etc.</p></li></ol></aside></section>
<section class="slide level2">
<h2>NLP basics: Sparse and dense text representation</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ol type="1">
<li><strong>Sparse embeddings</strong> <sup>1</sup>
<ol type="1">
<li><span style="color: grey;">One-hot encoding</span></li>
<li><span style="color: grey;">Bag-of-Words (BoW)</span></li>
<li>Term Frequency-Inverse Document Frequency (TF-IDF)</li>
</ol></li>
<li><span style="color: grey;">Dense embeddings</span></li>
</ol>
</div><div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/tfidf.png"></p>
<figcaption><a href="https://ted-mei.medium.com/demystify-tf-idf-in-indexing-and-ranking-5c3ae88c3fa0">Mai, 2019</a></figcaption>
</figure>
</div>
</div><!-- https://ted-mei.medium.com/demystify-tf-idf-in-indexing-and-ranking-5c3ae88c3fa0 -->
</div>
<aside><ol class="aside-footnotes"><li id="fn3"><p>We can represent words, n-grams, sentences, etc.</p></li></ol></aside></section>
<section class="slide level2">
<h2>NLP basics: Sparse and dense text representation</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ol type="1">
<li><span style="color: grey;">Sparse embeddings <sup>1</sup></span>
<ol type="1">
<li><span style="color: grey;">One-hot encoding</span></li>
<li><span style="color: grey;">Bag-of-Words (BoW)</span></li>
<li><span style="color: grey;">Term Frequency-Inverse Document Frequency (TF-IDF)</span></li>
</ol></li>
<li><strong>Dense embeddings</strong></li>
</ol>
</div><div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/word2vec.png"></p>
<figcaption><a href="https://jalammar.github.io/illustrated-word2vec/">Alammar, 2019</a></figcaption>
</figure>
</div>
</div><!-- https://jalammar.github.io/illustrated-word2vec/ -->
</div>
<aside><ol class="aside-footnotes"><li id="fn4"><p>We can represent words, n-grams, sentences, etc.</p></li></ol></aside></section>
<section class="slide level2">
<h2>NLP basics: Neural Network recap.</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/mlp.png"></p>
<figcaption>MLP - <a href="https://www.linkedin.com/pulse/explaining-multilayer-perceptrons-terms-general-matrix-ajit-jaokar-c5aje/">Jaokar, 2024</a></figcaption>
</figure>
</div>
</div><!-- https://www.linkedin.com/pulse/explaining-multilayer-perceptrons-terms-general-matrix-ajit-jaokar-c5aje/ --><div class="column" style="width:50%;">

</div></div>
</section>
<section class="slide level2">
<h2>NLP basics: Neural Network recap.</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/mlp.png"></p>
<figcaption>MLP - <a href="https://www.linkedin.com/pulse/explaining-multilayer-perceptrons-terms-general-matrix-ajit-jaokar-c5aje/">Jaokar, 2024</a></figcaption>
</figure>
</div>
</div><!-- https://www.linkedin.com/pulse/explaining-multilayer-perceptrons-terms-general-matrix-ajit-jaokar-c5aje/ --><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/rnn.png"></p>
<figcaption>RNN - <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah, 2015</a></figcaption>
</figure>
</div>
</div><!-- https://colah.github.io/posts/2015-08-Understanding-LSTMs/ -->
</div>
</section>
<section class="slide level2">
<h2>NLP basics: Neural Network recap.</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/transformer.png"></p>
<figcaption>Transformer - <a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">Vaswani, 2017</a></figcaption>
</figure>
</div>
</div><!-- Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017). --><div class="column" style="width:50%;">

</div></div>
</section>
<section class="slide level2">
<h2>NLP basics: Neural Network recap.</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/transformer.png"></p>
<figcaption>Transformer - <a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">Vaswani, 2017</a></figcaption>
</figure>
</div>
</div><!-- Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017). --><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/attention.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><!-- Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017). -->
</div>
</section>
<section class="slide level2">
<h2>NLP basics: Neural Network recap.</h2>

<!-- Park, 2019. SANVis: Visual Analytics for Understanding Self-Attention Networks -->
<img data-src="figs/week11-nlp/attention-v2.png" class="r-stretch quarto-figure-center"><p class="caption">Transformer - <a href="https://ieeexplore.ieee.org/abstract/document/8933677">Park, 2019</a></p></section>
<section class="slide level2">
<h2>Today’s Agenda</h2>
<ol type="1">
<li><span style="color: grey;">Natural Language Process (NLP) basics
<ul>
<li>Tasks and challenges</li>
<li>Analysis and representation</li>
<li>General resources</li>
<li>Sparse and dense text representation</li>
<li>Neural Network recap.</li>
</ul></span></li>
<li><strong>Visualization for NLP</strong>
<ul>
<li>General Text Visualization</li>
<li>Model agnostic explanation</li>
<li>Recurrent Neural Network (RNN) Visualization</li>
<li>Transformers (LLM) Visualization</li>
</ul></li>
</ol>
</section>
<section class="slide level2">
<h2>Visualization for NLP: General Text Visualization</h2>
<ol type="1">
<li><a href="https://ids-pub.bsz-bw.de/frontdoor/deliver/index/docId/2622/file/Fankhauser_etc_Exploring%20and%20Visualizing_2014.pdf">Exploring and Visualizing Variation in Language Resources</a></li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/2254556.2254572">Termite: Visualization Techniques for Assessing Textual Topic Models</a></li>
<li><a href="https://arxiv.org/abs/1703.00565">Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ</a></li>
</ol>
</section>
<section class="slide level2">
<h2>Visualization for NLP: General Text Visualization</h2>
<ol type="1">
<li><span style="color: grey;">Exploring and Visualizing Variation in Language Resources</span></li>
<li><span style="color: grey;">Termite: Visualization Techniques for Assessing Textual Topic Models</span></li>
<li><a href="https://arxiv.org/abs/1703.00565">Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ</a></li>
</ol>

<img data-src="figs/week11-nlp/paper5.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<p>Visualize linguistic variation between document categories in a language-independent way. The main visualization is a scatterplot of terms, using their frequencies as position and a probabilistic score that defines how strong the connection is between a word and a category. The authors tested it with a speech dataset from the 2012 Democratic and Republican conventions. In this case, Scattertext can highlight linguistic style and key themes of each dataset.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Visualization for NLP: Model agnostic explanation</h2>
<ol type="1">
<li><a href="https://doi.org/10.1109/TVCG.2018.2865044">Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models</a></li>
<li><a href="https://doi.org/10.1109/TVCG.2017.2744718">ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models</a></li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3490099.3511146">iSEA: An Interactive Pipeline for Semantic Error Analysis of NLP Models</a></li>
</ol>
</section>
<section class="slide level2">
<h2>Visualization for NLP: Model agnostic explanation</h2>
<ol type="1">
<li><span style="color: grey;">Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models</span></li>
<li><span style="color: grey;">ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models</span></li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3490099.3511146">iSEA: An Interactive Pipeline for Semantic Error Analysis of NLP Models</a></li>
</ol>

<img data-src="figs/week11-nlp/paper19-1.png" class="r-stretch"><aside class="notes">
<p>Automatically discovers semantically-grounded subpopulations with high error rates in the context of a human-in-the-loop interactive system. The user can learn more, validate error sources, and test hypotheses about errors.</p>
<ol type="1">
<li>Model Performance Overview:</li>
<li>Rule Discovery and 3) Rule Editor A rule is created by pattern mining that finds features combinations (rules) that frequencly occur in ‘model error cases’. That mining is applied over semantic and syntatic features, such as dependency labels, POS-tags, semantic roles, and lexical markers.</li>
<li>Overall statistics</li>
<li>Model prediction with SHAP tokens score, raw texts, ground-truth/predicted label<br>
</li>
<li>Concept construction enables a user to group Rules together.</li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Visualization for NLP: RNN Visualization</h2>
<ol type="1">
<li><a href="https://doi.org/10.1109/VAST.2017.8585721">Understanding Hidden Memories of Recurrent Neural Networks</a></li>
<li><a href="https://doi.org/10.1109/MCG.2018.2878902">RNNbow: Visualizing Learning Via Backpropagation Gradients in RNNs</a></li>
<li><a href="https://doi.org/10.1109/TVCG.2017.2744158">LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks</a></li>
</ol>
</section>
<section class="slide level2">
<h2>Visualization for NLP: RNN Visualization</h2>
<ol type="1">
<li><span style="color: grey;">Understanding Hidden Memories of Recurrent Neural Networks</span></li>
<li><span style="color: grey;">RNNbow: Visualizing Learning Via Backpropagation Gradients in RNNs</span></li>
<li><a href="https://doi.org/10.1109/TVCG.2017.2744158">LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks</a></li>
</ol>
<aside class="notes">
<ol type="1">
<li>Select view</li>
</ol>
<ul>
<li>Select a text section and define an activation threshold to highlight hidden states (blue lines)</li>
<li>Extra information presented in the meta-track, such as word POS-tagging and top-k model predictions</li>
</ul>
<ol start="2" type="1">
<li>Match View</li>
</ol>
<ul>
<li>Shows similar hidden state patterns in the dataset</li>
<li>Shows a heatmap with matched sentences (rows), hidden states (cols), and the activation value (color)</li>
<li>Shows a second heatmap encoding POS-tag of sentence words</li>
</ul>
<p>Tests with datasets of 1) news from the wall street journal 2) parenthesis language 3) dna sequences, and 4) musical chord progressions</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>

<img data-src="figs/week11-nlp/lstmvis.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section class="slide level2">
<h2>Visualization for NLP: LLM Visualization</h2>
<ol type="1">
<li><a href="https://debug-ml-iclr2019.github.io/cameraready/DebugML-19_paper_2.pdf">BertViz: A tool for visualizing multihead self-attention in the BERT model</a></li>
<li><a href="https://doi.org/10.1109/VISUAL.2019.8933677">Sanvis: Visual analytics for understanding self-attention networks</a></li>
<li><a href="https://doi.org/10.1109/TVCG.2020.3028976">Attention flows: Analyzing and comparing attention mechanisms in language models</a></li>
<li><a href="https://arxiv.org/abs/2103.14625">Dodrio: Exploring transformer models with interactive visualization</a></li>
<li><a href="https://journals.sagepub.com/doi/full/10.1177/14738716231168671">TopoBERT: Exploring the topology of fine-tuned word representations</a></li>
<li><a href="https://doi.org/10.1109/TVCG.2023.3327163">Attentionviz: A global view of transformer attention</a></li>
<li><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">On the Biology of a Large Language Model</a></li>
<li><a href="https://doi.org/10.1109/PacificVis64226.2025.00010">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</a></li>
</ol>
</section>
<section class="slide level2">
<h2>Visualization for NLP: LLM Visualization</h2>
<ol start="2" type="1">
<li><a href="https://doi.org/10.1109/VISUAL.2019.8933677">Sanvis: Visual analytics for understanding self-attention networks</a></li>
</ol>

<img data-src="figs/week11-nlp/sanvis.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<ol type="1">
<li>Attention piling (heatmap): key-words (x-axis), query-words (y-axis), and cell values are the attention score over the layer heads. Besides, instead of show information for all the layer heads, it aggregates them to show only one heatmap (piles).</li>
<li>Small multiple: Heads showed with some sort criterion (Head number, Entropy, Position)</li>
<li>Sanky diagram: how strong is the attention between two tokens</li>
<li>Head leans: extended information about each layer head.</li>
</ol>
<p>comprehend the alignment between encoder and decoder and the relationship between tokens</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Visualization for NLP: LLM Visualization</h2>
<ol start="4" type="1">
<li><a href="https://arxiv.org/abs/2103.14625">Dodrio: Exploring transformer models with interactive visualization</a></li>
</ol>

<img data-src="figs/week11-nlp/dodrio.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<ol type="1">
<li>Dependency view: shows sintatic dependences between tokens like a dependency tree/graph or a saliency view</li>
<li>Semantic attention graph: a graph that presents informations of a specific layer and head. Nodes are tokens and egdges are attention between them. Different views: radial, grid, and force</li>
<li>Attention head overview: grid of layers (rows) per heads (columns). Color palette maps a semantic score (read) and syntatic score (blue). Besides, the circle size means an importance score.</li>
</ol>
<p>attention weights syntatic dependences semantic information</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Visualization for NLP: LLM Visualization</h2>
<ol start="6" type="1">
<li><a href="https://doi.org/10.1109/TVCG.2023.3327163">Attentionviz: A global view of transformer attention</a></li>
</ol>

<img data-src="figs/week11-nlp/attention_viz.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<ol type="1">
<li>Creates an aligned projected space of Query and Key embeddings</li>
<li>In the main view, it shows a grid of projections per layer (rows) nad heads (columns)</li>
<li>In a detailed view of text models, it shows a BertViz-like view that connects query-key tokens with their attention scores.</li>
<li></li>
</ol>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Visualization for NLP: LLM Visualization</h2>
<ol start="7" type="1">
<li><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">On the Biology of a Large Language Model</a></li>
</ol>
<div class="columns">
<div class="column" style="width:50%;">
<h3>Does Claude plan its rhymes?</h3>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/claude_rhymes.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<h3>Mental math</h3>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/claude_math.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>
<section class="slide level2">
<h2>Visualization for NLP: LLM Visualization</h2>
<ol start="7" type="1">
<li><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">On the Biology of a Large Language Model</a></li>
</ol>
<div class="columns">
<div class="column" style="width:50%;">
<h3>Are Claude’s explanations always faithful?</h3>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/claude_faithful.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<h3>Multi-step reasoning</h3>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/week11-nlp/claude_reasoning.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>
<section class="slide level2">
<h2>Visualization for NLP: LLM Visualization</h2>
<ol start="7" type="1">
<li><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">On the Biology of a Large Language Model</a></li>
</ol>
<h3>Hallucinations</h3>

<!-- https://www.anthropic.com/research/tracing-thoughts-language-model -->
<img data-src="figs/week11-nlp/claude_hallucinate.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<p>applying attribution graphs to study a particular language model</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Visualization for NLP: LLM Visualization</h2>
<ol start="8" type="1">
<li><a href="https://doi.org/10.1109/PacificVis64226.2025.00010">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</a></li>
</ol>

<img data-src="figs/week11-nlp/paper21-1.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<ol type="1">
<li>Prompt panel: craft and edit prompts</li>
<li>Reasoning panel:</li>
<li>Evaluation panel: global and local evaluation with comparison of prompt editing history</li>
</ol>
<p>a visual analytics system to facilitate efficient prompt engineering for steering the multimodal reasoning performance of LLM</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Summary: NLP Foundations</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Key Takeaways:</strong></p>
<ol type="1">
<li>Even if human language is convoluted, we can still build advanced NLP systems that achieve good results.</li>
<li>These systems are difficult to explain and interpret.</li>
<li>Dense representations have much more information than sparse ones, such as the meaning, position, and relations between tokens.</li>
<li>In a transformer, Q and K spaces are the mechanisms for selection (interpretable via patterns of relevance), while V space is the content being selected (abstract, combined, and thus harder to ground linguistically on its own).</li>
</ol>
</div><div class="column" style="width:50%;">
<div style="background: #e8f5e9; padding: 25px; border-radius: 8px; margin-top: 30px;">

</div>
</div></div>

</section></section>

    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="figs/vida.jpg" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://engineering.nyu.edu" class="uri">https://engineering.nyu.edu</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="week11-nlp_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="week11-nlp_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="week11-nlp_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="week11-nlp_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="week11-nlp_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="week11-nlp_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="week11-nlp_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="week11-nlp_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="week11-nlp_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="week11-nlp_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'fast',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>