---
title: "Black-box Model Interpretation"
subtitle: "CS-GY 9223 - Fall 2025"
author: "Claudio Silva"
institute: "NYU Tandon School of Engineering"
date: "October 6, 2025"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: c/t
    show-slide-number: all
    hash-type: number
    logo: figs/vida.jpg
    width: 1920
    height: 1080
    preview-links: auto
    transition: fade
    transition-speed: fast
    footer: <https://engineering.nyu.edu>
    fontsize: 24pt
    css: lab-light-theme.css
resources:
  - model_assessment.pdf
---

# Black Box Model Assessment

## Agenda

\

### Goal: Study Model Agnostic Interpretability Methods. These should help to explain any type of ML Models.

1. Partial Dependence Plot (PDP)

2. Local Interpretable Model-agnostic Explanations (LIME)

3. SHAP (SHapley Additive exPlanations)

4. Comparative Analysis and Trade-offs


Examples and materials from Molnar's book:
           https://christophm.github.io/interpretable-ml-book/

::: {.notes}
Today we transition from white-box (interpretable) models to black-box explanation methods. These techniques work with ANY machine learning model - neural networks, ensemble methods, etc. The key distinction: white-box models are inherently interpretable, while black-box methods provide post-hoc explanations. We'll cover three major approaches: PDPs for global feature effects, LIME for local instance explanations, and SHAP for theoretically-grounded feature attribution. Each has different trade-offs between computational cost, interpretability, and theoretical guarantees.
:::


## Bike Rentals (Regression) 

::: {.r-fit-text}
This dataset contains daily counts of rented bicycles from the bicycle rental company Capital-Bikeshare in Washington D.C., along with weather and seasonal information. The goal is to predict how many bikes will be rented depending on the weather and the day. The data can be downloaded from the UCI Machine Learning Repository.

Here is the list of features used in Molnar's book:

- Count of bicycles including both casual and registered users. The count is used as the target in the regression task.
- The season, either spring, summer, fall or winter.
- Indicator whether the day was a holiday or not.
- The year, either 2011 or 2012.
- Number of days since the 01.01.2011 (the first day in the dataset). This feature was introduced to take account of the trend over time.
- Indicator whether the day was a working day or weekend.
- The weather situation on that day. One of:
clear, few clouds, partly cloudy, cloudy
mist + clouds, mist + broken clouds, mist + few clouds, mist
light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds
heavy rain + ice pallets + thunderstorm + mist, snow + mist
- Temperature in degrees Celsius.
- Relative humidity in percent (0 to 100).
- Wind speed in km per hour.
:::

::: footer
Molnar, C. (2022). [*Interpretable Machine Learning*](https://christophmolnar.com/books/interpretable-machine-learning). 2nd Edition.
:::

## Partial Dependence Plot (PDP)

Shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 2001).

![](figs/black-box/bike-use-temperature.jpg)

::: footer
Friedman, J. H. (2001). [*Greedy function approximation: A gradient boosting machine*](https://doi.org/10.1214/aos/1013203451). Annals of Statistics.
:::

::: {.notes}
PDPs are one of the oldest and most intuitive model-agnostic interpretation methods. The key idea: marginalize over all other features to isolate the effect of one (or two) features on the prediction. Here we see bike rentals vs temperature - notice the non-linear relationship with a sweet spot around 20-25°C. The visualization immediately reveals this pattern that might be hidden in model coefficients. PDPs answer: "On average, how does changing this feature affect predictions?"
:::

## PDP Visualization: Interpreting Feature Effects

:::: {.columns}

::: {.column width="33%"}
**1. Monotonic/Linear**

<div style="text-align: center; font-size: 4em; line-height: 1.2em;">
↗<br/>
&nbsp;&nbsp;↗<br/>
&nbsp;&nbsp;&nbsp;&nbsp;↗
</div>

Straight diagonal line

↑ Feature → ↑ Prediction

Feature has consistent positive (or negative) effect
:::

::: {.column width="33%"}
**2. Non-linear/Sweet Spot**

<div style="text-align: center; font-size: 4em; line-height: 1.2em;">
&nbsp;&nbsp;&nbsp;╱‾╲<br/>
&nbsp;&nbsp;╱&nbsp;&nbsp;&nbsp;&nbsp;╲<br/>
&nbsp;╱&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;╲
</div>

Curve peaks and drops

Optimal range exists

(like temperature example)
:::

::: {.column width="33%"}
**3. Flat Line**

<div style="text-align: center; font-size: 4em; line-height: 1.2em;">
<br/>
━━━━━━━<br/>
<br/>
</div>

Horizontal line

No marginal effect

Feature is globally unimportant
:::

::::

::: {.notes}
The key visual takeaway is the slope and shape of the line. The slope indicates the marginal effect. A flat line is a visually clear indicator of a globally unimportant feature. Contrast the flat line with the non-linear "sweet spot" of the temperature plot from the previous slide. When reviewing PDPs, look for: (1) Direction of effect (up/down), (2) Linearity vs non-linearity, (3) Interaction points where the effect changes dramatically.
:::

## Partial Dependence Plot (PDP)

High level idea: marginalizing the machine learning model output over the distributions of the all other features to show the relationship between the feature we are interested in and the predicted outcome.

::: {.r-stack}
![](figs/black-box/pdp-feature1.jpg){.fragment width="1400" height="700"}

![](figs/black-box/pdp-feature2.jpg){.fragment width="1400" height="700"}

![](figs/black-box/pdp-feature3.jpg){.fragment width="1400" height="700"}

![](figs/black-box/pdp-feature4.jpg){.fragment width="1400" height="700"}

![](figs/black-box/pdp-feature5.jpg){.fragment width="1400" height="700"}

![](figs/black-box/pdp-feature6.jpg){.fragment width="1400" height="700"}
:::

::: {.notes}
This animation shows the PDP computation process step by step. For each value of our feature of interest, we: (1) Replace that feature with a fixed value across ALL data points, (2) Get predictions for all those modified instances, (3) Average those predictions, (4) Repeat for different feature values. The result is a curve showing the average effect. Notice how we're effectively "scanning" through the feature space while averaging out the effects of all other features. This is powerful because it works with any black-box model.

While the animation shows the calculation (marginalizing/averaging), remind students that the resulting simple 2D line plot is the visualization product we interpret. The computational complexity is hidden from the end user - they only see the clean, interpretable curve that reveals the feature's marginal effect.
:::

## PDP: Code Example with scikit-learn

**Dataset:** California Housing - median house value prediction (8 features)

```{python}
#| echo: true
#| output: false
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.inspection import PartialDependenceDisplay
import matplotlib.pyplot as plt

# Load California housing dataset
# Features: median income, house age, average rooms, etc.
housing = fetch_california_housing()
X, y = housing.data, housing.target

# Train a gradient boosting regressor
model = GradientBoostingRegressor(
    n_estimators=100,
    max_depth=4,
    learning_rate=0.1,
    random_state=0
).fit(X, y)

# Create PDPs for: MedInc (0), HouseAge (1), and their interaction
# MedInc = median income, HouseAge = median house age
features = [0, 1, (0, 1)]
display = PartialDependenceDisplay.from_estimator(
    model, X, features, feature_names=housing.feature_names
)
```

::: footer
[scikit-learn Partial Dependence Documentation](https://scikit-learn.org/stable/modules/partial_dependence.html)
:::

::: {.notes}
This example shows how simple it is to compute PDPs in practice using scikit-learn. The `PartialDependenceDisplay.from_estimator()` method handles all the computation automatically: (1) It marginalizes over the training data, (2) Computes predictions for all feature values, (3) Averages the results, (4) Creates the visualization. Note that you can plot individual features `[0, 1]` or interactions `[(0, 1)]` which show how two features jointly affect predictions. The interaction plot is a 2D heatmap instead of a line plot. This high-level API abstracts away the complexity we saw in the animation, making PDPs accessible for practitioners.
:::

## PDP: Output Visualization

```{python}
#| echo: false
#| fig-width: 14
#| fig-height: 6
display.figure_
```

**Interpretation:**

- **Left (MedInc):** Strong positive monotonic relationship - higher median income → higher house prices (as expected)
- **Middle (HouseAge):** Non-monotonic effect - newer and very old houses have lower values, middle-aged houses peak
- **Right (Interaction):** Shows how income and age combine - high income dominates regardless of age (vertical gradient)

::: {.notes}
The California Housing dataset provides much clearer, more interpretable patterns: (1) **MedInc (Median Income)**: Shows a strong, near-linear positive relationship - as median income increases, predicted house values increase consistently. This makes intuitive sense and demonstrates a clear monotonic effect. (2) **HouseAge**: Reveals a non-monotonic pattern with a peak for middle-aged houses (around 20-30 years). Very new houses might lack established neighborhoods, while very old houses may need renovation. This is the kind of "sweet spot" pattern we discussed earlier. (3) **Interaction plot**: The 2D heatmap shows that median income has a dominant effect (notice the strong vertical color gradient), while house age modulates this effect more subtly. The interaction reveals that high-income areas command high prices regardless of house age. These diverse patterns demonstrate why PDPs are valuable for understanding feature effects.
:::

## Partial Dependence Plot (PDP)

::: {.fragment}
**Pros**

- Intuitive
- Interpretation is clear
- Easy to implement

:::

::: {.fragment}
**Cons**

- Assume independence among features
- Can only show few features
- Hidden heterogeneous effects from averaging
:::

## Local Interpretable Model-agnostic Explanations (LIME)

Training local surrograte models to explain *individual* predictions

:::: {.columns}

::: {.column width="60%"}
![](figs/black-box/lime-global-decision-boundaries.jpg)
:::

::: {.column width="40%"}
![](figs/black-box/lime-paper.jpg)
:::

::::

::: footer
Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). [*"Why Should I Trust You?" Explaining the Predictions of Any Classifier*](https://arxiv.org/pdf/1602.04938.pdf). KDD.
:::

::: {.notes}
LIME represents a paradigm shift from global to local explanations. The key insight: even if the global decision boundary is complex (left image shows a highly non-linear boundary), we can approximate it LOCALLY with a simple linear model. Think of it like approximating a curve with a tangent line - it only works near the point of tangency. The title "Why Should I Trust You?" captures the motivation: users need to understand individual predictions to build trust. LIME became hugely influential because it's model-agnostic, produces human-interpretable explanations, and works for any data type (tabular, text, images).
:::

## Training local surrogate models to explain individual predictions

The idea is quite intuitive. **First, forget about the training data and imagine you only have the black box model where you can input data points and get the predictions of the model. You can probe the box as often as you want. Your goal is to understand why the machine learning model made a certain prediction. LIME tests what happens to the predictions when you give variations of your data into the machine learning model.**

**LIME generates a new dataset consisting of perturbed samples and the corresponding predictions of the black box model.**

**On this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest.** The interpretable model can be anything from the interpretable models chapter, for example Lasso or a decision tree. **The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation. This kind of accuracy is also called *local fidelity*.**

Mathematically, local surrogate models with interpretability constraint can be expressed as follows:

$$\text{explanation}(x) = \arg\min_{g \in G} L(f, g, \pi_x) + \Omega(g)$$

::: footer
Section 9.2, on Molnar's book.
:::

## Local Interpretable Model-agnostic Explanations (LIME)

### Algorithm

1. Pick an input that you want an explanation for.
2. Sample the neighbors of the selected input (i.e. perturbation).
3. Train a linear classifier on the neighbors.
4. The weights on the linear classifier is the explanation.

## Local Interpretable Model-agnostic Explanations (LIME)

:::: {.columns}

::: {.column width="60%"}
![](figs/black-box/lime-random-forest-model.jpg)
:::

::: {.column width="40%"}
Random forest predictions given features x1 and x2.

Predicted classes: 1 (dark) or 0 (light).
:::

::::


## Local Interpretable Model-agnostic Explanations (LIME)

:::: {.columns}

::: {.column width="60%"}
![](figs/black-box/lime-random-forest-sampling.jpg)
:::

::: {.column width="40%"}
Instance of interest (big yellow dot) and data sampled from a normal distribution (small dots).
:::

::::


## Local Interpretable Model-agnostic Explanations (LIME)

:::: {.columns}

::: {.column width="60%"}
![](figs/black-box/lime-random-forest-weighting.jpg)
:::

::: {.column width="40%"}
Assign higher weight to points near the instance of interest.
I.e., $weight(p) = \sqrt{\frac{e^{-d^2}}{w^2}}$
where $d$ is the distance between $p$  and the
instantce of interest, and $w$ is the kernel width (self-defined).
:::

::::


## Local Interpretable Model-agnostic Explanations (LIME)

:::: {.columns}

::: {.column width="60%"}
![](figs/black-box/lime-random-forest-line.jpg)
:::

::: {.column width="40%"}
Use both the samples and sample weights to train a linear classifier.

Signs of the grid show the classifications of the locally learned model from the weighted samples. The red line marks the decision boundary (P(class=1) = 0.5).

The official implementation uses a Ridge Classifier as the linear model for explanation.
:::

::::


## Training local surrogate models to explain individual predictions

:::: {.columns}

::: {.column width="50%"}
![](figs/black-box/lime-random-forest-line.jpg)

$s_i$ = sample weight, $\lambda$ = regularization term
:::

::: {.column width="50%"}
**Ridge Classifier**

$$minimize \sum_{i=1}^{M} s_i(y_i - \hat{y}_i)^2$$

$$= \sum_{i=1}^{M} s_i(y_i - \sum_{j=0}^{p} w_j \times x_{ij})^2 + \lambda \sum_{j=0}^{p} w_j^2$$

<span style="color: red;">$w_j$ = trained weight to explain the importance of feature j</span>

<span style="color: red;">The higher the $\lambda$, the more sparse the $w$ (more zeros) will become.</span>
:::

::::

::: footer
Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). [*"Why Should I Trust You?" Explaining the Predictions of Any Classifier*](https://arxiv.org/pdf/1602.04938.pdf). KDD.
:::

## LIME Visualization: Bar Charts for Local Explanations

**Primary LIME Output: Sparse Bar Charts**

:::: {.columns}

::: {.column width="50%"}
**Case 1: High Rental Day**
*Prediction: ABOVE Trend*

<div style="background: #f8f9fa; padding: 20px; border-radius: 8px;">

<span style="color: #2ecc71; font-size: 1.5em;">▬▬▬▬▬▬▬▬▬▬</span> Temp > 20°C
<span style="color: #2ecc71; font-size: 1.5em;">▬▬▬▬▬▬▬</span> Windspeed Low
<span style="color: #e74c3c; font-size: 1.5em;">▬▬▬</span> Holiday = False

</div>

✓ Warm temperature strongly supports high rentals
✓ Low wind moderately supports
✗ Non-holiday slightly opposes
:::

::: {.column width="50%"}
**Case 2: Low Rental Day**
*Prediction: BELOW Trend*

<div style="background: #f8f9fa; padding: 20px; border-radius: 8px;">

<span style="color: #e74c3c; font-size: 1.5em;">▬▬▬▬▬▬▬▬▬▬▬▬</span> Weather: Rain
<span style="color: #e74c3c; font-size: 1.5em;">▬▬▬▬▬▬▬▬</span> Temp < 5°C
<span style="color: #2ecc71; font-size: 1.5em;">▬▬</span> Weekday = True

</div>

✗ Rain strongly opposes high rentals
✗ Cold temperature opposes
✓ Weekday weakly supports
:::

::::

::: {.notes}
Emphasize the visualization: **Color** (Green supports the prediction, Red opposes it), **Length of Bar** (magnitude of influence), and **Sparsity** (only a few features are shown—this makes it easy for a non-expert to trust). The bar chart is the key deliverable of LIME - it provides a contrastive explanation showing which features push toward vs away from the prediction. Compare the two cases: Case 1 is dominated by green (supporting features), while Case 2 is dominated by red (opposing features). This visual contrast makes the explanation immediately understandable.
:::

## Local Surrogate (LIME): Bike Rental Example

**Task:** Predict if bike rentals will be above or below trend

```{python}
#| echo: false
#| output: false

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
import lime
import lime.lime_tabular
import matplotlib.pyplot as plt

# Load bike sharing dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip"
import urllib.request
import zipfile
import io

# Download and extract
response = urllib.request.urlopen(url)
zip_file = zipfile.ZipFile(io.BytesIO(response.read()))
zip_file.extractall("/tmp/bike_data")

# Load day.csv
bike_df = pd.read_csv("/tmp/bike_data/day.csv")

# Create features
feature_names = ['season', 'weathersit', 'temp', 'hum', 'windspeed', 'workingday']
X = bike_df[feature_names].values
y = (bike_df['cnt'] > bike_df['cnt'].median()).astype(int)  # Above/below median

# Train random forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X, y)

# Create LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X,
    feature_names=feature_names,
    class_names=['BELOW trend', 'ABOVE trend'],
    mode='classification',
    random_state=42
)

# Find a high rental day (warm, good weather)
high_idx = bike_df[(bike_df['temp'] > 0.7) & (bike_df['weathersit'] == 1)].index[0]
high_instance = X[high_idx]
exp_high = explainer.explain_instance(high_instance, rf_model.predict_proba, num_features=5)

# Find a low rental day (cold, bad weather)
low_idx = bike_df[(bike_df['temp'] < 0.3) & (bike_df['weathersit'] == 3)].index[0]
low_instance = X[low_idx]
exp_low = explainer.explain_instance(low_instance, rf_model.predict_proba, num_features=5)

# Store predictions
pred_high = rf_model.predict_proba([high_instance])[0]
pred_low = rf_model.predict_proba([low_instance])[0]
```

:::: {.columns}

::: {.column width="50%"}
**High Rental Day**

Prediction: **{python} f"{pred_high[1]:.2f}"** probability ABOVE trend

```{python}
#| echo: false
#| fig-width: 6.5
#| fig-height: 4.5

fig = exp_high.as_pyplot_figure(label=1)
ax = fig.gca()
ax.set_xlabel('Feature Contribution to ABOVE Trend', fontsize=13)
ax.tick_params(labelsize=12)
ax.set_title('LIME Explanation', fontsize=15, fontweight='bold')
fig.tight_layout()
fig
```

<div style="font-size: 1em; margin-top: 10px; background: #e8f5e9; padding: 10px; border-radius: 5px;">
✓ High temperature strongly supports<br>
✓ Good weather condition supports
</div>
:::

::: {.column width="50%"}
**Low Rental Day**

Prediction: **{python} f"{pred_low[0]:.2f}"** probability BELOW trend

```{python}
#| echo: false
#| fig-width: 6.5
#| fig-height: 4.5

# For low rental, also explain label=1 (ABOVE) to show negative contributions
fig = exp_low.as_pyplot_figure(label=1)
ax = fig.gca()
ax.set_xlabel('Feature Contribution to ABOVE Trend', fontsize=13)
ax.tick_params(labelsize=12)
ax.set_title('LIME Explanation', fontsize=15, fontweight='bold')
fig.tight_layout()
fig
```

<div style="font-size: 1em; margin-top: 10px; background: #ffebee; padding: 10px; border-radius: 5px;">
✗ Bad weather strongly opposes<br>
✗ Low temperature opposes
</div>
:::

::::

::: {.notes}
This slide shows actual LIME explanations for bike rental predictions. The bar charts display feature contributions - green bars indicate features supporting the prediction, orange/red bars indicate features opposing it. Notice how the high rental day is dominated by positive contributions (warm temp, good weather), while the low rental day is dominated by negative contributions (bad weather, cold temp). The model predictions are shown at the top - one instance has high probability of being ABOVE trend, the other BELOW trend. This is exactly the kind of local explanation LIME provides - understanding which features matter most for each specific prediction.
:::

::: footer
Bike Sharing Dataset from UCI Machine Learning Repository. Fanaee-T, H., & Gama, J. (2014). Event labeling combining ensemble detectors and background knowledge. Progress in Artificial Intelligence, 2(2-3), 113-127.
:::

## LIME: Code Example

**Dataset:** California Housing (same model as PDP example)

```{python}
#| echo: true
#| output: false
import lime
import lime.lime_tabular
import numpy as np

# Reload California Housing data for this example
housing = fetch_california_housing()
X_housing, y_housing = housing.data, housing.target

# Retrain model on California Housing
model = GradientBoostingRegressor(
    n_estimators=100,
    max_depth=4,
    learning_rate=0.1,
    random_state=0
).fit(X_housing, y_housing)

# Create LIME explainer using training data
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_housing,
    feature_names=housing.feature_names,
    mode='regression',
    random_state=0
)

# Select an instance to explain (e.g., instance 0)
instance_idx = 0
instance = X_housing[instance_idx]

# Generate explanation for this instance
# predict_fn should return predictions
exp = explainer.explain_instance(
    instance,
    model.predict,
    num_features=5  # Show top 5 features
)
```

::: footer
[LIME Documentation](https://lime-ml.readthedocs.io/)
:::

::: {.notes}
LIME provides local explanations by training a simple interpretable model around a specific instance. The LimeTabularExplainer is initialized with the training data to understand feature distributions. The key parameter is `num_features` which controls sparsity - we only show the top 5 most important features for this prediction. Unlike PDP which gives global effects, LIME explains why the model made this specific prediction for this specific instance. The `predict` function is passed to LIME so it can probe the model with perturbed versions of the instance.
:::

## LIME: Output Visualization

```{python}
#| echo: false
#| fig-width: 12
#| fig-height: 7
# Get the explanation as a matplotlib figure
# Increase font sizes for better readability
fig = exp.as_pyplot_figure()
ax = fig.gca()
for text in ax.texts:
    text.set_fontsize(14)
ax.tick_params(labelsize=14)
ax.set_xlabel(ax.get_xlabel(), fontsize=16)
ax.set_title(ax.get_title(), fontsize=18)
fig.tight_layout()
fig
```

::: {style="font-size: 1.3em;"}
**Explanation for Instance 0:**

Predicted value: `{python} f"{model.predict([instance])[0]:.2f}"` (in units of $100k)

The bar chart shows which features push the prediction higher (positive) or lower (negative) for this specific house.
:::

::: {.notes}
The output shows a horizontal bar chart with the most important features for this specific prediction. Positive bars (pointing right) indicate features that increase the predicted house value, while negative bars (pointing left) decrease it. The length of each bar represents the magnitude of the feature's contribution. Notice this is a LOCAL explanation - it only applies to this one house. If we explained a different house, we'd likely get different important features. This is the key difference from PDP which shows global average effects. The sparse representation (only 5 features) makes it easy for users to understand and trust the prediction.
:::

## Local Interpretable Model-agnostic Explanations (LIME)


::: {.fragment}
**Pros**

- Explanations are short (= selective) and possibly contrastive.
  * we can control the sparsity of weight coefficients in the regressions method.
- Very easy to use.

:::

::: {.fragment}
**Cons**

- Unstable results due to sampling.
- Hard to weight similar neighbors in a high dimensional dataset.
- Many parameters for data scientists to hide biases.
:::

## SHAP (SHapley Additive exPlanations)

:::: {.columns}

::: {.column width="60%"}
![](figs/black-box/shap-example.jpg)
:::

::: {.column width="40%"}
SHAP (Lundberg and Lee 2017a) is a game-theory-inspired method created to explain predictions made by machine learning models. SHAP generates one value per input feature (also known as SHAP values) that indicates how the feature contributes to the prediction of the specified data point.
:::

::::

::: footer
Lundberg, S. M., & Lee, S. I. (2017). [*A Unified Approach to Interpreting Model Predictions*](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf). NeurIPS. | Molnar, C. (2024). [*SHAP Book*](https://christophmolnar.com/books/shap/).
:::

::: {.notes}
SHAP brings rigorous game theory to ML interpretability. The key innovation: apply Shapley values from cooperative game theory to feature attribution. This provides a principled answer to "how much did each feature contribute?" with mathematical guarantees (efficiency, symmetry, dummy, additivity axioms). SHAP unified several existing methods under one framework. The image shows a typical SHAP force plot - red pushes prediction higher, blue pushes lower. Unlike LIME which is heuristic, SHAP has strong theoretical foundations. Trade-off: computational cost is higher, but you get provably fair attributions.
:::


## A Short History of Shapley Values and SHAP

* 1953: The introduction of Shapley values in game theory (by Lloyd Shapley).
* 2010: The initial steps toward applying Shapley values in machine learning
  - original paper contained NO code!
* 2017: The advent of SHAP (by Lundberg and Lee), a turning point in machine learning.

## Theory of Shapley Values

Who's going to pay for that taxi?

Alice, Bob, and Charlie have dinner together and share a taxi ride home. The total cost is $51. The question is, how should they divide the costs fairly?

![](figs/black-box/shap-taxi.jpg)

::: {.notes}
The taxi example beautifully illustrates why Shapley values are "fair". Alice lives closest ($20 alone), Bob is farther ($38 with Alice), Charlie is farthest ($51 all together). A naive split ($17 each) is unfair - Alice subsidizes the others. Shapley values compute each person's marginal contribution across all possible orderings of joining the taxi. This ensures: (1) Total cost is exactly split, (2) Symmetric players pay equally, (3) Non-contributors pay nothing, (4) Values are additive. The same logic applies to features in ML: how much does each feature "contribute" to the prediction, measured fairly across all possible feature coalitions?
:::


## Theory of Shapley Values

The **marginal contribution** of a player to a coalition is the value of the coali- tion with the player minus the value of the coalition without the player. In the taxi example, the value of a coalition is equal to the cost of the ride as detailed in the above table. Therefore, the marginal contribution of, for instance, Charlie to a taxi already containing Bob is the cost of the taxi with Bob and Charlie, minus the cost of the taxi with Bob alone.

## Theory of Shapley Values

![](figs/black-box/shap-marginal-contribution.jpg)

## Theory of Shapley Values

How to average these marginal contributions per passenger?

One way to answer this question is by considering all possible permutations of Alice, Bob, and Charlie. There are 3! = 3 * 2 * 1 = 6 possible permutations of passengers:

* Alice, Bob, Charlie 
* Alice, Charlie, Bob
* Bob, Alice, Charlie
* Charlie, Alice, Bob 
* Bob, Charlie, Alice 
* Charlie, Bob, Alice

We can use these permutations to form coalitions, for example, for Alice. 

## Theory of Shapley Values

In two of these cases, Alice was added to an empty taxi, and in one case, she was added to a taxi with only Bob. By weighting the marginal contributions accordingly, we calculate the following weighted average marginal contribution for Alice, abbreviating Alice, Bob, and Charlie to A, B, and C:

![](figs/black-box/shap-marginal-alice.jpg)

## Theory of Shapley Values

for Bob:

![](figs/black-box/shap-marginal-bob.jpg)

for Charlie:

![](figs/black-box/shap-marginal-charlie.jpg)


## Calculating Shapley values

![](figs/black-box/shap-general-formation.jpg)

**The Shapley value is the weighted average of a player’s marginal contribu- tions to all possible coalitions.**

## The axioms behind Shapley values

* Efficiency: The sum of the contributions must precisely add up to the payout.

* Symmetry: If two players are identical, they should receive equal contributions.

* Dummy or Null Player: The value of a player who doesn't contribute to any coalition is zero.

* Additivity: In a game with two value functions, the Shapley values for the sum can be expressed as the sum of the Shapley values.

**These four axioms ensure the uniqueness of the Shapley values.**

## From Shapley Values to SHAP

Consider the following scenario: You have trained a machine learning model $f$ to predict apartment prices.

![](figs/black-box/shap-apartment-example.jpg)

## SHAP Visualization: Force Plot

**Visualizing the Efficiency Axiom**

<div style="background: #f8f9fa; padding: 30px; border-radius: 8px; margin: 20px 0;">

**Base Value** (E[f(x)]) = $300K &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Output Value** f(x) = $450K

<div style="margin: 30px 0;">
<span style="color: #e74c3c; font-size: 2em;">▬▬▬▬▬▬</span> <span style="font-size: 1.2em;">**+$80K** Area=85m² (high pushes price up)</span><br/>
<span style="color: #e74c3c; font-size: 2em;">▬▬▬▬</span> <span style="font-size: 1.2em;">**+$45K** Location=Downtown</span><br/>
<span style="color: #e74c3c; font-size: 2em;">▬▬▬</span> <span style="font-size: 1.2em;">**+$30K** Year=2020 (new)</span><br/>
<span style="color: #3498db; font-size: 2em;">▬▬</span> <span style="font-size: 1.2em;">**-$15K** Cat-banned=True</span><br/>
<span style="color: #3498db; font-size: 2em;">▬</span> <span style="font-size: 1.2em;">**-$10K** Floor=1 (ground floor)</span>
</div>

**Sum: $300K + $80K + $45K + $30K - $15K - $10K = $450K ✓**

</div>

::: {.notes}
This visualization directly maps the Efficiency Axiom from the theory section: the contributions (φᵢ) of all features (red/blue blocks) must sum up exactly from the Base Value (average prediction) to the Final Output (instance prediction). This is a powerful, rigorous local explanation. Red blocks push the prediction higher; blue blocks push it lower. The horizontal layout shows the additive nature - each feature's contribution stacks to reach the final prediction. This is fundamentally different from LIME which uses sampling; SHAP provides exact attributions based on game theory.
:::

## From Shapley Values to SHAP: Cat-Banned Feature

We want to evaluate the effect of cat-banned feature

![](figs/black-box/shap-cat-banned.jpg)

## From Shapley Values to SHAP: Cat-Banned Feature

Computing the Shapley value for cat-banned

![](figs/black-box/shap-cat-banned-2.jpg)

## Interpreting SHAP values

The Shapley value can be misinterpreted. The Shapley value of a feature value is not the difference of the predicted value after removing the feature from the model training. The interpretation of the Shapley value is: **Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.**

The Shapley value is the wrong explanation method if you seek sparse explanations (explanations that contain few features). Explanations created with the Shapley value method **always use all the features.** Humans prefer selective explanations, such as those produced by LIME. LIME might be the better choice for explanations lay-persons have to deal with. 

(From Molnar's book)

## SHAP Visualization: Global Feature Analysis (Summary Plot)

**From Local to Global: Understanding Feature Importance Across All Predictions**

:::: {.columns}

::: {.column width="60%"}
<div style="background: #f8f9fa; padding: 30px; border-radius: 8px;">

**Feature Importance** (Y-axis) ↓

<div style="margin: 20px 0; font-size: 1.3em;">
**Area (m²)** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color: #e74c3c;">●●●●●●●●●●</span><span style="color: #3498db;">●●●●</span><br/>
**Location** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color: #e74c3c;">●●●●●●●</span><span style="color: #3498db;">●●●</span><br/>
**Year Built** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color: #e74c3c;">●●●●●</span><span style="color: #3498db;">●●●●</span><br/>
**Cat-banned** &nbsp;&nbsp; <span style="color: #e74c3c;">●●</span><span style="color: #3498db;">●●●</span><br/>
**Floor** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color: #3498db;">●●●</span><span style="color: #e74c3c;">●●</span>
</div>

← Negative Impact &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **SHAP Value** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Positive Impact →

<span style="color: #3498db;">● Blue = Low feature value</span> &nbsp;&nbsp; <span style="color: #e74c3c;">● Red = High feature value</span>

</div>
:::

::: {.column width="40%"}
**Dual Interpretation:**

1. **Feature Importance** (Y-Axis vertical spread)
   - Wider spread = more important
   - Area has largest impact

2. **Direction of Effect** (Color)
   - Red (high) on right = positive impact
   - Blue (low) on left = negative impact

**Example Reading:**
- High area (red) → strong positive SHAP
- Low area (blue) → negative SHAP
- Clear correlation!
:::

::::

::: {.notes}
Explain the dual interpretation of this plot: (1) **Feature Importance (Y-Axis)**: Vertical spread/density of points indicates a feature's overall importance (like traditional feature importance). Features at the top matter more globally across all predictions. (2) **Correlation (Color)**: The color coding (Red for high feature value, Blue for low) shows the relationship. For example, 'Red points on the far right' means high feature values are associated with high positive impact on the prediction. Area shows both high importance AND clear directionality - bigger apartments cost more. Compare this to cat-banned which has mixed effects and lower importance. This plot aggregates SHAP values across many instances to give a global view.
:::

## SHAP (SHapley Additive exPlanations)

::: {.fragment}
**Pros**

- Fairly distributed feature importance to a prediction

- Contrastive explanations (can compare an instance to a subset or even to a single data point)

- Solid theory

:::

::: {.fragment}
**Cons**

- A lot of computing time
- Not sparse explanations (every feature is important)

:::

## SHAP (SHapley Additive exPlanations)

![](figs/black-box/shap-figure-waterfall.jpg)

## SHAP limitations

:::: {.columns}

::: {.column width="50%"}
![](figs/black-box/shap-problems.jpg)
:::

::: {.column width="50%"}
**Key Limitations:**

- **Computational Cost**: Exponential complexity O(2^n) for exact calculation
- **Feature Independence Assumption**: Unrealistic in correlated datasets
- **Interpretation Challenges**: Values represent marginal contributions, not causal effects
- **Instability**: Small perturbations can lead to different explanations

:::

::::

::: footer
Kumar, I. E., et al. (2020). [*Problems with Shapley-value-based explanations as feature importance measures*](http://proceedings.mlr.press/v119/kumar20e/kumar20e.pdf). ICML.
:::

::: {.notes}
Despite SHAP's theoretical elegance, it has important limitations that practitioners must understand. The Kumar et al. paper provides crucial critiques: (1) SHAP assumes features can be independently varied, but real features are often correlated (e.g., house size and bedrooms), (2) SHAP values show associative, not causal relationships - they tell you "what" contributes but not "why", (3) Computational cost grows exponentially with features (though approximations like TreeSHAP and KernelSHAP help), (4) Explanations can be unstable under small data perturbations. Bottom line: SHAP is a powerful tool but not a magic bullet. Always validate explanations against domain knowledge.
:::

## Comparative Trade-offs: Choosing the Right XAI Tool

| **Criterion** | **PDP** | **LIME** | **SHAP** |
|--------------|---------|----------|----------|
| **Scope** | Global (feature-level) | Local (instance-level) | Both (local + global) |
| **Primary Visualization** | Line plot (2D curve) | Bar chart (sparse) | Force plot, Summary plot |
| **Interpretability** | ★★★★★ Intuitive | ★★★★☆ Easy to understand | ★★★☆☆ Requires training |
| **Computational Cost** | ★★★★☆ Moderate | ★★★☆☆ Fast sampling | ★★☆☆☆ Expensive |
| **Theoretical Foundation** | Statistical (marginal effects) | Heuristic (local approximation) | Game theory (Shapley values) |
| **Feature Coverage** | One at a time | Sparse (selected) | All features (dense) |
| **Best For** | Understanding global trends | Quick local explanations | Rigorous feature attribution |
| **Key Limitation** | Assumes independence | Unstable (sampling) | Computational cost |

::: {.notes}
This table provides a decision guide for practitioners. **Choose PDP** when you need to understand how a feature affects predictions globally - great for presentations to stakeholders. **Choose LIME** when you need fast, interpretable explanations for individual predictions - ideal for production systems where speed matters. **Choose SHAP** when you need theoretically grounded attributions and can afford the computational cost - best for research or high-stakes decisions. In practice, use multiple methods: PDP for global insights, LIME for quick checks, SHAP for final validation. Remember: no method is perfect - always validate against domain knowledge and use visualizations to communicate effectively.
:::