<!DOCTYPE html>
<html lang="en"><head>
<script src="week8-clustering_files/libs/clipboard/clipboard.min.js"></script>
<script src="week8-clustering_files/libs/quarto-html/tabby.min.js"></script>
<script src="week8-clustering_files/libs/quarto-html/popper.min.js"></script>
<script src="week8-clustering_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="week8-clustering_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="week8-clustering_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="week8-clustering_files/libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.24">

  <meta name="author" content="Claudio Silva">
  <meta name="dcterms.date" content="2025-10-20">
  <title>Clustering and Dimensionality Reduction</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="week8-clustering_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="week8-clustering_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="week8-clustering_files/libs/revealjs/dist/theme/quarto-743137726eb562984e8d4ff610b648a8.css">
  <link rel="stylesheet" href="lab-light-theme.css">
  <link href="week8-clustering_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="week8-clustering_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="week8-clustering_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="week8-clustering_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section class="quarto-title-block center">
  <h1 class="title">Clustering and Dimensionality Reduction</h1>
  <p class="subtitle">CS-GY 9223 - Fall 2025</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Claudio Silva 
</div>
        <p class="quarto-title-affiliation">
            NYU Tandon School of Engineering
          </p>
    </div>
</div>

  <p class="date">2025-10-20</p>
</section>
<section class="slide level2">
<h2>Agenda</h2>
<ol type="1">
<li class="fragment"><strong>Clustering</strong>
<ul>
<li class="fragment">Introduction to unsupervised learning</li>
<li class="fragment">K-means clustering on IRIS dataset</li>
<li class="fragment">DBSCAN and other clustering methods</li>
</ul></li>
<li class="fragment"><strong>Dimensionality Reduction</strong>
<ul>
<li class="fragment">The manifold hypothesis and intrinsic dimensionality</li>
<li class="fragment">Principal Component Analysis (PCA)</li>
<li class="fragment">Eigenvectors, eigenvalues, and covariance matrices</li>
<li class="fragment">Singular Value Decomposition (SVD)</li>
<li class="fragment">Local Linear Embedding (LLE)</li>
<li class="fragment">Preview: Non-linear Manifold Learning (t-SNE, UMAP)</li>
</ul></li>
</ol>
<aside class="notes">
<p>Today we’re covering two fundamental unsupervised learning techniques that are crucial for understanding and visualizing machine learning models. First, we’ll explore clustering - finding natural groupings in data without labels. We’ll use the classic IRIS dataset to demonstrate K-means and discuss other methods like DBSCAN. Then we’ll dive deep into dimensionality reduction, starting with the mathematical foundations of PCA (eigenvectors, covariance matrices), moving to computational techniques like SVD, and exploring non-linear methods like LLE. This sets the foundation for next week’s advanced dimensionality reduction techniques including t-SNE and UMAP.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- Examples and materials from... -->
</section>
<section class="slide level2">
<h2>Clustering</h2>
<p>Etienne Bernard: “… the goal of clustering is to separate a set of examples into groups called clusters”</p>

<img data-src="figs/iris.jpg" class="r-stretch"><aside class="notes">
<p>Clustering is an unsupervised learning task where we try to find natural groupings in data without having labeled examples. Unlike classification, where we know the classes ahead of time, clustering discovers structure in the data. This is particularly useful for exploratory data analysis, customer segmentation, anomaly detection, and understanding the structure of your data before applying supervised learning methods. The IRIS dataset shown here is one of the classic examples in machine learning, containing measurements of iris flowers from three different species.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>IRIS</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href=""></a><span class="co"># Code source: Gaël Varoquaux</span></span>
<span id="cb1-2"><a href=""></a><span class="co"># Modified for documentation by Jaques Grobler</span></span>
<span id="cb1-3"><a href=""></a><span class="co"># License: BSD 3 clause</span></span>
<span id="cb1-4"><a href=""></a><span class="co">#</span></span>
<span id="cb1-5"><a href=""></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href=""></a></span>
<span id="cb1-7"><a href=""></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb1-8"><a href=""></a>iris <span class="op">=</span> datasets.load_iris()</span>
<span id="cb1-9"><a href=""></a></span>
<span id="cb1-10"><a href=""></a>_, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb1-11"><a href=""></a>scatter <span class="op">=</span> ax.scatter(iris.data[:, <span class="dv">2</span>], iris.data[:, <span class="dv">1</span>])</span>
<span id="cb1-12"><a href=""></a>ax.<span class="bu">set</span>(xlabel<span class="op">=</span>iris.feature_names[<span class="dv">2</span>], ylabel<span class="op">=</span>iris.feature_names[<span class="dv">1</span>])</span>
<span id="cb1-13"><a href=""></a>_ <span class="op">=</span> ax.legend(</span>
<span id="cb1-14"><a href=""></a>    scatter.legend_elements()[<span class="dv">0</span>], iris.target_names, loc<span class="op">=</span><span class="st">"lower right"</span>, title<span class="op">=</span><span class="st">"Classes"</span></span>
<span id="cb1-15"><a href=""></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<aside class="notes">
<p>Here’s the code to load and visualize the IRIS dataset. The IRIS dataset has 4 features (sepal length, sepal width, petal length, petal width) and 3 species classes. We’re plotting just two dimensions here (petal width vs sepal width) to make it easy to visualize. Notice we’re using sklearn’s built-in datasets module which makes it very easy to load standard benchmark datasets.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>IRIS</h2>
<div id="50a93533" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week8-clustering_files/figure-revealjs/cell-2-output-1.png" width="812" height="432"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>This unlabeled visualization highlights the core challenge of unsupervised learning. While some structure exists in this 2D projection, the full structure in the 4D space is harder to discern. Our goal with clustering is to algorithmically discover these natural groupings that are not visually obvious without labels. Notice that without color coding by species, it’s not immediately obvious that there are three distinct groups. This is where clustering algorithms come in - they can help us discover these natural groupings automatically.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>IRIS – another look</h2>
<div id="d1b8c35e" class="cell" data-fig-height="6" data-fig-width="9" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week8-clustering_files/figure-revealjs/cell-3-output-1.png" width="1335" height="885"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>This plot, now colored by the true species labels, confirms the inherent structure we are looking to discover. Note the clear separation of the Setosa species (blue), but also the significant overlap between Versicolor (red) and Virginica (green). This overlap explains why even simple clustering methods like K-means will struggle to achieve 100% agreement with the true labels, demonstrating the practical limits of clustering even on ‘simple’ data. The visualization uses petal length and sepal width, which provide good separation between species while still showing the challenging overlap between two classes.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>IRIS – clustering</h2>

<img data-src="figs/iris-bernard-clustering.jpg" class="r-stretch"><aside class="notes">
<p>This slide shows different clustering results on the IRIS dataset. Different clustering algorithms will find different groupings based on their underlying assumptions about cluster shape, density, and separation. Some algorithms assume spherical clusters (like K-means), while others can find arbitrary shapes (like DBSCAN). The quality of clustering can be evaluated both visually and using metrics like silhouette score or within-cluster sum of squares.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>IRIS – k-means</h2>

<img data-src="figs/iris-bernard-clustering-kmeans.jpg" class="r-stretch"><aside class="notes">
<p>K-means is one of the most popular clustering algorithms due to its simplicity and efficiency. It works by iteratively assigning points to the nearest cluster center (centroid) and then recomputing the centroids. The algorithm requires you to specify K (the number of clusters) in advance. For IRIS with K=3, it does a reasonable job of recovering the three species. K-means works well when clusters are roughly spherical and similar in size, but can struggle with elongated or irregular-shaped clusters. The algorithm is also sensitive to initialization, though techniques like K-means++ help address this.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Wolfram Mathematica FindClusters</h2>

<img data-src="figs/findclusters-methods.jpg" class="r-stretch"><aside class="notes">
<p>Wolfram Mathematica’s FindClusters function provides a nice overview of different clustering methods available. Each method has different strengths: K-means is fast and simple, hierarchical methods build tree structures of clusters, DBSCAN can find arbitrary shapes and identify outliers, spectral clustering works well with non-convex clusters. The choice of method depends on your data characteristics, domain knowledge, and what properties you want your clusters to have. In practice, it’s often worth trying multiple methods and comparing results.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Wolfram Mathematica FindClusters</h2>

<img data-src="figs/findclusters-methods-examples.jpg" class="r-stretch"><aside class="notes">
<p>This slide shows visual examples of how different clustering methods partition the same dataset. Notice how they produce very different results! K-means creates roughly circular boundaries, hierarchical methods create nested groupings, DBSCAN identifies dense regions and can mark sparse areas as outliers. This illustrates an important point: there’s no single “correct” clustering - the best method depends on what structure you’re trying to discover in your data. Visualization is crucial for understanding what each method is doing and whether the results make sense for your application.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>IRIS - classes</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div id="1781fb67" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week8-clustering_files/figure-revealjs/cell-4-output-1.png" width="812" height="432"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<p><img data-src="figs/iris-bernard-clustering-kmeans-crop.jpg"></p>
</div></div>
<aside class="notes">
<p>This side-by-side comparison is essential for understanding clustering performance. The left shows the ground truth (true labels), and the right shows the clusters discovered by K-means. While K-means accurately isolates the well-separated cluster, it struggles with the overlapped classes, often resulting in label-switching or mixing of points. This visually demonstrates the difference between classification (predicting known labels) and clustering (discovering structure). Remember, in real clustering scenarios, you often don’t have labels - this comparison is primarily for pedagogical purposes and algorithm evaluation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Recommended reading</h2>
<ul>
<li><p><strong>Required</strong> https://www.wolfram.com/language/introduction-machine-learning/clustering/ <a href="https://www.wolfram.com/language/introduction-machine-learning/clustering/">link</a></p></li>
<li><p>https://en.wikipedia.org/wiki/Cluster_analysis</p></li>
<li><p>https://en.wikipedia.org/wiki/K-means_clustering</p></li>
<li><p>https://en.wikipedia.org/wiki/DBSCAN</p></li>
</ul>
<aside class="notes">
<p>The Wolfram tutorial is required reading - it provides excellent interactive examples of different clustering methods. The Wikipedia articles provide good mathematical background. I particularly recommend the DBSCAN article as it introduces density-based clustering, which can find clusters of arbitrary shape unlike K-means. For your projects, understanding multiple clustering approaches will help you choose the right tool for your data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Dimensionality Reduction</h2>
<ul>
<li><p>Input data may have thousands or millions of dimensions!</p></li>
<li><p><strong>Dimensionality Reduction</strong> represents data with fewer dimensions</p>
<ul>
<li>easier learning – fewer parameters</li>
<li>visualization – show high-dimensional data in 2D or 3D</li>
<li>discover “intrinsic dimensionality” of the data</li>
</ul></li>
</ul>
<aside class="notes">
<p>Now we transition to dimensionality reduction, which is closely related to clustering but serves a different purpose. While clustering groups similar items, dimensionality reduction finds lower-dimensional representations of high-dimensional data. This is crucial in machine learning because: 1) it helps us visualize data that otherwise couldn’t be plotted, 2) it reduces computational cost and memory requirements, 3) it can improve model performance by removing noise and redundant features (curse of dimensionality), and 4) it helps us understand the intrinsic structure of the data. Many real-world datasets live on lower-dimensional manifolds embedded in high-dimensional space - DR helps us find these manifolds.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Dimensionality Reduction</h2>
<ul>
<li>Assumption: data lies on a lower dimensional space</li>
</ul>

<img data-src="figs/dm-data-is-low-dimensional.jpg" class="r-stretch"><div class="footer">
<p>Slides based on material from Prof.&nbsp;Yi Zhang</p>
</div>
<aside class="notes">
<p>This is the key insight: even though our data might be represented with thousands of features, it often lies on or near a much lower-dimensional manifold. Think about images of faces - each pixel is a dimension, so a 100x100 image is 10,000 dimensional. But faces don’t fill that entire space - they have structure (two eyes, one nose, etc.). The actual degrees of freedom are much fewer - perhaps orientation, expression, lighting, identity. Dimensionality reduction tries to find these underlying degrees of freedom. This is the manifold hypothesis that underlies much of machine learning.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Visualizing the Manifold Hypothesis: Images of ‘3’ Perturbed by Non-linear Operations</h2>

<img data-src="figs/perturbed-3.jpg" class="r-stretch"><ul>
<li><p>What operations did we perform? What’s the intrinsic dimensionality?</p></li>
<li><p>Here the underlying <strong>manifold</strong> is <strong>non-linear</strong></p></li>
</ul>
<div class="footer">
<p>Slides based on material from Christopher Bishop</p>
</div>
<aside class="notes">
<p>This is a great example from Christopher Bishop’s book. These images of the digit “3” are all different - they’re rotated, translated, scaled, and slightly deformed. Each image might be 28x28 = 784 dimensions. But how many degrees of freedom do we really have? We have rotation angle, x/y position, scale, and maybe a few shape parameters. So perhaps 5-7 dimensions really capture the variation. This is what we mean by intrinsic dimensionality. Crucially, notice these transformations are non-linear - small changes in rotation don’t correspond to linear changes in pixel values. This is why we need non-linear dimensionality reduction methods like t-SNE and UMAP, not just linear methods like PCA.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Digits</h2>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href=""></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb2-2"><a href=""></a></span>
<span id="cb2-3"><a href=""></a>digits <span class="op">=</span> load_digits(n_class<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb2-4"><a href=""></a>X, y <span class="op">=</span> digits.data, digits.target</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<aside class="notes">
<p>Now we’ll work with the digits dataset, which is a great playground for dimensionality reduction. It consists of 8x8 pixel images of handwritten digits (0-9). Each image is 64-dimensional (8x8), which is small enough to work with quickly but high enough dimensional that we can’t visualize it directly. We’ll load just 6 classes to keep visualization cleaner.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Digits - 0</h2>
<div id="4a36db03" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],
       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],
       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],
       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],
       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],
       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],
       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],
       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])</code></pre>
</div>
</div>
<aside class="notes">
<p>This is what a single digit image looks like as an 8x8 array of pixel intensities. Each value represents the grayscale intensity at that pixel. This array gets flattened into a 64-dimensional vector for machine learning algorithms. Even though we show it as a 2D grid, the algorithm sees it as a point in 64D space.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Digits - 1</h2>
<div id="0b371483" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([[ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.],
       [ 0.,  0.,  0., 11., 16.,  9.,  0.,  0.],
       [ 0.,  0.,  3., 15., 16.,  6.,  0.,  0.],
       [ 0.,  7., 15., 16., 16.,  2.,  0.,  0.],
       [ 0.,  0.,  1., 16., 16.,  3.,  0.,  0.],
       [ 0.,  0.,  1., 16., 16.,  6.,  0.,  0.],
       [ 0.,  0.,  1., 16., 16.,  6.,  0.,  0.],
       [ 0.,  0.,  0., 11., 16., 10.,  0.,  0.]])</code></pre>
</div>
</div>
<aside class="notes">
<p>Here’s another digit. Notice the variation in writing style even within the same digit class. Dimensionality reduction should ideally map similar digits close together in the low-dimensional space, regardless of these minor variations.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Digits</h2>
<div id="f8657cb5" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week8-clustering_files/figure-revealjs/cell-7-output-1.png" width="558" height="520"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>Here’s a grid showing 100 different digit images from our dataset. Notice the variety - different digits, different writing styles, different positions. Our goal with dimensionality reduction is to create a 2D or 3D representation where similar-looking digits are close together. This will help us visualize the structure of the data and understand how well different algorithms capture the similarity relationships.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Digits</h2>
<div id="92617e0b" class="cell" data-execution_count="7">
<div class="cell-output cell-output-stdout">
<pre><code>Computing Random projection embedding...
Computing Truncated SVD embedding...
Computing Linear Discriminant Analysis embedding...
Computing Isomap embedding...
Computing Standard LLE embedding...
Computing Modified LLE embedding...
Computing Hessian LLE embedding...
Computing LTSA LLE embedding...
Computing MDS embedding...
Computing Random Trees embedding...
Computing Spectral embedding...
Computing t-SNE embedding...
Computing NCA embedding...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week8-clustering_files/figure-revealjs/cell-8-output-2.png" width="763" height="409"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>This slide lists many popular dimensionality reduction algorithms. The figure (populated by the results) is a powerful tool for comparison. It demonstrates that the choice of algorithm—linear (like PCA/SVD) vs.&nbsp;non-linear (like t-SNE/LLE)—radically changes the resulting low-dimensional structure. This comparison motivates why we need to understand the underlying math and assumptions of each method. Notice how t-SNE creates distinct, well-separated clusters of digits, which is characteristic of non-linear manifold learning methods that preserve local neighborhood structure.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Principal Component Analysis</h2>
<ul>
<li class="fragment">PCA is directly related to the eigenvectors and eigenvalues of covariance matrices.</li>
<li class="fragment">Lets so make a quick review of eigenvectors, eigenvalues, and covariance matrices.</li>
</ul>
<div class="footer">
<p>Slides based on material from Prof.&nbsp;Luis Gustavo Nonato</p>
</div>
<aside class="notes">
<p>Now we’ll dive deep into PCA, which is the most fundamental dimensionality reduction technique. PCA is a linear method - it finds the best linear projection of the data. While it can’t capture complex non-linear structure like t-SNE can, it’s extremely fast, robust, well-understood mathematically, and often works surprisingly well. Plus understanding PCA gives you the foundation for understanding more complex methods. These slides are from Luis Gustavo Nonato’s excellent lectures. We’ll review the linear algebra foundations first.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Eigenvectors and Eigenvalues</h2>
<p>Given a <span class="math inline">\(d \times d\)</span> matrix <span class="math inline">\(A\)</span>, a pair <span class="math inline">\((\lambda, u)\)</span> that satisfies</p>
<p><span class="math inline">\(A u = \lambda u\)</span></p>
<p>is called eigenvalue <span class="math inline">\(\lambda\)</span> and corresponding eigenvector <span class="math inline">\(u\)</span> of <span class="math inline">\(A\)</span>.</p>
<aside class="notes">
<p>Quick linear algebra review: An eigenvector is a special vector that, when you multiply it by a matrix, just gets scaled - it doesn’t change direction. The scaling factor is the eigenvalue. This is crucial for PCA because the eigenvectors of the covariance matrix tell us the directions of maximum variance in the data, and the eigenvalues tell us how much variance there is in each direction. The largest eigenvectors become our principal components.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Symmetric Matrices</h2>
<ul>
<li><span class="math inline">\(\lambda \in \mathbb{R}\)</span> and <span class="math inline">\(u \in \mathbb{R}^d\)</span> (no complex numbers involved)</li>
<li>The eigenvectors are orthogonal</li>
</ul>

<img data-src="figs/pca-spectral-decomposition.jpg" class="r-stretch"><aside class="notes">
<p>Covariance matrices are symmetric, which is great news! It means all eigenvalues are real (not complex), and the eigenvectors are orthogonal to each other. This orthogonality is why principal components form a nice coordinate system - each component is independent. The spectral decomposition shown here is how we can decompose any symmetric matrix into its eigenvectors and eigenvalues.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Covariance Matrix</h2>

<img data-src="figs/covariance-matrix.jpg" class="r-stretch"><aside class="notes">
<p>The covariance matrix captures how features vary together. The diagonal elements are variances (how much each feature varies), and the off-diagonal elements are covariances (how pairs of features vary together). A large positive covariance means when one feature is large, the other tends to be large. A large negative covariance means they vary in opposite directions. Zero covariance means they’re uncorrelated.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Covariance Matrix</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="figs/large-covariance.jpg"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="figs/zero-convariance.jpg"></p>
</div></div>
<aside class="notes">
<p>Visual intuition: On the left, we see highly correlated data - when X increases, Y tends to increase too. This shows up as a large covariance. On the right, the features are uncorrelated - knowing X tells you nothing about Y. The covariance is near zero. PCA finds directions that maximize variance, which means it finds the directions where data spreads out the most.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Covariance Matrix</h2>

<img data-src="figs/covariance-summary.jpg" class="r-stretch"><aside class="notes">
<p>Summary slide showing the mathematical formula for covariance. Remember that we typically center the data (subtract the mean) before computing PCA, which is why we see (x-μ) terms here. The covariance matrix is the average of the outer product of centered data points.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Principal Component Analysis: intuition</h2>

<img data-src="figs/pca-intuition.jpg" class="r-stretch"><aside class="notes">
<p>Here’s the geometric intuition for PCA. Imagine you have a cloud of points in high-dimensional space. PCA finds the direction where the data varies the most (the first principal component), then the direction of next-most variation that’s orthogonal to the first (second PC), and so on. Geometrically, we’re finding the axes of the ellipsoid that best fits the data. Projecting onto the first few principal components gives us a lower-dimensional representation that captures most of the variance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Principal Component Analysis: intuition</h2>

<img data-src="figs/pca-intuition2.jpg" class="r-stretch"><aside class="notes">
<p>Another view: the red line shows the first principal component - the direction of maximum variance. If we project all points onto this line, we get a 1D representation. The key insight is that even though we’re throwing away information (the perpendicular distance to the line), we’re keeping the most important information (the spread along the line). This is why PCA works well for compression and visualization - it discards the dimensions with least variance, which are often noise.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Principal Component Analysis</h2>

<img data-src="figs/pca-description.jpg" class="r-stretch"><aside class="notes">
<p>The formal algorithm: 1) Center the data by subtracting the mean, 2) Compute the covariance matrix, 3) Find its eigenvectors and eigenvalues, 4) Sort eigenvectors by eigenvalue (largest first), 5) Project data onto the top k eigenvectors. The eigenvectors are your principal components, and the eigenvalues tell you how much variance each component explains. You can plot the cumulative explained variance to decide how many components to keep.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Principal Component Analysis</h2>

<img data-src="figs/pca-filtering.jpg" class="r-stretch"><aside class="notes">
<p>PCA can also be viewed as a filtering operation - keeping the signal (high variance directions) and removing noise (low variance directions). This is why PCA is used for denoising. You project into PCA space, keep only the top components, and project back. The assumption is that noise is spread across many dimensions while signal is concentrated in a few. This works surprisingly well for many types of data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>PCA of digits</h2>
<div id="892daa37" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week8-clustering_files/figure-revealjs/cell-9-output-1.png" width="801" height="414"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>Here’s PCA applied to the full digits dataset (all 10 classes). We’re reducing from 64 dimensions down to 2. Notice there’s some structure - the points aren’t randomly scattered - but it’s not super clean. Without the color labels, it’s hard to see the digit classes. PCA is a linear method, so it can’t capture the complex non-linear structure of digit manifolds. But it’s fast and gives us a reasonable starting point.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>PCA of digits</h2>
<div id="498499a5" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week8-clustering_files/figure-revealjs/cell-10-output-1.png" width="801" height="414"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>Now with color labels showing the true digit classes. You can see that PCA does capture some structure - there are some clusters corresponding to digit classes - but there’s substantial overlap. The digit “1” is somewhat separated (probably because it’s sparse - lots of empty space), but most other digits are mixed together. This is a limitation of linear methods for non-linear data. We’ll see that t-SNE does much better.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Scaling Up</h2>
<ul>
<li>Covariance matrix can be really big!
<ul>
<li><span class="math inline">\(\Sigma\)</span> is <span class="math inline">\(n\)</span> by <span class="math inline">\(n\)</span></li>
<li>10000 features are not uncommon</li>
<li>computing eigenvectors is slow…</li>
</ul></li>
<li>Solution: Singular Value Decomposition (SVD)
<ul>
<li>Finds the <span class="math inline">\(k\)</span> largest eigenvectors</li>
<li>Widely implemented robustly in major packages</li>
</ul></li>
</ul>
<aside class="notes">
<p>A practical issue: for high-dimensional data, computing the full covariance matrix and all its eigenvectors is computationally expensive. For 10,000 features, the covariance matrix has 100 million entries! SVD provides an efficient alternative - it can directly compute the top k principal components without forming the covariance matrix. Furthermore, SVD is generally more numerically stable than computing the eigenvalues of the covariance matrix directly, especially in the presence of noise or highly correlated features. This is why SVD is the practical computational backbone for PCA in modern machine learning libraries like scikit-learn. In practice, sklearn’s PCA uses randomized SVD for efficiency, making PCA tractable even for very high-dimensional data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Singular Value Decomposition (SVD)</h2>
<ul>
<li>https://en.wikipedia.org/wiki/Singular_value_decomposition</li>
</ul>

<img data-src="figs/svd.jpg" class="r-stretch"><aside class="notes">
<p>SVD decomposes any matrix into three matrices: U (left singular vectors), Sigma (singular values), and V (right singular vectors). For PCA, the right singular vectors V are the principal components, and the singular values squared are the eigenvalues. SVD is more numerically stable than computing eigenvalues of the covariance matrix directly. It’s the standard way to compute PCA in practice.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Dimensionality Reduction Techniques</h2>
<ul>
<li>https://en.wikipedia.org/wiki/Dimensionality_reduction
<ul>
<li>Principal component analysis (PCA)</li>
<li>Non-negative matrix factorization (NMF)</li>
<li>Linear discriminant analysis (LDA)</li>
<li>t-SNE</li>
<li>UMAP</li>
<li><strong>many others</strong></li>
</ul></li>
</ul>
<aside class="notes">
<p>There are many dimensionality reduction techniques beyond PCA. Linear methods like LDA consider class labels. Non-negative matrix factorization (NMF) constrains components to be non-negative, which is useful for parts-based representations. t-SNE and UMAP are non-linear methods that can capture complex manifold structure. Each has different strengths and use cases. For visualization, t-SNE and UMAP are popular because they preserve local neighborhood structure well.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Local Linear Embedding</h2>

<img data-src="figs/lle.jpg" class="r-stretch"><aside class="notes">
<p>LLE is a non-linear dimensionality reduction technique that’s designed to preserve local neighborhoods. The idea is simple but powerful: each point should be reconstructible as a weighted sum of its neighbors, and this reconstruction should be preserved in the low-dimensional space. This means if points are close in high-dimensional space, they’ll be close in low-dimensional space. LLE can “unfold” non-linear manifolds in a way that linear PCA cannot.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Preserving Local Manifold Neighborhoods</h2>

<img data-src="figs/preserving-local-neighborhoods.jpg" class="r-stretch"><aside class="notes">
<p>Non-linear dimensionality reduction, particularly methods like LLE, relies on the assumption that the local geometry of the high-dimensional data is a good proxy for the global structure. As illustrated here, the goal is to find a mapping that preserves the local neighborhood relationships (who is connected to whom) in the low-dimensional space, effectively ‘unrolling’ the manifold without tearing it apart. This is fundamentally different from PCA which only preserves global variance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>LLE</h2>

<img data-src="figs/lle-algorithm.jpg" class="r-stretch"><p>https://www.science.org/doi/10.1126/science.290.5500.2323</p>
<aside class="notes">
<p>Local Linear Embedding (LLE) works by first finding the set of weights that best reconstructs each data point from its neighbors in the high-dimensional space. The algorithm then searches for a low-dimensional embedding where these same reconstruction weights hold true. Unlike PCA, LLE is not based on maximizing variance; it’s based entirely on preserving these local linear dependencies. This makes LLE powerful for unrolling non-linear manifolds while maintaining neighborhood structure.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>PCA vs LLE</h2>
<p><img data-src="figs/pca-vs-lle.jpg" height="600"></p>
<aside class="notes">
<p>This figure is the canonical example illustrating the difference between linear (PCA) and non-linear (LLE) methods. PCA finds the direction of maximum variance, but since it is a straight line projection, it cannot ‘unroll’ the curved ‘Swiss Roll’ dataset, resulting in heavy overlap. LLE, by preserving local relationships, successfully unrolls the manifold into a clean 2D representation, demonstrating the power of non-linear methods when the intrinsic data manifold is curved. This is why we need both linear and non-linear methods in our toolkit.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Beyond PCA: Non-linear Methods</h2>
<ul>
<li><strong>t-SNE</strong> (t-Distributed Stochastic Neighbor Embedding)
<ul>
<li>Preserves local neighborhood structure</li>
<li>Great for visualization of clusters</li>
<li>Non-linear, adapts to different regions of data</li>
</ul></li>
<li><strong>UMAP</strong> (Uniform Manifold Approximation and Projection)
<ul>
<li>Faster than t-SNE, scales better</li>
<li>Better preserves global structure</li>
<li>Based on topological data analysis</li>
</ul></li>
</ul>
<aside class="notes">
<p>We’ve covered PCA and LLE in detail. There are two other extremely popular non-linear methods worth knowing about: t-SNE and UMAP. t-SNE has become the go-to method for visualizing high-dimensional data, especially in biology and machine learning. It’s excellent at revealing local cluster structure. UMAP is newer and addresses some of t-SNE’s limitations - it’s faster and better at preserving global structure. We’ll cover both in much more detail next week, including important caveats about how to use them properly and avoid common pitfalls.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Next Week: Advanced Dimensionality Reduction</h2>
<ul>
<li>Deep dive into t-SNE
<ul>
<li>How it works (probability distributions, KL divergence)</li>
<li>Critical parameters (perplexity)</li>
<li>Common pitfalls and how to avoid them</li>
</ul></li>
<li>UMAP in detail
<ul>
<li>Comparison with t-SNE</li>
<li>Parameter tuning (n_neighbors, min_dist)</li>
</ul></li>
<li>Interactive dimensionality reduction</li>
</ul>
<aside class="notes">
<p>Next week we’ll go deep on t-SNE and UMAP. We’ll cover the theory, practical considerations, and most importantly, how to interpret the results correctly. There’s a fantastic Distill article on t-SNE that we’ll discuss - it shows common ways people misread t-SNE plots and how to avoid those mistakes. We’ll also look at UMAP’s advantages and when to choose it over t-SNE. Finally, we’ll discuss interactive approaches that let you guide the projection process.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="figs/vida.jpg" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://engineering.nyu.edu" class="uri">https://engineering.nyu.edu</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="week8-clustering_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="week8-clustering_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="week8-clustering_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="week8-clustering_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="week8-clustering_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="week8-clustering_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="week8-clustering_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="week8-clustering_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="week8-clustering_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="week8-clustering_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'fast',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>