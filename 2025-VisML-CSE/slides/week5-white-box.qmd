---
title: "White-box Model Interpretation"
subtitle: "CS-GY 9223 - Fall 2025"
author: "Claudio Silva"
institute: "NYU Tandon School of Engineering"
date: "September 29, 2025"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: c/t
    show-slide-number: all
    hash-type: number
    logo: figs/vida.jpg
    width: 1920
    height: 1080
    preview-links: auto
    transition: fade
    transition-speed: fast
    footer: <https://engineering.nyu.edu>
    fontsize: 24pt
    css: lab-light-theme.css
---

## Week 5: White-box Model Interpretation
- Model Interpretation and Explanation
- White-box Approaches and Visualizations
- Related Research in VIS & AI

::: {.notes}
Today we focus on interpretable machine learning models and their visualization. We'll explore intrinsically interpretable models like linear regression, GAMs, decision trees, and rule-based systems. We'll see how visualization helps us understand model behavior and how different model types offer different forms of interpretability.
:::

## Outline
- Model Interpretation and Explanation
- <span style="color: grey;">White-box Approaches and Visualizations</span>
- <span style="color: grey;">Related Research in VIS & AI</span>

## What is Interpretability?

> **"Interpretability is the degree to which a human can understand the cause of a decision"**

::: {.incremental}
- Can you predict what the model will do?
- Can you understand *why* it made a particular decision?
- Can you trust the model's reasoning process?
:::

::: footer
Molnar, C. (2022). [*Interpretable Machine Learning*](https://christophm.github.io/interpretable-ml-book/). 2nd Edition.
:::

::: {.notes}
Interpretability fundamentally means understanding causation - why did the model produce this output? A more operational definition: a model is interpretable if a human can correctly and efficiently predict the model's results. This goes beyond just seeing outputs - it requires understanding the mechanism.
:::

## Why Model Interpretation & Explanation?
::: {.columns}
::: {.column width="60%"}
![](figs/white-box/paper1.png){width="700px"}
:::

::: {.column width="40%"}
- **Debugging & Validation**: Detect bugs, biases, data leakage
- **Knowledge Discovery**: Learn patterns, generate hypotheses
- **Building Trust**: Increase confidence, social acceptance
- **Compliance**: Legal/ethical requirements, fairness audits
:::
:::

::: {.notes}
Interpretability serves multiple critical functions. For debugging, we can catch issues like the model learning spurious correlations. For science, interpretable models reveal domain insights. For deployment, stakeholders need to trust the system. In regulated domains like healthcare and finance, interpretability may be legally required to justify decisions.
:::

## Machine-learning-assisted materials discovery using failed experiments

::: {.columns}
::: {.column width="40%"}
![SVM derived decision tree](figs/white-box/svm.png)
:::

::: {.column width="60%"}
- Researchers firstly built a database of chemistry experiments (new material).
- Then they train an SVM to predict whether a new chemistry experiment will be successful.
- Then they train a surrogate DT to explain the model to learn more about the experiment.
:::
:::

::: {.notes}
This Nature paper demonstrates using interpretable models to understand scientific experiments. By training a surrogate decision tree on SVM predictions, researchers could extract human-understandable rules about which experimental conditions lead to success.
:::

## Properties of Good Explanations

Human explanations are naturally:

::: {.incremental}
1. **Contrastive**: "Why this, rather than that?" (not exhaustive)
2. **Selective**: Focus on 1-3 key reasons (not all causes)
3. **Social**: Tailored to audience and context
4. **Focused on abnormal**: Highlight surprising factors
5. **Truthful but simple**: Balance accuracy with understandability
:::

::: footer
Molnar, C. (2022). [*Interpretable Machine Learning*](https://christophm.github.io/interpretable-ml-book/interpretability.html). 2nd Edition.
:::

::: {.notes}
Research on human explanations reveals important patterns. People don't want exhaustive causal chains - they want contrastive answers comparing to alternatives. Good explanations are selective, providing just a few key factors rather than everything. They should be tailored to the audience's knowledge and focus on surprising or abnormal causes rather than routine factors. This guides how we should design ML explanations.
:::

## Why Model Interpretation & Explanation?

::: {.columns}
::: {.column width="60%"}
![https://arxiv.org/abs/1702.08608](figs/white-box/paper2.png){width="700px"}
:::

::: {.column width="40%"}
- Fairness
- Privacy
- Reliability or Robustness
- Causality
- Trust
:::
:::

::: {.notes}
This paper highlights critical concerns in ML deployment. Fairness requires understanding if protected attributes influence decisions. Privacy needs transparency about what data influences predictions. Robustness demands knowing if the model relies on brittle features. Causality questions whether correlations are meaningful. All these build toward the ultimate goal: trust in AI systems.
:::

## Taxonomy of Interpretability Methods

::: {.columns}
::: {.column width="50%"}
**Intrinsic (White-box)**

- Interpretability built into model structure
- Examples: Linear models, short decision trees, sparse models
- Understand by examining model internals
- *Today's focus*

:::

::: {.column width="50%"}
**Post-hoc (Black-box)**

- Explain after training
- Works with any model (neural nets, ensembles)
- Examples: LIME, SHAP, saliency maps
- *Next week's topic*

:::
:::

**Additional dimensions:**
Model-specific vs Model-agnostic | Local vs Global | Feature importance vs Feature effects

::: footer
[Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html)
:::

::: {.notes}
Intrinsic interpretability means the model structure itself is understandable - you can look at a decision tree and trace the logic. Post-hoc methods generate explanations after the fact for any model. We also distinguish: model-specific (works for one type) vs model-agnostic (works for any), local (explains one prediction) vs global (explains overall behavior), and feature importance (which features matter) vs feature effects (how do features influence predictions). Today we focus on intrinsically interpretable white-box models.
:::

## Outline
- <span style="color: grey;">Model Interpretation and Explanation</span>
- White-box Approaches and Visualizations
- <span style="color: grey;">Related Research in VIS & AI</span>

## White-box Models

We discuss the following models that are intrinsically interpretable:

- Linear Regression
- Generalized Additive Models (GAM)
- Tree-based Models
- Decision Rules

::: {.notes}
These model families offer varying degrees of expressiveness and interpretability. Linear models are simplest but most limited; GAMs add non-linear flexibility; trees provide natural visual structure; rules offer explicit logic.
:::

## Linear Regression
Linear models can be used to model the dependence of a regression target y on some features x in a format as below:
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n + \varepsilon\end{equation}

The predicted target $y$ is a linear combination of the weighted features $\beta_i x_i$.
The estimated linear equation is a hyperplane in the feature/target space (a simple line in the case of a single feature). 

The weights specify the slope (gradient) of the hyperplane in each direction.

::: {.notes}
Linear regression remains one of the most interpretable models. Each coefficient directly tells us how much the prediction changes when that feature increases by one unit, holding all else constant. This direct interpretability makes linear models valuable for policy decisions and scientific inference.
::: 

## Linear Regression
![](figs/white-box/illustration1.png)

::: {.notes}
This visualization shows the geometric interpretation of linear regression as fitting a hyperplane through the data points. The residuals (vertical distances from points to the plane) are minimized during training using least squares optimization.
:::

## Linear Regression: An Example of Housing Price
![](figs/white-box/paper3.png)

How do you interpret the influence of each property on the prediction of housing price?

::: {.notes}
In this housing price model, we can directly read off the coefficients: each additional bedroom adds a certain amount to the price, each square foot contributes its value, proximity to schools has a measurable effect. The sign tells us direction (positive/negative), the magnitude tells us importance. This direct interpretability makes linear models valuable for explaining decisions to stakeholders.
:::


## Interpreting Linear Model Coefficients

**Basic interpretation:** An increase in feature $x_j$ by one unit changes the prediction by $\beta_j$ units

::: {.incremental}
- ✅ **Numerical features**: Direct marginal effect (holding others constant)
- ✅ **Categorical features**: Effect vs reference category
- ⚠️ **Scale-dependent**: Coefficients change with feature units
- ⚠️ **"Holding others constant"** assumes independence
:::

::: footer
Molnar, C. (2022). [*Interpretable Machine Learning*](https://christophm.github.io/interpretable-ml-book/limo.html). Chapter 4.1.
:::

::: {.notes}
The standard interpretation is deceptively simple: each coefficient tells you the marginal effect of that feature. But be careful - this interpretation assumes you can change one feature while holding others constant, which may not be realistic. For example, in housing, you can't easily change square footage without affecting other features. Coefficients are also scale-dependent - standardizing features helps comparisons.
:::

## Important Assumptions for Interpretation

Linear models make strong assumptions:

::: {.incremental}
1. **Linearity**: Effects are additive (no interactions unless explicitly added)
2. **Independence**: Features are not strongly correlated
3. **Homoscedasticity**: Constant error variance
4. **No multicollinearity**: Correlated features can flip coefficient signs!
:::

::: footer
Molnar, C. (2022). [*Interpretable Machine Learning*](https://christophm.github.io/interpretable-ml-book/limo.html). Chapter 4.1.
:::

::: {.notes}
When features are correlated (multicollinearity), coefficients become unstable and can even change signs in counterintuitive ways. For example, if square footage and number of rooms are highly correlated, their individual coefficients may not reflect their true importance. The model can still predict well, but individual coefficient interpretation becomes unreliable. Always check for multicollinearity using VIF (variance inflation factor) or correlation matrices.
:::

## Evaluation of Linear Regression Model

### R Square

$R^2$ (R-squared)
\begin{equation}
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\end{equation}

Mean Square Error (MSE)/Root Mean Square Error (RMSE)


\begin{equation}
MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\end{equation}


Mean Absolute Error (MAE)

\begin{equation}
MAE = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|
\end{equation}

::: {.notes}
R-squared tells us the proportion of variance explained by the model (ranges 0-1, higher is better). MSE/RMSE penalize large errors more heavily than MAE due to squaring. Choose metrics based on your application's tolerance for outliers - use MAE if you want robust estimates, MSE if large errors are particularly costly. Note that R-squared can be misleading with many features - adjusted R-squared accounts for model complexity.
:::


## Visual Analytics (VA) Systems for Linear Regression


::: {.columns}
::: {.column width="40%"}
![https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6634169
](figs/white-box/paper4.png)

:::

::: {.column width="60%"}
There is a trade-off between model complexity (number of features) and accuracy.

This VA system helps model building (feature ranking) and model validation.

:::
:::

::: {.notes}
This IEEE TVCG paper presents an interactive system for exploring the accuracy-complexity tradeoff in linear models. Users can interactively add or remove features and immediately see the impact on model performance metrics like R-squared and cross-validation error. This helps identify the simplest model that achieves acceptable accuracy.
:::



## Pros and Cons of Linear Models
::: {.columns}
::: {.column width="50%"}
Pros:

:::

::: {.column width="50%"}
Cons:


:::
:::

## Pros and Cons of Linear Models
::: {.columns}
::: {.column width="50%"}
Pros:


Easily interpretable

Statistical guarantees on inference (if assumptions are satisfied)

No hyperparameters (analytical solution)

:::

::: {.column width="50%"}
Cons:

$X$’s relationship with $Y$ can be non-linear. In these cases, linear regression may not provide good results.

If you don’t satisfy certain assumptions (namely normal distribution of residuals and homoscedasticity), then inference can be incorrect.

:::
:::

::: {.notes}
Linear models shine when relationships are truly linear and assumptions hold (normal residuals, homoscedasticity, no multicollinearity). But real-world data often violates these assumptions, leading to poor predictions and incorrect statistical inference. The analytical solution is fast but inflexible.
:::

## Limitations of Linear Models
- Features are assumed to follow Gaussian distribution
- No interactions between features

What if your dataset does not follow the assumptions?

::: {.notes}
The linearity assumption is particularly limiting. Many real phenomena have non-linear relationships (e.g., diminishing returns, threshold effects, exponential growth) and feature interactions (e.g., temperature × humidity jointly affecting comfort). When data violates linear assumptions, we need more flexible models like GAMs.
:::

## Generalized Additive Models (GAMs)

GAMs extend linear models by replacing linear terms with flexible shape functions:

\begin{equation}
g(\mathbb{E}[y|X]) = \beta_0 + \sum_{j=1}^{p} f_j(x_{j})
\end{equation}

**Key idea:** Replace $\beta_j x_j$ (linear) with $f_j(x_j)$ (flexible smooth function)

::: {.incremental}
- Each $f_j$ is learned from data (typically using splines)
- Maintains additive structure → still interpretable
- Can mix linear and non-linear terms
:::

::: footer
Molnar, C. (2022). [*Interpretable Machine Learning*](https://christophm.github.io/interpretable-ml-book/extend-lm.html). Chapter 4.2.
:::

::: {.notes}
GAMs replace linear terms with smooth functions that can capture non-linear patterns. Each feature gets its own learned shape function (often splines or smoothing functions). The model remains additive (no interactions by default) which preserves interpretability - we can visualize each function independently. The link function g allows for different target distributions (normal for regression, logistic for classification).
:::

## How GAMs Work: Splines as Building Blocks

GAMs use **splines** (piecewise polynomial functions) to approximate smooth curves:

::: {.columns}
::: {.column width="50%"}
**Technical approach:**
- Replace feature $x_j$ with basis functions
- Fit weights to these basis functions
- Add penalty term for smoothness
:::

::: {.column width="50%"}
**Interpretation:**
- Visualize each $f_j(x_j)$ as a curve
- Y-axis shows contribution to prediction
- Relative to mean prediction
:::
:::

::: footer
Molnar, C. (2022). [*Interpretable Machine Learning*](https://christophm.github.io/interpretable-ml-book/extend-lm.html). Chapter 4.2.
:::

::: {.notes}
Under the hood, GAMs convert each feature into multiple "basis functions" (like polynomial terms or spline segments). The model learns weights for these basis functions, similar to how linear regression learns coefficients. A smoothness penalty prevents overfitting by penalizing overly wiggly curves. This is controlled by cross-validation. The result: smooth, interpretable curves showing each feature's effect.
:::

## Generalized Additive Models (GAMs): An Example

\begin{equation}
Wage = f(year, age, education) = b_0 + f_1(year) + f_2(age) + f_3(education)
\end{equation}
![](figs/white-box/exp1.png)

::: {.notes}
This wage prediction example shows how GAMs capture different functional forms: year has a roughly linear upward trend, age shows a non-monotonic curve peaking in mid-career years, and education shows discrete jumps between levels. Each function is learned from data while the additive structure keeps the model interpretable.
:::


## Training GAMs (Backfitting)
![](figs/white-box/algo1.png)

::: {.notes}
The backfitting algorithm trains GAMs iteratively using a coordinate descent approach. Start with initial functions, then repeatedly update each function to fit the residuals from all other functions. This process converges to the optimal additive decomposition. It's computationally efficient and works well even with many features.
:::


## Generalized Additive Models (GAMs): Pros and Cons
::: {.columns}
::: {.column width="50%"}
Pros:


GAMs allow us to fit a non-linear $f_j$ to each $X_j$ , so that we can model non-linear relationships easily. 

The non-linear fits can potentially lead to better predictions.

Because the model is additive, we can examine the effect of each $X_j$ on $Y$ for each observation. This is useful for visualization.

:::

::: {.column width="50%"}
Cons:

GAMs are restricted to be additive. With many variables, important interactions can be missed or computationally infeasible to find.

:::
:::

::: {.notes}
GAMs balance flexibility and interpretability nicely - more expressive than linear models but still visualizable. But the no-interaction constraint can be limiting. If temperature and humidity interact to affect outcomes (e.g., heat index), a pure GAM will miss this synergistic effect and may underfit.
:::

## Explainable Boosting Machines
::: {.columns}
::: {.column width="65%"}
\begin{equation}
g(\mathbb{E}[y]) = \beta_0 + \sum f_j(x_j) 

\end{equation}

\begin{equation}
g(\mathbb{E}[y]) = \beta_0 + \sum f_j(x_j) + \sum f_{ij}(x_i, x_j)

\end{equation}

:::

::: {.column width="35%"}
However, as with linear regression, we can manually add interaction terms to the GAM model by including additional predictors of the form $X_j \times X_k$. In addition we can add low-dimensional interaction functions of the form $f_{jk}(X_j , X_k)$ into the model.
:::
:::

::: {.notes}
EBMs extend GAMs by automatically detecting and including pairwise interaction terms. This gives the model more expressiveness while keeping visualization tractable since we only need to show 1D shape functions and 2D interaction heatmaps. Microsoft's InterpretML library implements this efficiently using gradient boosting.
:::

## Explainable Boosting Machines
::: {.columns}
::: {.column width="65%"}
\begin{equation}
g(\mathbb{E}[y]) = \beta_0 + \sum f_j(x_j) 

\end{equation}

\begin{equation}
g(\mathbb{E}[y]) = \beta_0 + \sum f_j(x_j) + \sum f_{ij}(x_i, x_j)

\end{equation}

:::

::: {.column width="35%"}
What if we have a lot of interactions?
How do we choose our interactions?

:::
:::

::: {.notes}
Automatically selecting which interactions to include is an active research area. EBMs use gradient boosting to greedily select the most important pairwise interactions during training. The boosting process alternates between improving main effects and interactions, naturally prioritizing the most impactful terms.
:::

## Explainable Boosting Machines
<!-- ![]{./figures/exp2.png} -->
{{< video https://youtu.be/MREiHgHgl0k width=1080 height=720 >}}

::: {.notes}
This video from Microsoft Research demonstrates the InterpretML library and EBMs in action. Watch how the system automatically discovers important feature interactions and visualizes their effects on predictions using partial dependence plots and interaction heatmaps.
:::
<!-- ![](figs/white-box/exp2.png) -->

## Visualizing EBMs (or GAMs)
![Partial dependency plot](figs/white-box/exp3.png)

::: {.notes}
Partial dependence plots show how predictions change as we vary one feature while marginalizing over all others. The y-axis shows the average prediction change from the baseline, making it easy to see each feature's isolated effect. The rug plot at bottom shows the distribution of actual feature values.
:::

## Visualizing EBMs (or GAMs)
![Partial dependency plot](figs/white-box/exp4.png)

::: {.notes}
Here we see multiple partial dependence plots arranged in a dashboard layout. This allows comparing the relative importance and functional forms of different features at a glance. Features with flat lines have little effect, while steep curves indicate strong influence on predictions.
:::


## Visualizing EBMs (or GAMs)
![Partial dependency plot](figs/white-box/exp5.png)

::: {.notes}
This shows a 2D partial dependence plot for an interaction term between two features. The heatmap reveals how the two features jointly affect predictions, capturing synergistic effects that 1D plots would miss. Darker regions indicate combinations that strongly influence predictions.
:::

## Visualizing EBMs (or GAMs)
![Partial dependency plot](figs/white-box/exp6.png)

::: {.notes}
Individual Conditional Expectation (ICE) plots show how predictions change for individual instances rather than averaging across the dataset. Each line represents one data point. This reveals heterogeneity - different instances may respond differently to the same feature change, suggesting important interactions or subpopulations.
:::

## Visual Analytics (VA) Systems Using GAMs
![[Individual observation feature contributions](https://www.microsoft.com/en-us/research/publication/gamut-a-design-probe-to-understand-howdata-scientists-understand-machine-learning-models/)](figs/white-box/paper5.png)

::: {.notes}
GAMUT from Microsoft Research explores how data scientists interact with GAM visualizations through a design probe study. The system shows individual feature contributions for each observation, helping users understand both global patterns (overall feature effects) and local predictions (why this specific instance got this prediction).
:::

## Visual Analytics (VA) Systems Using GAMs
![](figs/white-box/paper6.jpg)

::: {.notes}
This system provides multiple coordinated views of GAM models, including partial dependence plots for understanding feature effects, residual analysis for identifying model weaknesses, and instance-level explanations. Interactive brushing and linking allows users to select interesting subgroups and explore their characteristics.
:::

## Visual Analytics (VA) Systems Using GAMs
![GAM Changer
](figs/white-box/paper7.png)

https://arxiv.org/pdf/2112.03245.pdf
https://github.com/interpretml/gam-changer

::: {.notes}
GAM Changer allows users to interactively edit learned shape functions to inject domain knowledge. Data scientists can manually adjust function shapes (e.g., enforcing monotonicity where it makes business sense) while seeing real-time impacts on model performance. This human-in-the-loop approach bridges data-driven learning with expert knowledge.
::: 

## Practice 1
Notebook:
https://colab.research.google.com/drive/1nKE6WIApebHi67yfhH6k5mZN86evLZOM?usp=sharing 

Some other libraries for PDP visualization:
https://scikit-learn.org/stable/modules/partial_dependence.html
https://interpret.ml/docs/pdp.html

::: {.notes}
This hands-on exercise walks through training GAMs with Python's interpret library and creating partial dependence visualizations. You'll explore how to identify non-linear patterns, detect feature interactions, and interpret model behavior through visualizations. Try comparing linear regression vs GAM on the same dataset.
::: 


## BREAK

## Tree-based Models: Example
![A decision tree of diabetes diagnosis](figs/white-box/paper8.png)

::: {.notes}
Decision trees provide natural visual explanations through their hierarchical structure. Each path from root to leaf represents a decision rule with explicit conditions. This diabetes diagnosis tree shows how glucose level, BMI, age, and other factors combine hierarchically to predict disease risk. Trees are highly interpretable when shallow but can become unwieldy when deep.
:::


## Visualization of Trees

https://treevis.net/ provides a gallery of tree visualization. These trees are used to visualize hierarchical structures, but not just tree-based machine learning models.
![](figs/white-box/paper9.png)

::: {.notes}
The TreeVis gallery showcases diverse tree visualization techniques from decades of HCI and InfoVis research. While many are designed for file systems, organizational charts, or taxonomies, the layout algorithms (node-link diagrams, treemaps, sunburst charts, icicle plots) all apply to ML decision trees. Choice of layout depends on what you want to emphasize - structure vs space utilization.
:::


## VA Systems Using Tree-based Models
::: {.columns}
::: {.column width="65%"}
![BaobabView](figs/white-box/paper10.png)


:::

::: {.column width="35%"}
It shows the flow of different class, and the class distribution in along the feature values.
:::
:::

::: {.notes}
BaobabView uses a custom layout optimized for classification trees. The width of edges encodes the number of instances flowing through each branch, and color shows class distribution at each node. This makes it easy to see where the model is confident vs uncertain, and which features do the most splitting work.
:::

## VA Systems Using Tree-based Models
![iForest](figs/white-box/paper11.png)

::: {.notes}
iForest visualizes random forest ensembles rather than individual trees. The system aggregates predictions across trees to show which regions of feature space have high consensus vs disagreement among ensemble members. Areas of disagreement may indicate decision boundaries, noisy data, or underspecified regions where the model is uncertain.
:::

## Interactive Construction and Analysis of Decision Trees

::: {.columns}
::: {.column width="50%"}
![Elzen & van Wijk, VAST 2011](figs/white-box/elzen-tree-overview.png)
:::

::: {.column width="50%"}
- Novel node-link visualization for very large decision trees
- Interactive construction: users can split nodes, prune branches
- Multiple views: overview, detail, rules
:::
:::

::: {.notes}
Elzen and van Wijk's VAST 2011 paper presents an interactive system for constructing and analyzing decision trees. The visualization uses a novel compact layout that can display very large trees. Users can interactively split nodes, prune branches, and explore different tree configurations. The system combines overview, detail, and rule extraction views.
:::

## Interactive Construction: Video Demonstration

{{< video figs/white-box/Elzen-Wijk-VAST2011.mov width=1080 height=720 >}}

::: {.notes}
This video demonstrates the interactive capabilities of the Elzen & van Wijk system. Watch how users can interactively construct decision trees, explore different splits, prune branches, and analyze the resulting tree structure. The system provides real-time feedback on tree performance as users make modifications.
:::

## Interactive Construction: Colored Flow Visualization

![Decision paths colored by class and features](figs/white-box/elzen-tree-colored.png)

::: {.notes}
This view shows decision paths colored by class labels (e.g., neck=no, lung, breast, bone=no). The flow visualization makes it easy to trace how different classes are separated through the tree. Width encodes the number of instances following each path. This design helps identify which features are most discriminative for each class.
:::

## Interactive Construction: Rule Visualization

![Decision rules with feature splits](figs/white-box/elzen-tree-rules.png)

::: {.notes}
This view shows the explicit decision rules at each node (e.g., y-bar ≤ 9.00, x2ybr > 2.00). The rainbow coloring helps distinguish different paths through the tree. Users can see the complete rule set from root to any leaf, making it easy to extract interpretable decision rules from the trained tree.
:::

## Decision Rules: Different Structures

::: {.columns}
::: {.column width="50%"}
![Rule List: If-then-else structure.
](figs/white-box/paper12.png)
Clearly see how the decision is made and which rule is more important.

:::

::: {.column width="50%"}
![Rule Set: A set of if-then rules.](figs/white-box/paper13.png)

The final decision is made based on a voting mechanism.

A recent user study shows that "if-then structure without any connecting else statements enables users to easily reason about the decision boundaries of classes."


:::
:::

::: {.notes}
Rule lists have a clear priority ordering with if-then-else chains - rules are tried sequentially until one fires. Rule sets allow multiple rules to fire simultaneously and vote on the final decision. User studies show people find rule sets more intuitive because they don't require mentally tracking a cascading else chain. Each rule stands independently.
:::

## Decision Rules: Different Structures
Disjunctive normal form (DNF, OR-of-ANDs)
Conjunctive normal form (CNF, AND-of-ORs)

![What form does this rule set follow?](figs/white-box/paper14.png)

::: {.notes}
This example shows DNF (Disjunctive Normal Form) rules: each rule is a conjunction (AND) of conditions, and we predict positive if ANY rule fires (OR). DNF is more common in ML because it naturally represents disjoint decision regions in feature space. CNF would require all conditions across rules to be met simultaneously, which is less useful for classification.
:::

## Decision Rules: Visual Factors Influence Rule Understanding

::: {.columns}
::: {.column width="65%"}
![Paper: https://arxiv.org/abs/2109.09160](figs/white-box/paper15.png)

:::

::: {.column width="35%"}

Can different visualizations of rules lead to different level of understanding of rules?

If so, what are the visual factors influence understanding and how they play a role in rule understanding?


:::
:::

::: {.notes}
This research investigates how visual presentation affects rule comprehension. Factors like rule ordering, grouping, highlighting, and textual formatting all influence how quickly and accurately people understand rule-based models. Good visual design can make complex rule sets much more accessible to non-experts, while poor design obscures patterns.
:::


## Evaluation of Rules
Given a rule below:

If $X$, then class $Y$.

Support / Coverage of a rule:

\begin{equation}
\text{Support} = \frac{\text{number of instances that match the conditions in } X}{\text{total number of instances}}
\end{equation}

Confidence / Accuracy of a rule:


\begin{equation}
\text{Confidence} = \frac{\text{number of instances that match conditions in } X \text{ and belong to class } Y}{\text{number of instances that match conditions in } X}
\end{equation}

::: {.notes}
Support measures how frequently the rule applies (what fraction of data it covers). Confidence measures how accurate the rule is when it fires (what fraction of covered instances are correctly classified). Good rules balance both - high confidence but very low support means the rule is too specific. High support but low confidence means it's too general and inaccurate.
:::

## Global Surrogate
Imagine that we have a black-box model (too complex to understand the internal structure), can we use white-box models to help us understand the model behavior of the black-box model?
![](figs/white-box/illustration2.png)

::: {.notes}
Global surrogate models approximate a complex black-box model with an interpretable white-box model. Train a decision tree or rule set to mimic the black-box's predictions. This trades some accuracy for interpretability - you're explaining the black-box's behavior, not the underlying true relationship. Useful when you need interpretability but your best-performing model is opaque.
:::


## Global Surrogate
Open the black box by understanding a "surrogate model" that approximate the behavior of the original black-box model.
![](figs/white-box/illustration3.png)

::: {.notes}
The surrogate training process: feed data through the black-box to get predictions, then train an interpretable model (decision tree, linear model, rules) to predict what the black-box would predict. The surrogate's feature importances and structure reveal what the black-box learned. Check surrogate fidelity - how well does it match the black-box predictions?
:::

## However…
What you want:


![](figs/white-box/illustration4.png){width=540px}


What you get:


![](figs/white-box/illustration5.png)

::: {.notes}
The classic surrogate model problem: you want a simple, interpretable explanation that's also highly faithful to the black-box. But there's often a trade-off - simple surrogates may miss important patterns (low fidelity), while high-fidelity surrogates become too complex to interpret. This is the fundamental tension in explanation approaches.
:::


## VA System for Rule List
![RuleMatrix](figs/white-box/paper16.png)

::: {.notes}
RuleMatrix visualizes rule lists using a matrix layout where rows are rules and columns are features. Cell color/intensity shows feature values in each rule's conditions. This compact representation lets you quickly scan for redundant rules, identify which features are most commonly used, and spot patterns across the rule set. Interactive features support rule refinement and editing.
:::

## VA Systems for Rules in Random Forest
![Explainable Matrix](figs/white-box/paper17.png)

::: {.notes}
Explainable Matrix extends rule visualization to random forest ensembles. Each tree generates rules, and the system aggregates and visualizes rule consensus across the forest. Users can see which rules appear consistently vs which are unique to specific trees. This helps understand ensemble behavior and identify stable patterns that the forest relies on.
:::

## Other white-box models?
- Naive Bayes
- K-nearest neighbors
- etc.

::: {.notes}
Beyond the models we've covered, other naturally interpretable models include: Naive Bayes (shows probability contributions from each feature via Bayes rule), K-Nearest Neighbors (predictions explained by showing similar training examples), and Logistic Regression (similar to linear regression but for classification). Each provides different forms of interpretability suited to different explanation needs.
:::

## Practice 2
Notebook:
https://colab.research.google.com/drive/12LV2Z_1BbP3efACYp2QxzsPaOrIn8a8l?usp=sharing

::: {.notes}
This hands-on exercise covers decision tree training, visualization, and rule extraction with sklearn. You'll experiment with tree depth, pruning strategies, and extracting interpretable rules from trained trees. Try comparing different tree visualization libraries and see how tree structure affects interpretability and performance.
::: 





## Outline
- <span style="color: grey;">Model Interpretation and Explanation</span>
- <span style="color: grey;">White-box Approaches and Visualizations</span>
- Related Research in VIS & AI

## Manipulating and Measuring Model Interpretability

![](figs/white-box/paper18.png)

https://arxiv.org/abs/1802.07810

::: {.notes}
This paper asks fundamental questions: Can we quantitatively measure interpretability? Can we manipulate model structure to increase interpretability while maintaining performance? The authors propose metrics for tree complexity, sparsity, and other interpretability factors. This work is important because it moves interpretability from a vague concept to something measurable and optimizable.
:::

## Stop explaining black box machine learning models for high stakes decisions
![](figs/white-box/paper19.png)

https://www.nature.com/articles/s42256-019-0048-x

::: {.notes}
Cynthia Rudin's influential paper argues that for high-stakes decisions (healthcare, criminal justice, lending), we should use inherently interpretable models rather than explaining black-boxes post-hoc. Post-hoc explanations can be misleading, incomplete, or unfaithful to the model. Instead, invest effort in building accurate interpretable models from the start. This sparked important debates about the interpretability-accuracy tradeoff.
:::



## Slice Finder: Automated Data Slicing for Model Validation

::: {.columns}

::: {.column width="50%"}

![](figs/white-box/paper20.png)
:::

:::{.column width="50%"}
How about we use whether the model prediction is wrong or not to train a "surrogate tree"?
:::
:::



https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8731353

::: {.notes}
Slice Finder uses a clever approach: train a decision tree to predict where your model makes errors. The tree splits identify data slices where performance degrades. This automates the manual process of searching for problematic subgroups. The resulting tree provides an interpretable description of failure modes - "the model struggles when age > 65 AND income < 30k".
:::


## Toolkits
InterpretML: https://github.com/interpretml/interpret

::: {.notes}
InterpretML is Microsoft's open-source library for interpretable machine learning. It implements GAMs, Explainable Boosting Machines (EBMs), and various explanation techniques. The library includes both glass-box models (inherently interpretable) and black-box explanation methods (LIME, SHAP). It provides unified APIs and visualization tools, making it easy to compare different interpretability approaches. Highly recommended for practical work.
::: 
