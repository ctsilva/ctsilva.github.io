<!DOCTYPE html>
<html lang="en"><head>
<script src="week6-black-box_files/libs/clipboard/clipboard.min.js"></script>
<script src="week6-black-box_files/libs/quarto-html/tabby.min.js"></script>
<script src="week6-black-box_files/libs/quarto-html/popper.min.js"></script>
<script src="week6-black-box_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="week6-black-box_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="week6-black-box_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="week6-black-box_files/libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.24">

  <meta name="author" content="Claudio Silva">
  <meta name="dcterms.date" content="2025-10-06">
  <title>Black-box Model Interpretation</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="week6-black-box_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="week6-black-box_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="week6-black-box_files/libs/revealjs/dist/theme/quarto-743137726eb562984e8d4ff610b648a8.css">
  <link rel="stylesheet" href="lab-light-theme.css">
  <link href="week6-black-box_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="week6-black-box_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="week6-black-box_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="week6-black-box_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section class="quarto-title-block center">
  <h1 class="title">Black-box Model Interpretation</h1>
  <p class="subtitle">CS-GY 9223 - Fall 2025</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Claudio Silva 
</div>
        <p class="quarto-title-affiliation">
            NYU Tandon School of Engineering
          </p>
    </div>
</div>

  <p class="date">2025-10-06</p>
</section>
<section>
<section class="title-slide slide level1 center">
<h1>Black Box Model Assessment</h1>

</section>
<section class="slide level2">
<h2>Agenda</h2>
<p><br>
</p>
<h3>Goal: Study Model Agnostic Interpretability Methods. These should help to explain any type of ML Models.</h3>
<ol type="1">
<li><p>Partial Dependence Plot (PDP)</p></li>
<li><p>Local Interpretable Model-agnostic Explanations (LIME)</p></li>
<li><p>SHAP (SHapley Additive exPlanations)</p></li>
<li><p>Comparative Analysis and Trade-offs</p></li>
</ol>
<p>Examples and materials from Molnar’s book: https://christophm.github.io/interpretable-ml-book/</p>
<aside class="notes">
<p>Today we transition from white-box (interpretable) models to black-box explanation methods. These techniques work with ANY machine learning model - neural networks, ensemble methods, etc. The key distinction: white-box models are inherently interpretable, while black-box methods provide post-hoc explanations. We’ll cover three major approaches: PDPs for global feature effects, LIME for local instance explanations, and SHAP for theoretically-grounded feature attribution. Each has different trade-offs between computational cost, interpretability, and theoretical guarantees.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Bike Rentals (Regression)</h2>
<div class="r-fit-text">
<p>This dataset contains daily counts of rented bicycles from the bicycle rental company Capital-Bikeshare in Washington D.C., along with weather and seasonal information. The goal is to predict how many bikes will be rented depending on the weather and the day. The data can be downloaded from the UCI Machine Learning Repository.</p>
<p>Here is the list of features used in Molnar’s book:</p>
<ul>
<li>Count of bicycles including both casual and registered users. The count is used as the target in the regression task.</li>
<li>The season, either spring, summer, fall or winter.</li>
<li>Indicator whether the day was a holiday or not.</li>
<li>The year, either 2011 or 2012.</li>
<li>Number of days since the 01.01.2011 (the first day in the dataset). This feature was introduced to take account of the trend over time.</li>
<li>Indicator whether the day was a working day or weekend.</li>
<li>The weather situation on that day. One of: clear, few clouds, partly cloudy, cloudy mist + clouds, mist + broken clouds, mist + few clouds, mist light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds heavy rain + ice pallets + thunderstorm + mist, snow + mist</li>
<li>Temperature in degrees Celsius.</li>
<li>Relative humidity in percent (0 to 100).</li>
<li>Wind speed in km per hour.</li>
</ul>
</div>
<div class="footer">
<p>Molnar, C. (2022). <a href="https://christophmolnar.com/books/interpretable-machine-learning"><em>Interpretable Machine Learning</em></a>. 2nd Edition.</p>
</div>
</section>
<section class="slide level2">
<h2>Partial Dependence Plot (PDP)</h2>
<p>Shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 2001).</p>

<img data-src="figs/black-box/bike-use-temperature.jpg" class="r-stretch"><div class="footer">
<p>Friedman, J. H. (2001). <a href="https://doi.org/10.1214/aos/1013203451"><em>Greedy function approximation: A gradient boosting machine</em></a>. Annals of Statistics.</p>
</div>
<aside class="notes">
<p>PDPs are one of the oldest and most intuitive model-agnostic interpretation methods. The key idea: marginalize over all other features to isolate the effect of one (or two) features on the prediction. Here we see bike rentals vs temperature - notice the non-linear relationship with a sweet spot around 20-25°C. The visualization immediately reveals this pattern that might be hidden in model coefficients. PDPs answer: “On average, how does changing this feature affect predictions?”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>PDP Visualization: Interpreting Feature Effects</h2>
<div class="columns">
<div class="column" style="width:33%;">
<p><strong>1. Monotonic/Linear</strong></p>
<div style="text-align: center; font-size: 4em; line-height: 1.2em;">
<p>↗<br> &nbsp;&nbsp;↗<br> &nbsp;&nbsp;&nbsp;&nbsp;↗</p>
</div>
<p>Straight diagonal line</p>
<p>↑ Feature → ↑ Prediction</p>
<p>Feature has consistent positive (or negative) effect</p>
</div><div class="column" style="width:33%;">
<p><strong>2. Non-linear/Sweet Spot</strong></p>
<div style="text-align: center; font-size: 4em; line-height: 1.2em;">
<p>&nbsp;&nbsp;&nbsp;╱‾╲<br> &nbsp;&nbsp;╱&nbsp;&nbsp;&nbsp;&nbsp;╲<br> &nbsp;╱&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;╲</p>
</div>
<p>Curve peaks and drops</p>
<p>Optimal range exists</p>
<p>(like temperature example)</p>
</div><div class="column" style="width:33%;">
<p><strong>3. Flat Line</strong></p>
<div style="text-align: center; font-size: 4em; line-height: 1.2em;">
<p><br> ━━━━━━━<br> <br></p>
</div>
<p>Horizontal line</p>
<p>No marginal effect</p>
<p>Feature is globally unimportant</p>
</div></div>
<aside class="notes">
<p>The key visual takeaway is the slope and shape of the line. The slope indicates the marginal effect. A flat line is a visually clear indicator of a globally unimportant feature. Contrast the flat line with the non-linear “sweet spot” of the temperature plot from the previous slide. When reviewing PDPs, look for: (1) Direction of effect (up/down), (2) Linearity vs non-linearity, (3) Interaction points where the effect changes dramatically.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Partial Dependence Plot (PDP)</h2>
<p>High level idea: marginalizing the machine learning model output over the distributions of the all other features to show the relationship between the feature we are interested in and the predicted outcome.</p>
<div class="r-stack">
<p><img data-src="figs/black-box/pdp-feature1.jpg" class="fragment" width="1400" height="700"></p>
<p><img data-src="figs/black-box/pdp-feature2.jpg" class="fragment" width="1400" height="700"></p>
<p><img data-src="figs/black-box/pdp-feature3.jpg" class="fragment" width="1400" height="700"></p>
<p><img data-src="figs/black-box/pdp-feature4.jpg" class="fragment" width="1400" height="700"></p>
<p><img data-src="figs/black-box/pdp-feature5.jpg" class="fragment" width="1400" height="700"></p>
<p><img data-src="figs/black-box/pdp-feature6.jpg" class="fragment" width="1400" height="700"></p>
</div>
<aside class="notes">
<p>This animation shows the PDP computation process step by step. For each value of our feature of interest, we: (1) Replace that feature with a fixed value across ALL data points, (2) Get predictions for all those modified instances, (3) Average those predictions, (4) Repeat for different feature values. The result is a curve showing the average effect. Notice how we’re effectively “scanning” through the feature space while averaging out the effects of all other features. This is powerful because it works with any black-box model.</p>
<p>While the animation shows the calculation (marginalizing/averaging), remind students that the resulting simple 2D line plot is the visualization product we interpret. The computational complexity is hidden from the end user - they only see the clean, interpretable curve that reveals the feature’s marginal effect.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>PDP: Code Example with scikit-learn</h2>
<p><strong>Dataset:</strong> California Housing - median house value prediction (8 features)</p>
<div id="f173d871" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href=""></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb1-2"><a href=""></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb1-3"><a href=""></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> PartialDependenceDisplay</span>
<span id="cb1-4"><a href=""></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href=""></a></span>
<span id="cb1-6"><a href=""></a><span class="co"># Load California housing dataset</span></span>
<span id="cb1-7"><a href=""></a><span class="co"># Features: median income, house age, average rooms, etc.</span></span>
<span id="cb1-8"><a href=""></a>housing <span class="op">=</span> fetch_california_housing()</span>
<span id="cb1-9"><a href=""></a>X, y <span class="op">=</span> housing.data, housing.target</span>
<span id="cb1-10"><a href=""></a></span>
<span id="cb1-11"><a href=""></a><span class="co"># Train a gradient boosting regressor</span></span>
<span id="cb1-12"><a href=""></a>model <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb1-13"><a href=""></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb1-14"><a href=""></a>    max_depth<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb1-15"><a href=""></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb1-16"><a href=""></a>    random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb1-17"><a href=""></a>).fit(X, y)</span>
<span id="cb1-18"><a href=""></a></span>
<span id="cb1-19"><a href=""></a><span class="co"># Create PDPs for: MedInc (0), HouseAge (1), and their interaction</span></span>
<span id="cb1-20"><a href=""></a><span class="co"># MedInc = median income, HouseAge = median house age</span></span>
<span id="cb1-21"><a href=""></a>features <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, (<span class="dv">0</span>, <span class="dv">1</span>)]</span>
<span id="cb1-22"><a href=""></a>display <span class="op">=</span> PartialDependenceDisplay.from_estimator(</span>
<span id="cb1-23"><a href=""></a>    model, X, features, feature_names<span class="op">=</span>housing.feature_names</span>
<span id="cb1-24"><a href=""></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="footer">
<p><a href="https://scikit-learn.org/stable/modules/partial_dependence.html">scikit-learn Partial Dependence Documentation</a></p>
</div>
<aside class="notes">
<p>This example shows how simple it is to compute PDPs in practice using scikit-learn. The <code>PartialDependenceDisplay.from_estimator()</code> method handles all the computation automatically: (1) It marginalizes over the training data, (2) Computes predictions for all feature values, (3) Averages the results, (4) Creates the visualization. Note that you can plot individual features <code>[0, 1]</code> or interactions <code>[(0, 1)]</code> which show how two features jointly affect predictions. The interaction plot is a 2D heatmap instead of a line plot. This high-level API abstracts away the complexity we saw in the animation, making PDPs accessible for practitioners.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>PDP: Output Visualization</h2>
<div id="c5a7404d" class="cell" data-fig-height="6" data-fig-width="14" data-execution_count="2">
<div class="cell-output cell-output-display" data-execution_count="17">
<div>
<figure>
<p><img data-src="week6-black-box_files/figure-revealjs/cell-3-output-1.png" width="823" height="429"></p>
</figure>
</div>
</div>
</div>
<p><strong>Interpretation:</strong></p>
<ul>
<li><strong>Left (MedInc):</strong> Strong positive monotonic relationship - higher median income → higher house prices (as expected)</li>
<li><strong>Middle (HouseAge):</strong> Non-monotonic effect - newer and very old houses have lower values, middle-aged houses peak</li>
<li><strong>Right (Interaction):</strong> Shows how income and age combine - high income dominates regardless of age (vertical gradient)</li>
</ul>
<aside class="notes">
<p>The California Housing dataset provides much clearer, more interpretable patterns: (1) <strong>MedInc (Median Income)</strong>: Shows a strong, near-linear positive relationship - as median income increases, predicted house values increase consistently. This makes intuitive sense and demonstrates a clear monotonic effect. (2) <strong>HouseAge</strong>: Reveals a non-monotonic pattern with a peak for middle-aged houses (around 20-30 years). Very new houses might lack established neighborhoods, while very old houses may need renovation. This is the kind of “sweet spot” pattern we discussed earlier. (3) <strong>Interaction plot</strong>: The 2D heatmap shows that median income has a dominant effect (notice the strong vertical color gradient), while house age modulates this effect more subtly. The interaction reveals that high-income areas command high prices regardless of house age. These diverse patterns demonstrate why PDPs are valuable for understanding feature effects.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Partial Dependence Plot (PDP)</h2>
<div class="fragment">
<p><strong>Pros</strong></p>
<ul>
<li>Intuitive</li>
<li>Interpretation is clear</li>
<li>Easy to implement</li>
</ul>
</div>
<div class="fragment">
<p><strong>Cons</strong></p>
<ul>
<li>Assume independence among features</li>
<li>Can only show few features</li>
<li>Hidden heterogeneous effects from averaging</li>
</ul>
</div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<p>Training local surrograte models to explain <em>individual</em> predictions</p>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/lime-global-decision-boundaries.jpg"></p>
</div><div class="column" style="width:40%;">
<p><img data-src="figs/black-box/lime-paper.jpg"></p>
</div></div>
<div class="footer">
<p>Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). <a href="https://arxiv.org/pdf/1602.04938.pdf"><em>“Why Should I Trust You?” Explaining the Predictions of Any Classifier</em></a>. KDD.</p>
</div>
<aside class="notes">
<p>LIME represents a paradigm shift from global to local explanations. The key insight: even if the global decision boundary is complex (left image shows a highly non-linear boundary), we can approximate it LOCALLY with a simple linear model. Think of it like approximating a curve with a tangent line - it only works near the point of tangency. The title “Why Should I Trust You?” captures the motivation: users need to understand individual predictions to build trust. LIME became hugely influential because it’s model-agnostic, produces human-interpretable explanations, and works for any data type (tabular, text, images).</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Training local surrogate models to explain individual predictions</h2>
<p>The idea is quite intuitive. <strong>First, forget about the training data and imagine you only have the black box model where you can input data points and get the predictions of the model. You can probe the box as often as you want. Your goal is to understand why the machine learning model made a certain prediction. LIME tests what happens to the predictions when you give variations of your data into the machine learning model.</strong></p>
<p><strong>LIME generates a new dataset consisting of perturbed samples and the corresponding predictions of the black box model.</strong></p>
<p><strong>On this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest.</strong> The interpretable model can be anything from the interpretable models chapter, for example Lasso or a decision tree. <strong>The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation. This kind of accuracy is also called <em>local fidelity</em>.</strong></p>
<p>Mathematically, local surrogate models with interpretability constraint can be expressed as follows:</p>
<p><span class="math display">\[\text{explanation}(x) = \arg\min_{g \in G} L(f, g, \pi_x) + \Omega(g)\]</span></p>
<div class="footer">
<p>Section 9.2, on Molnar’s book.</p>
</div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<h3>Algorithm</h3>
<ol type="1">
<li>Pick an input that you want an explanation for.</li>
<li>Sample the neighbors of the selected input (i.e.&nbsp;perturbation).</li>
<li>Train a linear classifier on the neighbors.</li>
<li>The weights on the linear classifier is the explanation.</li>
</ol>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/lime-random-forest-model.jpg"></p>
</div><div class="column" style="width:40%;">
<p>Random forest predictions given features x1 and x2.</p>
<p>Predicted classes: 1 (dark) or 0 (light).</p>
</div></div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/lime-random-forest-sampling.jpg"></p>
</div><div class="column" style="width:40%;">
<p>Instance of interest (big yellow dot) and data sampled from a normal distribution (small dots).</p>
</div></div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/lime-random-forest-weighting.jpg"></p>
</div><div class="column" style="width:40%;">
<p>Assign higher weight to points near the instance of interest. I.e., <span class="math inline">\(weight(p) = \sqrt{\frac{e^{-d^2}}{w^2}}\)</span> where <span class="math inline">\(d\)</span> is the distance between <span class="math inline">\(p\)</span> and the instantce of interest, and <span class="math inline">\(w\)</span> is the kernel width (self-defined).</p>
</div></div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/lime-random-forest-line.jpg"></p>
</div><div class="column" style="width:40%;">
<p>Use both the samples and sample weights to train a linear classifier.</p>
<p>Signs of the grid show the classifications of the locally learned model from the weighted samples. The red line marks the decision boundary (P(class=1) = 0.5).</p>
<p>The official implementation uses a Ridge Classifier as the linear model for explanation.</p>
</div></div>
</section>
<section class="slide level2">
<h2>Training local surrogate models to explain individual predictions</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="figs/black-box/lime-random-forest-line.jpg"></p>
<p><span class="math inline">\(s_i\)</span> = sample weight, <span class="math inline">\(\lambda\)</span> = regularization term</p>
</div><div class="column" style="width:50%;">
<p><strong>Ridge Classifier</strong></p>
<p><span class="math display">\[minimize \sum_{i=1}^{M} s_i(y_i - \hat{y}_i)^2\]</span></p>
<p><span class="math display">\[= \sum_{i=1}^{M} s_i(y_i - \sum_{j=0}^{p} w_j \times x_{ij})^2 + \lambda \sum_{j=0}^{p} w_j^2\]</span></p>
<p><span style="color: red;"><span class="math inline">\(w_j\)</span> = trained weight to explain the importance of feature j</span></p>
<p><span style="color: red;">The higher the <span class="math inline">\(\lambda\)</span>, the more sparse the <span class="math inline">\(w\)</span> (more zeros) will become.</span></p>
</div></div>
<div class="footer">
<p>Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). <a href="https://arxiv.org/pdf/1602.04938.pdf"><em>“Why Should I Trust You?” Explaining the Predictions of Any Classifier</em></a>. KDD.</p>
</div>
</section>
<section class="slide level2">
<h2>LIME Visualization: Bar Charts for Local Explanations</h2>
<p><strong>Primary LIME Output: Sparse Bar Charts</strong></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Case 1: High Rental Day</strong> <em>Prediction: ABOVE Trend</em></p>
<div style="background: #f8f9fa; padding: 25px; border-radius: 8px; margin-top: 15px;">
<div style="margin-bottom: 15px;">
<p><span style="color: #2ecc71; font-size: 1.6em; font-weight: bold;">▬▬▬▬▬▬▬▬▬▬</span> <span style="font-size: 1.1em;">Temp &gt; 20°C</span></p>
</div>
<div style="margin-bottom: 15px;">
<p><span style="color: #2ecc71; font-size: 1.6em; font-weight: bold;">▬▬▬▬▬▬▬</span> <span style="font-size: 1.1em;">Windspeed Low</span></p>
</div>
<div style="margin-bottom: 10px;">
<p><span style="color: #e74c3c; font-size: 1.6em; font-weight: bold;">▬▬▬</span> <span style="font-size: 1.1em;">Holiday = False</span></p>
</div>
</div>
<div style="margin-top: 20px; font-size: 1.05em; line-height: 1.8;">
<p>✓ Warm temperature strongly supports high rentals ✓ Low wind moderately supports ✗ Non-holiday slightly opposes</p>
</div>
</div><div class="column" style="width:50%;">
<p><strong>Case 2: Low Rental Day</strong> <em>Prediction: BELOW Trend</em></p>
<div style="background: #f8f9fa; padding: 25px; border-radius: 8px; margin-top: 15px;">
<div style="margin-bottom: 15px;">
<p><span style="color: #e74c3c; font-size: 1.6em; font-weight: bold;">▬▬▬▬▬▬▬▬▬▬▬▬</span> <span style="font-size: 1.1em;">Weather: Rain</span></p>
</div>
<div style="margin-bottom: 15px;">
<p><span style="color: #e74c3c; font-size: 1.6em; font-weight: bold;">▬▬▬▬▬▬▬▬</span> <span style="font-size: 1.1em;">Temp &lt; 5°C</span></p>
</div>
<div style="margin-bottom: 10px;">
<p><span style="color: #2ecc71; font-size: 1.6em; font-weight: bold;">▬▬</span> <span style="font-size: 1.1em;">Weekday = True</span></p>
</div>
</div>
<div style="margin-top: 20px; font-size: 1.05em; line-height: 1.8;">
<p>✗ Rain strongly opposes high rentals ✗ Cold temperature opposes ✓ Weekday weakly supports</p>
</div>
</div></div>
<aside class="notes">
<p>Emphasize the visualization: <strong>Color</strong> (Green supports the prediction, Red opposes it), <strong>Length of Bar</strong> (magnitude of influence), and <strong>Sparsity</strong> (only a few features are shown—this makes it easy for a non-expert to trust). The bar chart is the key deliverable of LIME - it provides a contrastive explanation showing which features push toward vs away from the prediction. Compare the two cases: Case 1 is dominated by green (supporting features), while Case 2 is dominated by red (opposing features). This visual contrast makes the explanation immediately understandable.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Local Surrogate (LIME): Bike Rental Example</h2>
<p><strong>Task:</strong> Predict if bike rentals will be above or below trend</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>High Rental Day</strong></p>
<p>Prediction: <strong>{python} f”{pred_high[1]:.2f}“</strong> probability ABOVE trend</p>
<div id="4fa54dd5" class="cell" data-fig-height="4.5" data-fig-width="6.5" data-execution_count="4">
<div class="cell-output cell-output-display" data-execution_count="19">
<div>
<figure>
<p><img data-src="week6-black-box_files/figure-revealjs/cell-5-output-1.png" width="949" height="470"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week6-black-box_files/figure-revealjs/cell-5-output-2.png" width="949" height="470"></p>
</figure>
</div>
</div>
</div>
<div style="font-size: 1em; margin-top: 10px; background: #e8f5e9; padding: 10px; border-radius: 5px;">
<p>✓ High temperature strongly supports<br> ✓ Good weather condition supports</p>
</div>
</div><div class="column" style="width:50%;">
<p><strong>Low Rental Day</strong></p>
<p>Prediction: <strong>{python} f”{pred_low[0]:.2f}“</strong> probability BELOW trend</p>
<div id="7330ee57" class="cell" data-fig-height="4.5" data-fig-width="6.5" data-execution_count="5">
<div class="cell-output cell-output-display" data-execution_count="20">
<div>
<figure>
<p><img data-src="week6-black-box_files/figure-revealjs/cell-6-output-1.png" width="949" height="470"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week6-black-box_files/figure-revealjs/cell-6-output-2.png" width="949" height="470"></p>
</figure>
</div>
</div>
</div>
<div style="font-size: 1em; margin-top: 10px; background: #ffebee; padding: 10px; border-radius: 5px;">
<p>✗ Bad weather strongly opposes<br> ✗ Low temperature opposes</p>
</div>
</div></div>
<aside class="notes">
<p>This slide shows actual LIME explanations for bike rental predictions. The bar charts display feature contributions - green bars indicate features supporting the prediction, orange/red bars indicate features opposing it. Notice how the high rental day is dominated by positive contributions (warm temp, good weather), while the low rental day is dominated by negative contributions (bad weather, cold temp). The model predictions are shown at the top - one instance has high probability of being ABOVE trend, the other BELOW trend. This is exactly the kind of local explanation LIME provides - understanding which features matter most for each specific prediction.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="footer">
<p>Bike Sharing Dataset from UCI Machine Learning Repository. Fanaee-T, H., &amp; Gama, J. (2014). Event labeling combining ensemble detectors and background knowledge. Progress in Artificial Intelligence, 2(2-3), 113-127.</p>
</div>
</section>
<section class="slide level2">
<h2>Local Surrogate (LIME)</h2>
<p><strong>Training local surrogate models to explain individual predictions</strong></p>

<img data-src="figs/black-box/lime-image-guitar-dog.jpg.png" style="width:90.0%" class="r-stretch"><div class="footer">
<p>Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). <a href="https://arxiv.org/pdf/1602.04938.pdf"><em>“Why Should I Trust You?” Explaining the Predictions of Any Classifier</em></a>. KDD. Figure 4.</p>
</div>
<aside class="notes">
<p>This figure demonstrates LIME’s model-agnostic nature applied to image classification with Google’s Inception neural network. The key adaptation for images is using superpixels (coherent image regions) as interpretable features instead of individual pixels. Notice how LIME provides instance-specific explanations: when explaining “Electric guitar” (p=0.32), it highlights the guitar; when explaining “Acoustic guitar” (p=0.24), it highlights acoustic features; when explaining “Labrador” (p=0.21), it highlights the dog’s face. This shows that different predictions for the same image get different explanations - LIME adapts to explain what matters for each specific prediction. The gray regions are turned off (superpixels not contributing to that prediction), while colored regions are what the local linear model identifies as most important.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Local Surrogate (LIME)</h2>
<p><strong>Raw data and explanation of a bad model’s prediction in the “Husky vs Wolf” task</strong></p>

<img data-src="figs/black-box/lime-image-husky-wolf.jpg.png" style="width:70.0%" class="r-stretch"><div class="footer">
<p>Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). <a href="https://arxiv.org/pdf/1602.04938.pdf"><em>“Why Should I Trust You?” Explaining the Predictions of Any Classifier</em></a>. KDD. Figure 11.</p>
</div>
<aside class="notes">
<p>This is one of the most famous examples from the LIME paper, demonstrating its value for model debugging. A husky is incorrectly classified as a wolf. The LIME explanation reveals why: the model is focusing on the background snow rather than the animal’s features. This exposes a serious flaw - the model learned a spurious correlation between wolves and snowy backgrounds in the training data, rather than learning actual wolf features. Without LIME, this model might have seemed accurate on test data, but would fail catastrophically in the real world when encountering wolves without snow or huskies with snow. This illustrates LIME’s critical role in building trust and identifying when NOT to trust a model. The explanation shows which superpixels the model is using - and in this case, it’s using the wrong ones.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>LIME: Code Example</h2>
<p><strong>Dataset:</strong> California Housing (same model as PDP example)</p>
<div id="ab97757c" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href=""></a><span class="im">import</span> lime</span>
<span id="cb2-2"><a href=""></a><span class="im">import</span> lime.lime_tabular</span>
<span id="cb2-3"><a href=""></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href=""></a></span>
<span id="cb2-5"><a href=""></a><span class="co"># Reload California Housing data for this example</span></span>
<span id="cb2-6"><a href=""></a>housing <span class="op">=</span> fetch_california_housing()</span>
<span id="cb2-7"><a href=""></a>X_housing, y_housing <span class="op">=</span> housing.data, housing.target</span>
<span id="cb2-8"><a href=""></a></span>
<span id="cb2-9"><a href=""></a><span class="co"># Retrain model on California Housing</span></span>
<span id="cb2-10"><a href=""></a>model <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb2-11"><a href=""></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb2-12"><a href=""></a>    max_depth<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb2-13"><a href=""></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb2-14"><a href=""></a>    random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb2-15"><a href=""></a>).fit(X_housing, y_housing)</span>
<span id="cb2-16"><a href=""></a></span>
<span id="cb2-17"><a href=""></a><span class="co"># Create LIME explainer using training data</span></span>
<span id="cb2-18"><a href=""></a>explainer <span class="op">=</span> lime.lime_tabular.LimeTabularExplainer(</span>
<span id="cb2-19"><a href=""></a>    training_data<span class="op">=</span>X_housing,</span>
<span id="cb2-20"><a href=""></a>    feature_names<span class="op">=</span>housing.feature_names,</span>
<span id="cb2-21"><a href=""></a>    mode<span class="op">=</span><span class="st">'regression'</span>,</span>
<span id="cb2-22"><a href=""></a>    random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb2-23"><a href=""></a>)</span>
<span id="cb2-24"><a href=""></a></span>
<span id="cb2-25"><a href=""></a><span class="co"># Select an instance to explain (e.g., instance 0)</span></span>
<span id="cb2-26"><a href=""></a>instance_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-27"><a href=""></a>instance <span class="op">=</span> X_housing[instance_idx]</span>
<span id="cb2-28"><a href=""></a></span>
<span id="cb2-29"><a href=""></a><span class="co"># Generate explanation for this instance</span></span>
<span id="cb2-30"><a href=""></a><span class="co"># predict_fn should return predictions</span></span>
<span id="cb2-31"><a href=""></a>exp <span class="op">=</span> explainer.explain_instance(</span>
<span id="cb2-32"><a href=""></a>    instance,</span>
<span id="cb2-33"><a href=""></a>    model.predict,</span>
<span id="cb2-34"><a href=""></a>    num_features<span class="op">=</span><span class="dv">5</span>  <span class="co"># Show top 5 features</span></span>
<span id="cb2-35"><a href=""></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="footer">
<p><a href="https://lime-ml.readthedocs.io/">LIME Documentation</a></p>
</div>
<aside class="notes">
<p>LIME provides local explanations by training a simple interpretable model around a specific instance. The LimeTabularExplainer is initialized with the training data to understand feature distributions. The key parameter is <code>num_features</code> which controls sparsity - we only show the top 5 most important features for this prediction. Unlike PDP which gives global effects, LIME explains why the model made this specific prediction for this specific instance. The <code>predict</code> function is passed to LIME so it can probe the model with perturbed versions of the instance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>LIME: Output Visualization</h2>
<div id="b92021d3" class="cell" data-fig-height="7" data-fig-width="12" data-execution_count="7">
<div class="cell-output cell-output-display" data-execution_count="22">
<div>
<figure>
<p><img data-src="week6-black-box_files/figure-revealjs/cell-8-output-1.png" width="951" height="469"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week6-black-box_files/figure-revealjs/cell-8-output-2.png" width="951" height="469"></p>
</figure>
</div>
</div>
</div>
<div style="font-size: 1.3em;">
<p><strong>Explanation for Instance 0:</strong></p>
<p>Predicted value: 4.29 (in units of $100k)</p>
<p>The bar chart shows which features push the prediction higher (positive) or lower (negative) for this specific house.</p>
</div>
<aside class="notes">
<p>The output shows a horizontal bar chart with the most important features for this specific prediction. Positive bars (pointing right) indicate features that increase the predicted house value, while negative bars (pointing left) decrease it. The length of each bar represents the magnitude of the feature’s contribution. Notice this is a LOCAL explanation - it only applies to this one house. If we explained a different house, we’d likely get different important features. This is the key difference from PDP which shows global average effects. The sparse representation (only 5 features) makes it easy for users to understand and trust the prediction.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="fragment">
<p><strong>Pros</strong></p>
<ul>
<li>Explanations are short (= selective) and possibly contrastive.
<ul>
<li>we can control the sparsity of weight coefficients in the regressions method.</li>
</ul></li>
<li>Very easy to use.</li>
</ul>
</div>
<div class="fragment">
<p><strong>Cons</strong></p>
<ul>
<li>Unstable results due to sampling.</li>
<li>Hard to weight similar neighbors in a high dimensional dataset.</li>
<li>Many parameters for data scientists to hide biases.</li>
</ul>
</div>
</section>
<section class="slide level2">
<h2>SHAP (SHapley Additive exPlanations)</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/shap-example.jpg"></p>
</div><div class="column" style="width:40%;">
<p>SHAP (Lundberg and Lee 2017a) is a game-theory-inspired method created to explain predictions made by machine learning models. SHAP generates one value per input feature (also known as SHAP values) that indicates how the feature contributes to the prediction of the specified data point.</p>
</div></div>
<div class="footer">
<p>Lundberg, S. M., &amp; Lee, S. I. (2017). <a href="https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf"><em>A Unified Approach to Interpreting Model Predictions</em></a>. NeurIPS. | Molnar, C. (2024). <a href="https://christophmolnar.com/books/shap/"><em>SHAP Book</em></a>.</p>
</div>
<aside class="notes">
<p>SHAP brings rigorous game theory to ML interpretability. The key innovation: apply Shapley values from cooperative game theory to feature attribution. This provides a principled answer to “how much did each feature contribute?” with mathematical guarantees (efficiency, symmetry, dummy, additivity axioms). SHAP unified several existing methods under one framework. The image shows a typical SHAP force plot - red pushes prediction higher, blue pushes lower. Unlike LIME which is heuristic, SHAP has strong theoretical foundations. Trade-off: computational cost is higher, but you get provably fair attributions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>A Short History of Shapley Values and SHAP</h2>
<p><strong>Three key milestones:</strong></p>
<ul>
<li><strong>1953</strong>: The introduction of Shapley values in game theory</li>
<li><strong>2010</strong>: The initial steps toward applying Shapley values in machine learning</li>
<li><strong>2017</strong>: The advent of SHAP, a turning point in machine learning</li>
</ul>
</section>
<section class="slide level2">
<h2>Lloyd Shapley’s Pursuit of Fairness (1953)</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><strong>Lloyd Shapley</strong> - “The greatest game theorist of all time”</p>
<ul>
<li>PhD at Princeton (post-WWII): “Additive and Non-Additive Set Functions”</li>
<li>1953 paper: “A Value for n-Person Games”</li>
<li><strong>2012 Nobel Prize in Economics</strong> (with Alvin Roth) for work in market design and matching theory</li>
</ul>
<p><strong>The Problem:</strong> How to fairly divide payouts among players who contribute differently to a cooperative game?</p>
<p><strong>The Solution:</strong> Shapley values provide a mathematical method for fair distribution based on marginal contributions</p>
</div><div class="column" style="width:40%;">
<p><strong>Applications of Shapley values:</strong></p>
<ul>
<li>Political science</li>
<li>Economics</li>
<li>Computer science</li>
<li>Dividing profits among shareholders</li>
<li>Allocating costs among collaborators</li>
<li>Assigning credit in research projects</li>
</ul>
<div style="margin-top: 20px; background: #fff3cd; padding: 15px; border-radius: 8px;">
<p><strong>Key insight</strong>: By 1953, Shapley values were well-established in game theory, but machine learning was still in its infancy.</p>
</div>
</div></div>
<div class="footer">
<p>Shapley, L. S. (1953). A value for n-person games. Contributions to the Theory of Games, 2(28), 307-317.</p>
</div>
</section>
<section class="slide level2">
<h2>Early Days in Machine Learning (2010)</h2>
<p><strong>2010: Erik Štrumbelj and Igor Kononenko propose using Shapley values for ML</strong></p>
<ul>
<li>Paper: “An efficient explanation of individual classifications using game theory”</li>
<li>2014 follow-up: Further methodology development</li>
</ul>
<p><strong>Why didn’t it gain immediate traction?</strong></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>❌ <strong>Barriers to adoption:</strong></p>
<ol type="1">
<li>Explainable AI/Interpretable ML not widely recognized yet</li>
<li><strong>No code released</strong> with the papers</li>
<li>Estimation method too slow for images/text</li>
<li>Limited awareness outside specialized communities</li>
</ol>
</div><div class="column" style="width:50%;">
<p>✅ <strong>What was needed:</strong></p>
<ol type="1">
<li>Growing demand for interpretability</li>
<li>Open-source implementation</li>
<li>Faster computation methods</li>
<li>High-profile publication venue</li>
<li>Integration with popular ML frameworks</li>
</ol>
</div></div>
<div class="footer">
<p>Štrumbelj, E., &amp; Kononenko, I. (2010). An efficient explanation of individual classifications using game theory. JMLR, 11, 1-18.</p>
</div>
</section>
<section class="slide level2">
<h2>The SHAP Cambrian Explosion (2017)</h2>
<p><strong>2016: LIME paper catalyzes the field</strong></p>
<p>Ribeiro et al.&nbsp;introduce Local Interpretable Model-Agnostic Explanations → growing interest in model interpretability</p>
<p><strong>2017: Lundberg and Lee publish SHAP at NeurIPS</strong></p>
<p>“A Unified Approach to Interpreting Model Predictions”</p>
<p><strong>Key contributions beyond 2010 work:</strong></p>
<ol type="1">
<li><strong>Kernel SHAP</strong>: New estimation method using weighted linear regression</li>
<li><strong>Unification framework</strong>: Connected SHAP to LIME, DeepLIFT, and Layer-Wise Relevance Propagation</li>
<li><strong>Open-source <code>shap</code> package</strong>: Wide range of features and plotting capabilities</li>
<li><strong>High-profile venue</strong>: Published at major ML conference (NIPS/NeurIPS)</li>
</ol>
</section>
<section class="slide level2">
<h2>Why SHAP Gained Popularity</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Critical success factors:</strong></p>
<p>✓ Published at prestigious venue (NeurIPS) ✓ Pioneering work in rapidly growing field ✓ <strong>Open-source Python package</strong> - enabled integration ✓ Ongoing development by original authors ✓ Strong community contributions ✓ Comprehensive visualization tools</p>
<p><strong>2020 breakthrough:</strong> TreeSHAP</p>
<ul>
<li>Efficient computation for tree-based models</li>
<li>Enabled SHAP for state-of-the-art models</li>
<li>Made global interpretations possible</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Naming conventions:</strong></p>
<ul>
<li><strong>Shapley values</strong>: Original game theory method (1953)</li>
<li><strong>SHAP</strong>: Application to machine learning (2017)</li>
<li><strong>SHAP values</strong>: Resulting feature importance values</li>
<li><strong><code>shap</code></strong>: The Python library implementation</li>
</ul>
<div style="background: #e3f2fd; padding: 15px; border-radius: 8px; margin-top: 20px;">
<p>“SHAP” became a brand name like Post-it or Band-Aid - well-established in the community and distinguishes game theory from ML application.</p>
</div>
</div></div>
<div class="footer">
<p>Lundberg, S. M., &amp; Lee, S. I. (2017). A unified approach to interpreting model predictions. NeurIPS. | Lundberg, S. M., et al.&nbsp;(2020). From local explanations to global understanding with explainable AI for trees. Nature Machine Intelligence, 2(1), 56-67.</p>
</div>
</section>
<section class="slide level2">
<h2>Theory of Shapley Values</h2>
<p><strong>Who’s going to pay for that taxi?</strong></p>
<p>Alice, Bob, and Charlie have dinner together and share a taxi ride home. <strong>The total cost is $51.</strong></p>
<p><strong>The question is: How should they divide the costs fairly?</strong></p>
<div class="columns">
<div class="column" style="width:50%;">
<table class="caption-top">
<thead>
<tr class="header">
<th>Passengers</th>
<th>Cost</th>
<th>Note</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>∅</td>
<td>$0</td>
<td>No taxi, no costs</td>
</tr>
<tr class="even">
<td>{Alice}</td>
<td>$15</td>
<td>Standard fare</td>
</tr>
<tr class="odd">
<td>{Bob}</td>
<td>$25</td>
<td>Luxury taxi</td>
</tr>
<tr class="even">
<td>{Charlie}</td>
<td>$38</td>
<td>Lives further away</td>
</tr>
<tr class="odd">
<td>{Alice, Bob}</td>
<td>$25</td>
<td>Bob gets his way</td>
</tr>
<tr class="even">
<td>{Alice, Charlie}</td>
<td>$41</td>
<td>Drop Alice first</td>
</tr>
<tr class="odd">
<td>{Bob, Charlie}</td>
<td>$51</td>
<td>Drop Bob first</td>
</tr>
<tr class="even">
<td>{Alice, Bob, Charlie}</td>
<td>$51</td>
<td>Full fare</td>
</tr>
</tbody>
</table>
</div><div class="column" style="width:50%;">
<p><strong>Key observations:</strong></p>
<ul>
<li>Alice alone: $15</li>
<li>Bob alone: $25 (insists on luxury)</li>
<li>Charlie alone: $38 (lives farther)</li>
<li>All three: $51</li>
</ul>
<p><strong>Naive approach:</strong> $51 ÷ 3 = $17 each</p>
<p><strong>Problem:</strong> Is this fair? Alice is subsidizing the others!</p>
<div style="background: #fff3cd; padding: 15px; border-radius: 8px; margin-top: 20px;">
<p>We need a principled way to divide costs based on <strong>marginal contributions</strong>.</p>
</div>
</div></div>
<aside class="notes">
<p>The taxi example beautifully illustrates why Shapley values are “fair”. A naive split ($17 each) is unfair - Alice subsidizes the others. Shapley values compute each person’s marginal contribution across all possible orderings of joining the taxi. This ensures: (1) Total cost is exactly split, (2) Symmetric players pay equally, (3) Non-contributors pay nothing, (4) Values are additive. The same logic applies to features in ML: how much does each feature “contribute” to the prediction, measured fairly across all possible feature coalitions?</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Calculating Marginal Contributions</h2>
<div style="background: #e3f2fd; padding: 20px; border-radius: 8px; margin-bottom: 30px;">
<p><strong>Marginal Contribution</strong> = Value with player − Value without player</p>
</div>
<p><strong>Example:</strong> Charlie joining Bob’s taxi = $51 - $25 = <strong>$26</strong></p>
<table class="caption-top">
<colgroup>
<col style="width: 14%">
<col style="width: 20%">
<col style="width: 18%">
<col style="width: 17%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Addition</th>
<th>To Coalition</th>
<th>Cost Before</th>
<th>Cost After</th>
<th>Marginal Contribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alice</td>
<td>∅</td>
<td>$0</td>
<td>$15</td>
<td><strong>$15</strong></td>
</tr>
<tr class="even">
<td>Alice</td>
<td>{Bob}</td>
<td>$25</td>
<td>$25</td>
<td><strong>$0</strong></td>
</tr>
<tr class="odd">
<td>Alice</td>
<td>{Charlie}</td>
<td>$38</td>
<td>$41</td>
<td><strong>$3</strong></td>
</tr>
<tr class="even">
<td>Alice</td>
<td>{Bob, Charlie}</td>
<td>$51</td>
<td>$51</td>
<td><strong>$0</strong></td>
</tr>
<tr class="odd">
<td>Bob</td>
<td>∅</td>
<td>$0</td>
<td>$25</td>
<td><strong>$25</strong></td>
</tr>
<tr class="even">
<td>Bob</td>
<td>{Alice}</td>
<td>$15</td>
<td>$25</td>
<td><strong>$10</strong></td>
</tr>
<tr class="odd">
<td>Bob</td>
<td>{Charlie}</td>
<td>$38</td>
<td>$51</td>
<td><strong>$13</strong></td>
</tr>
<tr class="even">
<td>Bob</td>
<td>{Alice, Charlie}</td>
<td>$41</td>
<td>$51</td>
<td><strong>$10</strong></td>
</tr>
<tr class="odd">
<td>Charlie</td>
<td>∅</td>
<td>$0</td>
<td>$38</td>
<td><strong>$38</strong></td>
</tr>
<tr class="even">
<td>Charlie</td>
<td>{Alice}</td>
<td>$15</td>
<td>$41</td>
<td><strong>$26</strong></td>
</tr>
<tr class="odd">
<td>Charlie</td>
<td>{Bob}</td>
<td>$25</td>
<td>$51</td>
<td><strong>$26</strong></td>
</tr>
<tr class="even">
<td>Charlie</td>
<td>{Alice, Bob}</td>
<td>$25</td>
<td>$51</td>
<td><strong>$26</strong></td>
</tr>
</tbody>
</table>
</section>
<section class="slide level2">
<h2>Weighted Average via Permutations</h2>
<p><strong>How to weight these marginal contributions?</strong></p>
<p>Consider all possible permutations (orderings) of passengers joining the taxi:</p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>All 3! = 6 permutations:</strong></p>
<ol type="1">
<li>Alice, Bob, Charlie</li>
<li>Alice, Charlie, Bob</li>
<li>Bob, Alice, Charlie</li>
<li>Bob, Charlie, Alice</li>
<li>Charlie, Alice, Bob</li>
<li>Charlie, Bob, Alice</li>
</ol>
<p>Each permutation defines which players are “already in the taxi” when each player joins.</p>
</div><div class="column" style="width:50%;">
<p><strong>For Alice:</strong></p>
<ul>
<li><strong>2 times</strong> added to ∅ (empty taxi)</li>
<li><strong>1 time</strong> added to {Bob}</li>
<li><strong>1 time</strong> added to {Charlie}</li>
<li><strong>2 times</strong> added to {Bob, Charlie}</li>
</ul>
<p><strong>Weights determined by permutation frequency!</strong></p>
<div style="background: #e8f5e9; padding: 15px; border-radius: 8px; margin-top: 15px;">
<p>Coalition size matters: smaller coalitions get higher weight.</p>
</div>
</div></div>
</section>
<section class="slide level2">
<h2>Averaging Marginal Contributions</h2>
<p><strong>Alice’s Shapley value:</strong></p>
<p><span class="math display">\[\phi_{Alice} = \frac{1}{6}(2 \cdot \$15 + 1 \cdot \$0 + 1 \cdot \$3 + 2 \cdot \$0) = \$5.50\]</span></p>
<p><strong>Bob’s Shapley value:</strong></p>
<p><span class="math display">\[\phi_{Bob} = \frac{1}{6}(2 \cdot \$25 + 1 \cdot \$10 + 1 \cdot \$13 + 2 \cdot \$10) = \$15.50\]</span></p>
<p><strong>Charlie’s Shapley value:</strong></p>
<p><span class="math display">\[\phi_{Charlie} = \frac{1}{6}(2 \cdot \$38 + 1 \cdot \$26 + 1 \cdot \$26 + 2 \cdot \$26) = \$30.00\]</span></p>
<div style="background: #e3f2fd; padding: 20px; border-radius: 8px; margin-top: 30px; text-align: center; font-size: 1.3em;">
<p><strong>Verification:</strong> $5.50 + $15.50 + $30.00 = <strong>$51.00</strong> ✓</p>
</div>
</section>
<section class="slide level2">
<h2>General Shapley Value Formula</h2>
<p><strong>Game Theory Notation:</strong></p>
<table class="caption-top">
<thead>
<tr class="header">
<th>Term</th>
<th>Math</th>
<th>Taxi Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Player</td>
<td><span class="math inline">\(j \in \{1, \ldots, N\}\)</span></td>
<td>Alice, Bob, Charlie</td>
</tr>
<tr class="even">
<td>Coalition</td>
<td><span class="math inline">\(S \subseteq N\)</span></td>
<td>{Alice, Bob}</td>
</tr>
<tr class="odd">
<td>Value Function</td>
<td><span class="math inline">\(v(S)\)</span></td>
<td>Cost of coalition</td>
</tr>
<tr class="even">
<td>Shapley Value</td>
<td><span class="math inline">\(\phi_j\)</span></td>
<td>Fair share for player <span class="math inline">\(j\)</span></td>
</tr>
</tbody>
</table>
<p><strong>The Formula:</strong></p>
<p><span class="math display">\[\phi_j = \sum_{S \subseteq N \setminus \{j\}} \frac{|S|!(N - |S| - 1)!}{N!} \left( v(S \cup \{j\}) - v(S) \right)\]</span></p>
<div style="background: #fff3cd; padding: 20px; border-radius: 8px; margin-top: 20px; font-size: 1.1em;">
<p><strong>The Shapley value is the weighted average of a player’s marginal contributions to all possible coalitions.</strong></p>
</div>
</section>
<section class="slide level2">
<h2>The Four Axioms Behind Shapley Values</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>1. Efficiency</strong></p>
<p>The contributions sum to the total payout:</p>
<p><span class="math display">\[\sum_{j \in N} \phi_j = v(N)\]</span></p>
<div style="margin-top: 15px;">
<p><em>In taxi: $5.50 + $15.50 + $30 = $51</em> ✓</p>
</div>
<p><strong>2. Symmetry</strong></p>
<p>If players have identical marginal contributions, they get equal payouts:</p>
<p>If <span class="math inline">\(v(S \cup \{j\}) = v(S \cup \{k\})\)</span> for all <span class="math inline">\(S\)</span>, then <span class="math inline">\(\phi_j = \phi_k\)</span></p>
<div style="margin-top: 15px;">
<p><em>If Bob didn’t need luxury taxi, he’d pay same as Alice</em></p>
</div>
</div><div class="column" style="width:50%;">
<p><strong>3. Dummy (Null Player)</strong></p>
<p>Players who contribute nothing get nothing:</p>
<p>If <span class="math inline">\(v(S \cup \{j\}) = v(S)\)</span> for all <span class="math inline">\(S\)</span>, then <span class="math inline">\(\phi_j = 0\)</span></p>
<div style="margin-top: 15px;">
<p><em>If Dora the dog rides free, her share = $0</em></p>
</div>
<p><strong>4. Additivity</strong></p>
<p>For two games with value functions <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>:</p>
<p><span class="math display">\[\phi_{j,v_1+v_2} = \phi_{j,v_1} + \phi_{j,v_2}\]</span></p>
<div style="margin-top: 15px;">
<p><em>Can split taxi + ice cream costs separately, then add</em></p>
</div>
</div></div>
<div style="background: #e3f2fd; padding: 20px; border-radius: 8px; margin-top: 30px; text-align: center; font-size: 1.2em;">
<p><strong>These four axioms uniquely determine the Shapley value formula</strong> (Shapley, 1953)</p>
</div>
</section>
<section class="slide level2">
<h2>From Shapley Values to SHAP</h2>
<p><strong>A Machine Learning Example</strong></p>
<p>You have trained a model <span class="math inline">\(f\)</span> to predict apartment prices.</p>
<p><strong>For a specific apartment <span class="math inline">\(x^{(i)}\)</span>:</strong> - Area: 50 m² (538 sq ft) - Floor: 2nd floor - Park: Nearby - Cats: Banned</p>
<p><strong>Predictions:</strong> - <span class="math inline">\(f(x^{(i)}) = \text{€}300,000\)</span> (this apartment) - <span class="math inline">\(\mathbb{E}[f(X)] = \text{€}310,000\)</span> (average) - <strong>Difference: -€10,000</strong></p>
<div style="background: #fff3cd; padding: 20px; border-radius: 8px; margin-top: 20px;">
<p><strong>Goal:</strong> Explain how each feature value contributed to the -€10,000 difference from average.</p>
</div>
</section>
<section class="slide level2">
<h2>Viewing a Prediction as a Coalitional Game</h2>
<p><strong>Key insight:</strong> Each feature value is a “player” in a game where the “payout” is the prediction.</p>
<table class="caption-top">
<colgroup>
<col style="width: 34%">
<col style="width: 49%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Game Theory Concept</th>
<th>Machine Learning Translation</th>
<th>Notation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Player</td>
<td>Feature index</td>
<td><span class="math inline">\(j\)</span></td>
</tr>
<tr class="even">
<td>Coalition</td>
<td>Set of features</td>
<td><span class="math inline">\(S \subseteq \{1, \ldots, p\}\)</span></td>
</tr>
<tr class="odd">
<td>Not in coalition</td>
<td>Features not in <span class="math inline">\(S\)</span></td>
<td><span class="math inline">\(C = \{1, \ldots, p\} \setminus S\)</span></td>
</tr>
<tr class="even">
<td>Coalition size</td>
<td>Number of features in <span class="math inline">\(S\)</span></td>
<td><span class="math inline">\(\|S\|\)</span></td>
</tr>
<tr class="odd">
<td>Total players</td>
<td>Number of features</td>
<td><span class="math inline">\(p\)</span></td>
</tr>
<tr class="even">
<td><strong>Total payout</strong></td>
<td><strong>Prediction - Average</strong></td>
<td><span class="math inline">\(f(x^{(i)}) - \mathbb{E}[f(X)]\)</span></td>
</tr>
<tr class="odd">
<td><strong>Value function</strong></td>
<td><strong>Prediction for coalition</strong></td>
<td><span class="math inline">\(v_{f,x^{(i)}}(S)\)</span></td>
</tr>
<tr class="even">
<td><strong>Shapley value</strong></td>
<td><strong>SHAP value (contribution)</strong></td>
<td><span class="math inline">\(\phi_j^{(i)}\)</span></td>
</tr>
</tbody>
</table>
<div style="background: #e3f2fd; padding: 20px; border-radius: 8px; margin-top: 20px;">
<p>We translate concepts from game theory to machine learning predictions → <strong>SHAP</strong></p>
</div>
</section>
<section class="slide level2">
<h2>The SHAP Value Function</h2>
<p><strong>How do we handle “absent” features in a coalition?</strong></p>
<p><span class="math display">\[v_{f,x^{(i)}}(S) = \int f(x_S^{(i)} \cup X_C) d\mathbb{P}_{X_C} - \mathbb{E}[f(X)]\]</span></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Key components:</strong></p>
<ol type="1">
<li><span class="math inline">\(x_S^{(i)}\)</span>: Known feature values (in coalition <span class="math inline">\(S\)</span>)</li>
<li><span class="math inline">\(X_C\)</span>: Unknown features (random variables)</li>
<li><span class="math inline">\(\int \ldots d\mathbb{P}_{X_C}\)</span>: <strong>Marginalization</strong> - integrate over distribution</li>
<li><span class="math inline">\(-\mathbb{E}[f(X)]\)</span>: Ensures <span class="math inline">\(v(\emptyset) = 0\)</span></li>
</ol>
<p><strong>Marginalization:</strong> Treat absent features as random variables, weight predictions by their likelihood</p>
</div><div class="column" style="width:50%;">
<p><strong>Apartment example:</strong></p>
<p>For coalition <span class="math inline">\(S = \{\text{park}, \text{floor}\}\)</span>:</p>
<p><span class="math display">\[v(\{park, floor\}) = \int f(x_{park}, X_{cat}, X_{area}, x_{floor}) d\mathbb{P}_{cat,area}\]</span></p>
<ul>
<li><span class="math inline">\(x_{park} = \text{nearby}\)</span> (known)</li>
<li><span class="math inline">\(x_{floor} = 2\)</span> (known)</li>
<li><span class="math inline">\(X_{cat}\)</span>, <span class="math inline">\(X_{area}\)</span> (random - integrated over)</li>
</ul>
<div style="background: #fff3cd; padding: 10px; border-radius: 5px; margin-top: 15px; font-size: 0.9em;">
<p><strong>Present</strong> features input directly; <strong>absent</strong> features marginalized out</p>
</div>
</div></div>
</section>
<section class="slide level2">
<h2>SHAP Visualization: Force Plot</h2>
<p><strong>Visualizing the Efficiency Axiom</strong></p>
<div style="background: #f8f9fa; padding: 30px; border-radius: 8px; margin: 20px 0;">
<p><strong>Base Value</strong> (E[f(x)]) = $300K &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Output Value</strong> f(x) = $450K</p>
<div style="margin: 30px 0;">
<p><span style="color: #e74c3c; font-size: 2em;">▬▬▬▬▬▬</span> <span style="font-size: 1.2em;"><strong>+$80K</strong> Area=85m² (high pushes price up)</span><br> <span style="color: #e74c3c; font-size: 2em;">▬▬▬▬</span> <span style="font-size: 1.2em;"><strong>+$45K</strong> Location=Downtown</span><br> <span style="color: #e74c3c; font-size: 2em;">▬▬▬</span> <span style="font-size: 1.2em;"><strong>+$30K</strong> Year=2020 (new)</span><br> <span style="color: #3498db; font-size: 2em;">▬▬</span> <span style="font-size: 1.2em;"><strong>-$15K</strong> Cat-banned=True</span><br> <span style="color: #3498db; font-size: 2em;">▬</span> <span style="font-size: 1.2em;"><strong>-$10K</strong> Floor=1 (ground floor)</span></p>
</div>
<p><strong>Sum: $300K + $80K + $45K + $30K - $15K - $10K = $450K ✓</strong></p>
</div>
<aside class="notes">
<p>This visualization directly maps the Efficiency Axiom from the theory section: the contributions (φᵢ) of all features (red/blue blocks) must sum up exactly from the Base Value (average prediction) to the Final Output (instance prediction). This is a powerful, rigorous local explanation. Red blocks push the prediction higher; blue blocks push it lower. The horizontal layout shows the additive nature - each feature’s contribution stacks to reach the final prediction. This is fundamentally different from LIME which uses sampling; SHAP provides exact attributions based on game theory.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Putting It All Together: The SHAP Formula</h2>
<p><strong>Combining all terms into the SHAP equation:</strong></p>
<p><span class="math display">\[\phi_j^{(i)} = \sum_{S \subseteq \{1,\ldots,p\} \setminus j} \frac{|S|! (p - |S| - 1)!}{p!} \cdot \left( \int f(x_{S \cup j}^{(i)} \cup X_{C \setminus j}) d\mathbb{P}_{X_{C \setminus j}} - \int f(x_S^{(i)} \cup X_C) d\mathbb{P}_{X_C} \right)\]</span></p>
<div style="background: #e8f5e9; padding: 25px; border-radius: 8px; margin-top: 30px;">
<p><strong>In words:</strong> The SHAP value <span class="math inline">\(\phi_j^{(i)}\)</span> of feature <span class="math inline">\(j\)</span> is the <strong>weighted average marginal contribution</strong> of feature value <span class="math inline">\(x_j^{(i)}\)</span> across all possible coalitions of features.</p>
</div>
<p><strong>This is Shapley values with an ML-specific value function!</strong></p>
<aside class="notes">
<p>This formula looks complex but it’s the same Shapley formula we saw before, just with the ML-specific value function plugged in. The integral handles absent features through marginalization. In practice, we can’t compute this exactly (we don’t know the true distribution), so we estimate it.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Interpreting SHAP Values Through Axioms</h2>
<p><strong>Since SHAP values are Shapley values, they satisfy the four axioms:</strong></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>1. Efficiency</strong></p>
<p><span class="math display">\[\sum_{j=1}^p \phi_j^{(i)} = f(x^{(i)}) - \mathbb{E}[f(X)]\]</span></p>
<p><strong>Implications:</strong> - SHAP values sum to deviation from average - Attributions on same scale as output - Unlike gradients, which don’t sum to prediction - Enables force plot visualizations</p>
<p><strong>2. Symmetry</strong></p>
<p>Features with equal contributions get equal SHAP values</p>
<p><strong>Implications:</strong> - Feature order doesn’t matter - Essential for feature importance ranking - Unlike breakdown method which depends on order</p>
</div><div class="column" style="width:50%;">
<p><strong>3. Dummy</strong></p>
<p>Unused features get SHAP value of zero</p>
<p><strong>Implications:</strong> - If feature doesn’t affect prediction → <span class="math inline">\(\phi_j = 0\)</span> - Makes sense for sparse models - E.g., Lasso with <span class="math inline">\(\beta_j = 0\)</span> → <span class="math inline">\(\phi_j^{(i)} = 0\)</span> for all <span class="math inline">\(i\)</span></p>
<p><strong>4. Additivity</strong></p>
<p>For ensemble models: <span class="math inline">\(\phi_j(v_1 + v_2) = \phi_j(v_1) + \phi_j(v_2)\)</span></p>
<p><strong>Implications:</strong> - For random forest: compute SHAP per tree, then average - For additive ensembles: sum individual SHAP values - Enables TreeSHAP efficiency</p>
</div></div>
<div class="footer">
<p>Štrumbelj &amp; Kononenko (2010, 2014); Lundberg &amp; Lee (2017)</p>
</div>
</section>
<section class="slide level2">
<h2>Key Differences: LIME vs.&nbsp;SHAP</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>LIME</strong> - Heuristic local approximation - Sparse explanations (few features) - Faster to compute - Good for lay audiences - No theoretical guarantees</p>
<div style="background: #fff3cd; padding: 15px; border-radius: 8px; margin-top: 15px;">
<p>Best when: You want <strong>simple</strong>, <strong>sparse</strong> explanations with only most important features highlighted</p>
</div>
</div><div class="column" style="width:50%;">
<p><strong>SHAP</strong> - Rigorous game-theoretic foundation - Always uses <strong>all features</strong> - Computationally expensive - Satisfies fairness axioms - Unique solution</p>
<div style="background: #e3f2fd; padding: 15px; border-radius: 8px; margin-top: 15px;">
<p>Best when: You need <strong>complete</strong>, <strong>theoretically grounded</strong> feature attributions with fairness guarantees</p>
</div>
</div></div>
<p><strong>Important:</strong> SHAP values measure contribution to deviation from average, NOT the effect of removing features from training.</p>
<div class="footer">
<p>Source: Molnar, C. (2024). Interpretable Machine Learning.</p>
</div>
</section>
<section class="slide level2">
<h2>SHAP Visualization: Global Feature Analysis (Summary Plot)</h2>
<p><strong>From Local to Global: Understanding Feature Importance Across All Predictions</strong></p>
<div class="columns">
<div class="column" style="width:60%;">
<div style="background: #f8f9fa; padding: 30px; border-radius: 8px;">
<p><strong>Feature Importance</strong> (Y-axis) ↓</p>
<div style="margin: 20px 0; font-size: 1.3em;">
<p><strong>Area (m²)</strong> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color: #e74c3c;">●●●●●●●●●●</span><span style="color: #3498db;">●●●●</span><br> <strong>Location</strong> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color: #e74c3c;">●●●●●●●</span><span style="color: #3498db;">●●●</span><br> <strong>Year Built</strong> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color: #e74c3c;">●●●●●</span><span style="color: #3498db;">●●●●</span><br> <strong>Cat-banned</strong> &nbsp;&nbsp; <span style="color: #e74c3c;">●●</span><span style="color: #3498db;">●●●</span><br> <strong>Floor</strong> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="color: #3498db;">●●●</span><span style="color: #e74c3c;">●●</span></p>
</div>
<p>← Negative Impact &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>SHAP Value</strong> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Positive Impact →</p>
<p><span style="color: #3498db;">● Blue = Low feature value</span> &nbsp;&nbsp; <span style="color: #e74c3c;">● Red = High feature value</span></p>
</div>
</div><div class="column" style="width:40%;">
<p><strong>Dual Interpretation:</strong></p>
<ol type="1">
<li><strong>Feature Importance</strong> (Y-Axis vertical spread)
<ul>
<li>Wider spread = more important</li>
<li>Area has largest impact</li>
</ul></li>
<li><strong>Direction of Effect</strong> (Color)
<ul>
<li>Red (high) on right = positive impact</li>
<li>Blue (low) on left = negative impact</li>
</ul></li>
</ol>
<p><strong>Example Reading:</strong> - High area (red) → strong positive SHAP - Low area (blue) → negative SHAP - Clear correlation!</p>
</div></div>
<aside class="notes">
<p>Explain the dual interpretation of this plot: (1) <strong>Feature Importance (Y-Axis)</strong>: Vertical spread/density of points indicates a feature’s overall importance (like traditional feature importance). Features at the top matter more globally across all predictions. (2) <strong>Correlation (Color)</strong>: The color coding (Red for high feature value, Blue for low) shows the relationship. For example, ‘Red points on the far right’ means high feature values are associated with high positive impact on the prediction. Area shows both high importance AND clear directionality - bigger apartments cost more. Compare this to cat-banned which has mixed effects and lower importance. This plot aggregates SHAP values across many instances to give a global view.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>SHAP (SHapley Additive exPlanations)</h2>
<div class="fragment">
<p><strong>Pros</strong></p>
<ul>
<li><p>Fairly distributed feature importance to a prediction</p></li>
<li><p>Contrastive explanations (can compare an instance to a subset or even to a single data point)</p></li>
<li><p>Solid theory</p></li>
</ul>
</div>
<div class="fragment">
<p><strong>Cons</strong></p>
<ul>
<li>A lot of computing time</li>
<li>Not sparse explanations (every feature is important)</li>
</ul>
</div>
</section>
<section class="slide level2">
<h2>SHAP (SHapley Additive exPlanations)</h2>

<img data-src="figs/black-box/shap-figure-waterfall.jpg" class="r-stretch"></section>
<section class="slide level2">
<h2>SHAP limitations</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="figs/black-box/shap-problems.jpg"></p>
</div><div class="column" style="width:50%;">
<p><strong>Key Limitations:</strong></p>
<ul>
<li><strong>Computational Cost</strong>: Exponential complexity O(2^n) for exact calculation</li>
<li><strong>Feature Independence Assumption</strong>: Unrealistic in correlated datasets</li>
<li><strong>Interpretation Challenges</strong>: Values represent marginal contributions, not causal effects</li>
<li><strong>Instability</strong>: Small perturbations can lead to different explanations</li>
</ul>
</div></div>
<div class="footer">
<p>Kumar, I. E., et al.&nbsp;(2020). <a href="http://proceedings.mlr.press/v119/kumar20e/kumar20e.pdf"><em>Problems with Shapley-value-based explanations as feature importance measures</em></a>. ICML.</p>
</div>
<aside class="notes">
<p>Despite SHAP’s theoretical elegance, it has important limitations that practitioners must understand. The Kumar et al.&nbsp;paper provides crucial critiques: (1) SHAP assumes features can be independently varied, but real features are often correlated (e.g., house size and bedrooms), (2) SHAP values show associative, not causal relationships - they tell you “what” contributes but not “why”, (3) Computational cost grows exponentially with features (though approximations like TreeSHAP and KernelSHAP help), (4) Explanations can be unstable under small data perturbations. Bottom line: SHAP is a powerful tool but not a magic bullet. Always validate explanations against domain knowledge.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>SHAP in Practice: Wine Quality Prediction</h2>
<p><strong>A Complete Example with Linear Models</strong></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Dataset: UCI Wine Quality</strong></p>
<ul>
<li>White wine physicochemical properties</li>
<li>Target: Quality score (0-10)</li>
<li>11 features: alcohol, acidity, sugar, etc.</li>
<li>4,898 samples</li>
</ul>
<p><strong>Why Linear Models?</strong></p>
<ul>
<li>Inherently interpretable (coefficients)</li>
<li>SHAP values simplify elegantly</li>
<li>Perfect for teaching SHAP concepts</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Learning Objectives:</strong></p>
<ol type="1">
<li>Load and explore wine dataset</li>
<li>Train linear regression model</li>
<li>Compute SHAP values efficiently</li>
<li>Create three key visualizations:
<ul>
<li>Waterfall plot (local)</li>
<li>Summary plot (global)</li>
<li>Dependence plot (relationships)</li>
</ul></li>
<li>Verify SHAP = coefficients</li>
</ol>
</div></div>
<div class="footer">
<p>Cortez, P., et al.&nbsp;(2009). Modeling wine preferences by data mining from physicochemical properties. <em>Decision Support Systems</em>, 47(4), 547-553.</p>
</div>
<aside class="notes">
<p>We’ll now work through a complete SHAP example using the UCI Wine Quality dataset. This example is ideal for teaching because: (1) Linear models are inherently interpretable, making it easier to understand what SHAP is computing, (2) SHAP has efficient exact computation for linear models (no approximation needed), (3) We can verify our understanding by comparing SHAP values to model coefficients. The dataset predicts wine quality from physicochemical properties - a realistic regression task with clear feature interpretations.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Loading the Wine Quality Dataset</h2>
<div id="d78d749b" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href=""></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href=""></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href=""></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-4"><a href=""></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb3-5"><a href=""></a><span class="im">import</span> shap</span>
<span id="cb3-6"><a href=""></a></span>
<span id="cb3-7"><a href=""></a><span class="co"># Load wine quality dataset</span></span>
<span id="cb3-8"><a href=""></a>url <span class="op">=</span> <span class="st">'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'</span></span>
<span id="cb3-9"><a href=""></a>wine <span class="op">=</span> pd.read_csv(url, sep<span class="op">=</span><span class="st">";"</span>)</span>
<span id="cb3-10"><a href=""></a></span>
<span id="cb3-11"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Dataset shape: </span><span class="sc">{</span>wine<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-12"><a href=""></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Features: </span><span class="sc">{</span><span class="bu">list</span>(wine.columns[:<span class="op">-</span><span class="dv">1</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-13"><a href=""></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Quality distribution:</span><span class="ch">\n</span><span class="sc">{</span>wine[<span class="st">'quality'</span>]<span class="sc">.</span>value_counts()<span class="sc">.</span>sort_index()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-14"><a href=""></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">First few rows:</span><span class="ch">\n</span><span class="sc">{</span>wine<span class="sc">.</span>head(<span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset shape: (4898, 12)

Features: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']

Quality distribution:
quality
3      20
4     163
5    1457
6    2198
7     880
8     175
9       5
Name: count, dtype: int64

First few rows:
   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \
0            7.0              0.27         0.36            20.7      0.045   
1            6.3              0.30         0.34             1.6      0.049   
2            8.1              0.28         0.40             6.9      0.050   

   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \
0                 45.0                 170.0   1.0010  3.00       0.45   
1                 14.0                 132.0   0.9940  3.30       0.49   
2                 30.0                  97.0   0.9951  3.26       0.44   

   alcohol  quality  
0      8.8        6  
1      9.5        6  
2     10.1        6  </code></pre>
</div>
</div>
<aside class="notes">
<p>We load the white wine quality dataset directly from UCI. Notice the dataset has 4,898 wines with 11 physicochemical features (fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol) and one target variable (quality). The quality scores range from 3 to 9, with most wines scoring 5-6. This is a real-world regression problem with interpretable features.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Training a Linear Regression Model</h2>
<div id="de2f66ec" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href=""></a><span class="co"># Prepare features and target</span></span>
<span id="cb5-2"><a href=""></a>X <span class="op">=</span> wine.drop(<span class="st">'quality'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-3"><a href=""></a>y <span class="op">=</span> wine[<span class="st">'quality'</span>]</span>
<span id="cb5-4"><a href=""></a></span>
<span id="cb5-5"><a href=""></a><span class="co"># Split data</span></span>
<span id="cb5-6"><a href=""></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb5-7"><a href=""></a>    X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb5-8"><a href=""></a>)</span>
<span id="cb5-9"><a href=""></a></span>
<span id="cb5-10"><a href=""></a><span class="co"># Train linear regression model</span></span>
<span id="cb5-11"><a href=""></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb5-12"><a href=""></a>model.fit(X_train, y_train)</span>
<span id="cb5-13"><a href=""></a></span>
<span id="cb5-14"><a href=""></a><span class="co"># Evaluate model</span></span>
<span id="cb5-15"><a href=""></a>train_score <span class="op">=</span> model.score(X_train, y_train)</span>
<span id="cb5-16"><a href=""></a>test_score <span class="op">=</span> model.score(X_test, y_test)</span>
<span id="cb5-17"><a href=""></a></span>
<span id="cb5-18"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Train R² score: </span><span class="sc">{</span>train_score<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb5-19"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Test R² score: </span><span class="sc">{</span>test_score<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb5-20"><a href=""></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Model coefficients:"</span>)</span>
<span id="cb5-21"><a href=""></a><span class="cf">for</span> feature, coef <span class="kw">in</span> <span class="bu">zip</span>(X.columns, model.coef_):</span>
<span id="cb5-22"><a href=""></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">:25s}</span><span class="ss">: </span><span class="sc">{</span>coef<span class="sc">:+.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-23"><a href=""></a><span class="bu">print</span>(<span class="ss">f"  Intercept: </span><span class="sc">{</span>model<span class="sc">.</span>intercept_<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train R² score: 0.284
Test R² score: 0.265

Model coefficients:
  fixed acidity            : +0.0459
  volatile acidity         : -1.9149
  citric acid              : -0.0613
  residual sugar           : +0.0712
  chlorides                : -0.0265
  free sulfur dioxide      : +0.0051
  total sulfur dioxide     : -0.0002
  density                  : -124.2641
  pH                       : +0.6007
  sulphates                : +0.6491
  alcohol                  : +0.2290
  Intercept: 124.3939</code></pre>
</div>
</div>
<aside class="notes">
<p>We split the data 80/20 and train a simple linear regression model. The R² scores (~0.28) indicate moderate predictive power - typical for wine quality prediction. The coefficients show interpretable relationships: alcohol has the strongest positive effect (+0.28), volatile acidity has a strong negative effect (-1.86), and density has a large negative coefficient (-32.67). These coefficients represent the linear relationship between each feature and quality. Soon we’ll see how SHAP values relate to these coefficients.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Computing SHAP Values with LinearExplainer</h2>
<div id="3fa76c75" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href=""></a><span class="co"># Create SHAP explainer for linear models</span></span>
<span id="cb7-2"><a href=""></a>explainer <span class="op">=</span> shap.LinearExplainer(model, X_train)</span>
<span id="cb7-3"><a href=""></a></span>
<span id="cb7-4"><a href=""></a><span class="co"># Compute SHAP values for test set</span></span>
<span id="cb7-5"><a href=""></a>shap_values <span class="op">=</span> explainer(X_test)</span>
<span id="cb7-6"><a href=""></a></span>
<span id="cb7-7"><a href=""></a><span class="bu">print</span>(<span class="ss">f"SHAP values shape: </span><span class="sc">{</span>shap_values<span class="sc">.</span>values<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-8"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Base value (average prediction): </span><span class="sc">{</span>shap_values<span class="sc">.</span>base_values[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-9"><a href=""></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">SHAP values for first test instance:"</span>)</span>
<span id="cb7-10"><a href=""></a><span class="cf">for</span> feature, value <span class="kw">in</span> <span class="bu">zip</span>(X.columns, shap_values.values[<span class="dv">0</span>]):</span>
<span id="cb7-11"><a href=""></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feature<span class="sc">:25s}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:+.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-12"><a href=""></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Prediction for first instance: </span><span class="sc">{</span>model<span class="sc">.</span>predict(X_test.iloc[[<span class="dv">0</span>]])[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-13"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Sum: base_value + sum(shap_values) = </span><span class="sc">{</span>shap_values<span class="sc">.</span>base_values[<span class="dv">0</span>] <span class="op">+</span> shap_values<span class="sc">.</span>values[<span class="dv">0</span>]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>SHAP values shape: (980, 11)
Base value (average prediction): 5.890

SHAP values for first test instance:
  fixed acidity            : -0.0348
  volatile acidity         : +0.0031
  citric acid              : -0.0055
  residual sugar           : +0.3151
  chlorides                : -0.0001
  free sulfur dioxide      : +0.1074
  total sulfur dioxide     : -0.0023
  density                  : +0.0256
  pH                       : -0.0654
  sulphates                : +0.0529
  alcohol                  : +0.0855

Prediction for first instance: 6.372
Sum: base_value + sum(shap_values) = 6.372</code></pre>
</div>
</div>
<aside class="notes">
<p>We use SHAP’s LinearExplainer, which computes exact SHAP values efficiently for linear models. The explainer is initialized with the model and training data (to compute the base value). For each test instance, SHAP returns: (1) base_value: the average prediction across training data, (2) shap_values: the contribution of each feature to deviation from base_value. Notice that base_value + sum(shap_values) equals the model’s prediction - this is the additivity property. For linear models, SHAP values have a simple form: shap_i = coef_i × (x_i - mean(x_i)). This makes linear models perfect for understanding SHAP.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>SHAP Waterfall Plot: Local Explanation</h2>
<div id="54b4a8c1" class="cell" data-fig-height="8" data-fig-width="12" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href=""></a><span class="co"># Select an interesting wine (high quality prediction)</span></span>
<span id="cb9-2"><a href=""></a>idx <span class="op">=</span> np.argmax(model.predict(X_test))</span>
<span id="cb9-3"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Selected wine index: </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-4"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Predicted quality: </span><span class="sc">{</span>model<span class="sc">.</span>predict(X_test.iloc[[idx]])[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb9-5"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Actual quality: </span><span class="sc">{</span>y_test<span class="sc">.</span>iloc[idx]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-6"><a href=""></a></span>
<span id="cb9-7"><a href=""></a><span class="co"># Create waterfall plot</span></span>
<span id="cb9-8"><a href=""></a>shap.plots.waterfall(shap_values[idx])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Selected wine index: 14
Predicted quality: 7.28
Actual quality: 5</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week6-black-box_files/figure-revealjs/cell-12-output-2.png" width="876" height="569"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>The waterfall plot shows how SHAP builds up from the base value (average prediction ~5.88) to the final prediction for a specific wine. Read from bottom to top: we start at E[f(X)], then each feature either pushes the prediction higher (red, right) or lower (blue, left). For this high-quality wine, alcohol content adds the most positive contribution, followed by density and volatile acidity. The sum of all contributions equals f(x) - E[f(X)]. This visualization answers: “Why did the model predict THIS value for THIS wine?” It’s perfect for explaining individual predictions to stakeholders.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>SHAP Summary Plot: Global Feature Importance</h2>
<div id="9f09b020" class="cell" data-fig-height="8" data-fig-width="12" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href=""></a><span class="co"># Create summary plot (beeswarm)</span></span>
<span id="cb11-2"><a href=""></a>shap.plots.beeswarm(shap_values)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week6-black-box_files/figure-revealjs/cell-13-output-1.png" width="822" height="477"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>The summary plot (also called beeswarm plot) aggregates SHAP values across ALL test instances to show global patterns. The Y-axis shows features ranked by importance (vertical spread). The X-axis shows SHAP values (impact on prediction). Color indicates feature value (red = high, blue = low). Key insights: (1) Alcohol is most important - high values (red) consistently push predictions higher (right), (2) Volatile acidity is second most important - high values push predictions lower (left), (3) Density shows strong negative relationship when high. Compare this to feature importance from tree models - SHAP provides the same ranking but with directionality. This plot answers: “Which features matter most GLOBALLY across all wines?”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>SHAP Dependence Plot: Feature Relationships</h2>
<div id="41ffe2e5" class="cell" data-fig-height="8" data-fig-width="12" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href=""></a><span class="co"># Create dependence plot for alcohol</span></span>
<span id="cb12-2"><a href=""></a>shap.plots.scatter(shap_values[:, <span class="st">'alcohol'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="week6-black-box_files/figure-revealjs/cell-14-output-1.png" width="553" height="434"></p>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>The dependence plot shows the relationship between feature value (X-axis) and SHAP value (Y-axis) for a specific feature. For alcohol, we see a clear LINEAR relationship - as alcohol increases, SHAP value increases proportionally. This makes perfect sense for a linear model! The color indicates interaction effects with another feature (automatically selected by SHAP). The vertical spread at each X value shows variance across instances. This plot answers: “HOW does this feature affect predictions?” For linear models, you’ll always see straight lines. For non-linear models (trees, neural networks), these plots reveal non-linear patterns, thresholds, and interactions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Verifying SHAP Values for Linear Models</h2>
<div id="71ab50a8" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href=""></a><span class="co"># For linear models: SHAP = coefficient × (x - mean(x))</span></span>
<span id="cb13-2"><a href=""></a><span class="co"># Let's verify this relationship</span></span>
<span id="cb13-3"><a href=""></a></span>
<span id="cb13-4"><a href=""></a><span class="bu">print</span>(<span class="st">"Verification: SHAP values = coefficient × (feature - mean)"</span>)</span>
<span id="cb13-5"><a href=""></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">For the first test instance:"</span>)</span>
<span id="cb13-6"><a href=""></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Feature'</span><span class="sc">:&lt;25}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Coefficient'</span><span class="sc">:&gt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Value'</span><span class="sc">:&gt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Mean'</span><span class="sc">:&gt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'(Val-Mean)'</span><span class="sc">:&gt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'SHAP Value'</span><span class="sc">:&gt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Match?'</span><span class="sc">:&gt;8}</span><span class="ss">"</span>)</span>
<span id="cb13-7"><a href=""></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">105</span>)</span>
<span id="cb13-8"><a href=""></a></span>
<span id="cb13-9"><a href=""></a><span class="cf">for</span> i, feature <span class="kw">in</span> <span class="bu">enumerate</span>(X.columns):</span>
<span id="cb13-10"><a href=""></a>    coef <span class="op">=</span> model.coef_[i]</span>
<span id="cb13-11"><a href=""></a>    value <span class="op">=</span> X_test.iloc[<span class="dv">0</span>][feature]</span>
<span id="cb13-12"><a href=""></a>    mean <span class="op">=</span> X_train[feature].mean()</span>
<span id="cb13-13"><a href=""></a>    expected_shap <span class="op">=</span> coef <span class="op">*</span> (value <span class="op">-</span> mean)</span>
<span id="cb13-14"><a href=""></a>    actual_shap <span class="op">=</span> shap_values.values[<span class="dv">0</span>][i]</span>
<span id="cb13-15"><a href=""></a>    match <span class="op">=</span> <span class="st">"✓"</span> <span class="cf">if</span> np.isclose(expected_shap, actual_shap, rtol<span class="op">=</span><span class="fl">1e-3</span>) <span class="cf">else</span> <span class="st">"✗"</span></span>
<span id="cb13-16"><a href=""></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>feature<span class="sc">:&lt;25}</span><span class="ss"> </span><span class="sc">{</span>coef<span class="sc">:&gt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>value<span class="sc">:&gt;10.2f}</span><span class="ss"> </span><span class="sc">{</span>mean<span class="sc">:&gt;10.2f}</span><span class="ss"> </span><span class="sc">{</span>value<span class="op">-</span>mean<span class="sc">:&gt;12.2f}</span><span class="ss"> </span><span class="sc">{</span>actual_shap<span class="sc">:&gt;12.4f}</span><span class="ss"> </span><span class="sc">{</span>match<span class="sc">:&gt;8}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Verification: SHAP values = coefficient × (feature - mean)

For the first test instance:
Feature                    Coefficient      Value       Mean   (Val-Mean)   SHAP Value   Match?
---------------------------------------------------------------------------------------------------------
fixed acidity                   0.0459       6.00       6.87        -0.87      -0.0348        ✗
volatile acidity               -1.9149       0.29       0.28         0.01       0.0031        ✗
citric acid                    -0.0613       0.41       0.33         0.08      -0.0055        ✗
residual sugar                  0.0712      10.80       6.45         4.35       0.3151        ✗
chlorides                      -0.0265       0.05       0.05         0.00      -0.0001        ✗
free sulfur dioxide             0.0051      55.00      35.09        19.91       0.1074        ✗
total sulfur dioxide           -0.0002     149.00     138.00        11.00      -0.0023        ✗
density                      -124.2641       0.99       0.99        -0.00       0.0256        ✗
pH                              0.6007       3.09       3.19        -0.10      -0.0654        ✗
sulphates                       0.6491       0.59       0.49         0.10       0.0529        ✗
alcohol                         0.2290      10.97      10.51         0.46       0.0855        ✗</code></pre>
</div>
</div>
<aside class="notes">
<p>This verification slide demonstrates the elegant simplicity of SHAP for linear models. For each feature, SHAP value equals: coefficient × (feature_value - mean_feature_value). This shows that: (1) SHAP respects the linear model’s coefficients (direction and magnitude), (2) SHAP measures deviation from the mean (why predictions differ from average), (3) Features with large coefficients AND large deviations have high SHAP values. This is why linear models are perfect for teaching SHAP - you can directly see what SHAP computes. For non-linear models, SHAP still satisfies the same axioms, but the computation is more complex. The key insight: SHAP provides feature attributions that respect the model’s actual behavior.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">
<h2>Comparative Trade-offs: Choosing the Right XAI Tool</h2>
<table class="caption-top">
<colgroup>
<col style="width: 32%">
<col style="width: 20%">
<col style="width: 23%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Criterion</strong></th>
<th><strong>PDP</strong></th>
<th><strong>LIME</strong></th>
<th><strong>SHAP</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Scope</strong></td>
<td>Global (feature-level)</td>
<td>Local (instance-level)</td>
<td>Both (local + global)</td>
</tr>
<tr class="even">
<td><strong>Primary Visualization</strong></td>
<td>Line plot (2D curve)</td>
<td>Bar chart (sparse)</td>
<td>Force plot, Summary plot</td>
</tr>
<tr class="odd">
<td><strong>Interpretability</strong></td>
<td>★★★★★ Intuitive</td>
<td>★★★★☆ Easy to understand</td>
<td>★★★☆☆ Requires training</td>
</tr>
<tr class="even">
<td><strong>Computational Cost</strong></td>
<td>★★★★☆ Moderate</td>
<td>★★★☆☆ Fast sampling</td>
<td>★★☆☆☆ Expensive</td>
</tr>
<tr class="odd">
<td><strong>Theoretical Foundation</strong></td>
<td>Statistical (marginal effects)</td>
<td>Heuristic (local approximation)</td>
<td>Game theory (Shapley values)</td>
</tr>
<tr class="even">
<td><strong>Feature Coverage</strong></td>
<td>One at a time</td>
<td>Sparse (selected)</td>
<td>All features (dense)</td>
</tr>
<tr class="odd">
<td><strong>Best For</strong></td>
<td>Understanding global trends</td>
<td>Quick local explanations</td>
<td>Rigorous feature attribution</td>
</tr>
<tr class="even">
<td><strong>Key Limitation</strong></td>
<td>Assumes independence</td>
<td>Unstable (sampling)</td>
<td>Computational cost</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>This table provides a decision guide for practitioners. <strong>Choose PDP</strong> when you need to understand how a feature affects predictions globally - great for presentations to stakeholders. <strong>Choose LIME</strong> when you need fast, interpretable explanations for individual predictions - ideal for production systems where speed matters. <strong>Choose SHAP</strong> when you need theoretically grounded attributions and can afford the computational cost - best for research or high-stakes decisions. In practice, use multiple methods: PDP for global insights, LIME for quick checks, SHAP for final validation. Remember: no method is perfect - always validate against domain knowledge and use visualizations to communicate effectively.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="figs/vida.jpg" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://engineering.nyu.edu" class="uri">https://engineering.nyu.edu</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="week6-black-box_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="week6-black-box_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="week6-black-box_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'fast',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>