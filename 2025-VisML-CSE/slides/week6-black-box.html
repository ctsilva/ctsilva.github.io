<!DOCTYPE html>
<html lang="en"><head>
<script src="week6-black-box_files/libs/clipboard/clipboard.min.js"></script>
<script src="week6-black-box_files/libs/quarto-html/tabby.min.js"></script>
<script src="week6-black-box_files/libs/quarto-html/popper.min.js"></script>
<script src="week6-black-box_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="week6-black-box_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="week6-black-box_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="week6-black-box_files/libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.24">

  <meta name="author" content="Claudio Silva">
  <meta name="dcterms.date" content="2025-10-06">
  <title>Black-box Model Interpretation</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="week6-black-box_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="week6-black-box_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="week6-black-box_files/libs/revealjs/dist/theme/quarto-743137726eb562984e8d4ff610b648a8.css">
  <link href="week6-black-box_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="week6-black-box_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="week6-black-box_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="week6-black-box_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section class="quarto-title-block center">
  <h1 class="title">Black-box Model Interpretation</h1>
  <p class="subtitle">CS-GY 9223 - Fall 2025</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Claudio Silva 
</div>
        <p class="quarto-title-affiliation">
            NYU Tandon School of Engineering
          </p>
    </div>
</div>

  <p class="date">2025-10-06</p>
</section>
<section>
<section class="title-slide slide level1 center">
<h1>Black Box Model Assessment</h1>

</section>
<section class="slide level2">
<h2>Agenda</h2>
<p><br>
</p>
<h3>Goal: Study Model Agnostic Interpretability Methods. These should help to explain any type of ML Models.</h3>
<ol type="1">
<li><p>Partial Dependence Plot (PDP)</p></li>
<li><p>Local Interpretable Model-agnostic Explanations (LIME)</p></li>
<li><p>SHAP (SHapley Additive exPlanations)</p></li>
</ol>
<p>Examples and materials from Molnar’s book: https://christophm.github.io/interpretable-ml-book/</p>
</section>
<section class="slide level2">
<h2>Bike Rentals (Regression)</h2>
<div class="r-fit-text">
<p>This dataset contains daily counts of rented bicycles from the bicycle rental company Capital-Bikeshare in Washington D.C., along with weather and seasonal information. The goal is to predict how many bikes will be rented depending on the weather and the day. The data can be downloaded from the UCI Machine Learning Repository.</p>
<p>Here is the list of features used in Molnar’s book:</p>
<ul>
<li>Count of bicycles including both casual and registered users. The count is used as the target in the regression task.</li>
<li>The season, either spring, summer, fall or winter.</li>
<li>Indicator whether the day was a holiday or not.</li>
<li>The year, either 2011 or 2012.</li>
<li>Number of days since the 01.01.2011 (the first day in the dataset). This feature was introduced to take account of the trend over time.</li>
<li>Indicator whether the day was a working day or weekend.</li>
<li>The weather situation on that day. One of: clear, few clouds, partly cloudy, cloudy mist + clouds, mist + broken clouds, mist + few clouds, mist light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds heavy rain + ice pallets + thunderstorm + mist, snow + mist</li>
<li>Temperature in degrees Celsius.</li>
<li>Relative humidity in percent (0 to 100).</li>
<li>Wind speed in km per hour.</li>
</ul>
</div>
</section>
<section class="slide level2">
<h2>Partial Dependence Plot (PDP)</h2>
<p>Shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 2001).</p>

<img data-src="figs/black-box/bike-use-temperature.jpg" class="r-stretch"></section>
<section class="slide level2">
<h2>Partial Dependence Plot (PDP)</h2>
<p>High level idea: marginalizing the machine learning model output over the distributions of the all other features to show the relationship between the feature we are interested in and the predicted outcome.</p>
<div class="r-stack">
<p><img data-src="figs/black-box/pdp-feature1.jpg" class="fragment" width="950" height="450"></p>
<p><img data-src="figs/black-box/pdp-feature2.jpg" class="fragment" width="950" height="450"></p>
<p><img data-src="figs/black-box/pdp-feature3.jpg" class="fragment" width="950" height="450"></p>
<p><img data-src="figs/black-box/pdp-feature4.jpg" class="fragment" width="950" height="450"></p>
<p><img data-src="figs/black-box/pdp-feature5.jpg" class="fragment" width="950" height="450"></p>
<p><img data-src="figs/black-box/pdp-feature6.jpg" class="fragment" width="950" height="450"></p>
</div>
</section>
<section class="slide level2">
<h2>Partial Dependence Plot (PDP)</h2>
<div class="fragment">
<p><strong>Pros</strong></p>
<ul>
<li>Intuitive</li>
<li>Interpretation is clear</li>
<li>Easy to implement</li>
</ul>
</div>
<div class="fragment">
<p><strong>Cons</strong></p>
<ul>
<li>Assume independence among features</li>
<li>Can only show few features</li>
<li>Hidden heterogeneous effects from averaging</li>
</ul>
</div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<p>Training local surrograte models to explain <em>individual</em> predictions</p>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/lime-global-decision-boundaries.jpg"></p>
</div><div class="column" style="width:40%;">
<p><img data-src="figs/black-box/lime-paper.jpg"></p>
</div></div>
<p>https://arxiv.org/pdf/1602.04938.pdf</p>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="r-fit-text">
<p>The idea is quite intuitive.</p>
<ul>
<li><p>First, forget about the training data and imagine you only have the black box model where you can input data points and get the predictions of the model. You can probe the box as often as you want. Your goal is to understand why the machine learning model made a certain prediction. LIME tests what happens to the predictions when you give variations of your data into the machine learning model.</p></li>
<li><p>LIME generates a new dataset consisting of perturbed samples and the corresponding predictions of the black box model.</p></li>
<li><p>On this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest. The interpretable model can be anything from the interpretable models chapter, for example Lasso or a decision tree. The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation. This kind of accuracy is also called <em>local fidelity</em>.</p></li>
</ul>
</div>
<p>https://christophm.github.io/interpretable-ml-book/</p>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>

<img data-src="figs/black-box/lime-equations.jpg" class="r-stretch"><p>https://arxiv.org/pdf/1602.04938.pdf</p>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<h3>Algorithm</h3>
<ol type="1">
<li>Pick an input that you want an explanation for.</li>
<li>Sample the neighbors of the selected input (i.e.&nbsp;perturbation).</li>
<li>Train a linear classifier on the neighbors.</li>
<li>The weights on the linear classifier is the explanation.</li>
</ol>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/lime-random-forest-model.jpg"></p>
</div><div class="column" style="width:40%;">
<p>Random forest predictions given features x1 and x2.</p>
<p>Predicted classes: 1 (dark) or 0 (light).</p>
</div></div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/lime-random-forest-sampling.jpg"></p>
</div><div class="column" style="width:40%;">
<p>Instance of interest (big yellow dot) and data sampled from a normal distribution (small dots).</p>
</div></div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/lime-random-forest-weighting.jpg"></p>
</div><div class="column" style="width:40%;">
<p>Assign higher weight to points near the instance of interest. I.e., <span class="math inline">\(weight(p) = \sqrt{\frac{e^{-d^2}}{w^2}}\)</span> where <span class="math inline">\(d\)</span> is the distance between <span class="math inline">\(p\)</span> and the instantce of interest, and <span class="math inline">\(w\)</span> is the kernel width (self-defined).</p>
</div></div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/lime-random-forest-line.jpg"></p>
</div><div class="column" style="width:40%;">
<p>Use both the samples and sample weights to train a linear classifier.</p>
<p>Signs of the grid show the classifications of the locally learned model from the weighted samples. The red line marks the decision boundary (P(class=1) = 0.5).</p>
<p>The official implementation uses a Ridge Classifier as the linear model for explanation.</p>
</div></div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="r-fit-text">
<div class="columns">
<div class="column" style="width:50%;">
<p>Let us look at a concrete example. We go back to the bike rental data and turn the prediction problem into a classification: After taking into account the trend that the bicycle rental has become more popular over time, we want to know on a certain day whether the number of bicycles rented will be above or below the trend line. You can also interpret “above” as being above the average number of bicycles, but adjusted for the trend.</p>
<p>First we train a random forest with 100 trees on the classification task. On what day will the number of rental bikes be above the trend-free average, based on weather and calendar information?</p>
<p>The explanations are created with 2 features. The results of the sparse local linear models trained for two instances with different predicted classes:</p>
</div><div class="column" style="width:50%;">
<p><img data-src="figs/black-box/lime-example-result.jpg"></p>
</div></div>
</div>
</section>
<section class="slide level2">
<h2>Local Interpretable Model-agnostic Explanations (LIME)</h2>
<div class="fragment">
<p><strong>Pros</strong></p>
<ul>
<li>Explanations are short (= selective) and possibly contrastive.
<ul>
<li>we can control the sparsity of weight coefficients in the regressions method.</li>
</ul></li>
<li>Very easy to use.</li>
</ul>
</div>
<div class="fragment">
<p><strong>Cons</strong></p>
<ul>
<li>Unstable results due to sampling.</li>
<li>Hard to weight similar neighbors in a high dimensional dataset.</li>
<li>Many parameters for data scientists to hide biases.</li>
</ul>
</div>
</section>
<section class="slide level2">
<h2>SHAP (SHapley Additive exPlanations)</h2>
<p>Examples and materials from Molnar’s new book: https://christophmolnar.com/books/shap/</p>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="figs/black-box/shap-example.jpg"></p>
</div><div class="column" style="width:40%;">
<p>SHAP (Lundberg and Lee 2017a) is a game-theory-inspired method created to explain predictions made by machine learning models. SHAP generates one value per input feature (also known as SHAP values) that indicates how the feature contributes to the prediction of the specified data point.</p>
</div></div>
</section>
<section class="slide level2">
<h2>A Short History of Shapley Values and SHAP</h2>
<ul>
<li>1953: The introduction of Shapley values in game theory (by Lloyd Shapley).</li>
<li>2010: The initial steps toward applying Shapley values in machine learning
<ul>
<li>original paper contained NO code!</li>
</ul></li>
<li>2017: The advent of SHAP (by Lundberg and Lee), a turning point in machine learning.</li>
</ul>
</section>
<section class="slide level2">
<h2>Theory of Shapley Values</h2>
<p>Who’s going to pay for that taxi?</p>
<p>Alice, Bob, and Charlie have dinner together and share a taxi ride home. The total cost is $51. The question is, how should they divide the costs fairly?</p>

<img data-src="figs/black-box/shap-taxi.jpg" class="r-stretch"></section>
<section class="slide level2">
<h2>Theory of Shapley Values</h2>
<p>The <strong>marginal contribution</strong> of a player to a coalition is the value of the coali- tion with the player minus the value of the coalition without the player. In the taxi example, the value of a coalition is equal to the cost of the ride as detailed in the above table. Therefore, the marginal contribution of, for instance, Charlie to a taxi already containing Bob is the cost of the taxi with Bob and Charlie, minus the cost of the taxi with Bob alone.</p>
</section>
<section class="slide level2">
<h2>Theory of Shapley Values</h2>

<img data-src="figs/black-box/shap-marginal-contribution.jpg" class="r-stretch"></section>
<section class="slide level2">
<h2>Theory of Shapley Values</h2>
<p>How to average these marginal contributions per passenger?</p>
<p>One way to answer this question is by considering all possible permutations of Alice, Bob, and Charlie. There are 3! = 3 * 2 * 1 = 6 possible permutations of passengers:</p>
<ul>
<li>Alice, Bob, Charlie</li>
<li>Alice, Charlie, Bob</li>
<li>Bob, Alice, Charlie</li>
<li>Charlie, Alice, Bob</li>
<li>Bob, Charlie, Alice</li>
<li>Charlie, Bob, Alice</li>
</ul>
<p>We can use these permutations to form coalitions, for example, for Alice.</p>
</section>
<section class="slide level2">
<h2>Theory of Shapley Values</h2>
<p>In two of these cases, Alice was added to an empty taxi, and in one case, she was added to a taxi with only Bob. By weighting the marginal contributions accordingly, we calculate the following weighted average marginal contribution for Alice, abbreviating Alice, Bob, and Charlie to A, B, and C:</p>

<img data-src="figs/black-box/shap-marginal-alice.jpg" class="r-stretch"></section>
<section class="slide level2">
<h2>Theory of Shapley Values</h2>
<p>for Bob:</p>
<p><img data-src="figs/black-box/shap-marginal-bob.jpg"></p>
<p>for Charlie:</p>
<p><img data-src="figs/black-box/shap-marginal-charlie.jpg"></p>
</section>
<section class="slide level2">
<h2>Calculating Shapley values</h2>

<img data-src="figs/black-box/shap-general-formation.jpg" class="r-stretch"><p><strong>The Shapley value is the weighted average of a player’s marginal contribu- tions to all possible coalitions.</strong></p>
</section>
<section class="slide level2">
<h2>The axioms behind Shapley values</h2>
<ul>
<li><p>Efficiency: The sum of the contributions must precisely add up to the payout.</p></li>
<li><p>Symmetry: If two players are identical, they should receive equal contributions.</p></li>
<li><p>Dummy or Null Player: The value of a player who doesn’t contribute to any coalition is zero.</p></li>
<li><p>Additivity: In a game with two value functions, the Shapley values for the sum can be expressed as the sum of the Shapley values.</p></li>
</ul>
<p><strong>These four axioms ensure the uniqueness of the Shapley values.</strong></p>
</section>
<section class="slide level2">
<h2>From Shapley Values to SHAP</h2>
<p>Consider the following scenario: You have trained a machine learning model <span class="math inline">\(f\)</span> to predict apartment prices.</p>

<img data-src="figs/black-box/shap-apartment-example.jpg" class="r-stretch"></section>
<section class="slide level2">
<h2>From Shapley Values to SHAP</h2>
<p>We want to evaluate the effort of cat-banned</p>

<img data-src="figs/black-box/shap-cat-banned.jpg" class="r-stretch"></section>
<section class="slide level2">
<h2>From Shapley Values to SHAP</h2>
<p>We want to evaluate the effort of cat-banned</p>

<img data-src="figs/black-box/shap-cat-banned-2.jpg" class="r-stretch"></section>
<section class="slide level2">
<h2>Interpreting SHAP values</h2>
<p>The Shapley value can be misinterpreted. The Shapley value of a feature value is not the difference of the predicted value after removing the feature from the model training. The interpretation of the Shapley value is: <strong>Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.</strong></p>
<p>The Shapley value is the wrong explanation method if you seek sparse explanations (explanations that contain few features). Explanations created with the Shapley value method <strong>always use all the features.</strong> Humans prefer selective explanations, such as those produced by LIME. LIME might be the better choice for explanations lay-persons have to deal with.</p>
<p>(From Molnar’s book)</p>
</section>
<section class="slide level2">
<h2>SHAP (SHapley Additive exPlanations)</h2>
<div class="fragment">
<p><strong>Pros</strong></p>
<ul>
<li><p>Fairly distributed feature importance to a prediction</p></li>
<li><p>Contrastive explanations (can compare an instance to a subset or even to a single data point)</p></li>
<li><p>Solid theory</p></li>
</ul>
</div>
<div class="fragment">
<p><strong>Cons</strong></p>
<ul>
<li>A lot of computing time</li>
<li>Not sparse explanations (every feature is important)</li>
</ul>
</div>
</section>
<section class="slide level2">
<h2>SHAP (SHapley Additive exPlanations)</h2>

<img data-src="figs/black-box/shap-figure-waterfall.jpg" class="r-stretch"></section>
<section class="slide level2">
<h2>SHAP limitations</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="figs/black-box/shap-problems.jpg"></p>
<p>http://proceedings.mlr.press/v119/kumar20e/kumar20e.pdf</p>
</div><div class="column" style="width:40%;">

</div></div>

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="figs/vida.jpg" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://engineering.nyu.edu" class="uri">https://engineering.nyu.edu</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="week6-black-box_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="week6-black-box_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="week6-black-box_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="week6-black-box_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'fast',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>